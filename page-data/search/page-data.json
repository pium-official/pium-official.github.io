{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"이 글은 우테코 피움팀 크루 '주노', '그레이', '조이', '하마드'\n가 작성했습니다. 서론 다음 discussion으로부터 시작된 안건입니다.\ndiscussion - 무중단 배포 환경 구성 우리가 서버 한 대로 서비스를 운영한다고 가정해보자. 새로운 기능이 추가되고 배포되기 위해서는 애플리케이션을 빌드한 뒤 구동이 완료되어야한다. 이 과정에서 기존의…","fields":{"slug":"/dont-stop-deploy/"},"frontmatter":{"date":"October 19, 2023","title":"무중단 배포 환경 구성하기","tags":["HTTPS"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀\n>\n크루 '[주노](https://github.com/Choi-JJunho)', '[그레이](https://github.com/kim0914)', '[조이](https://github.com/yeonkkk)', '[하마드](https://github.com/rawfishthelgh)'\n> 가 작성했습니다.\n\n## 서론\n\n> 다음 discussion으로부터 시작된 안건입니다.\n> [discussion - 무중단 배포 환경 구성](https://github.com/woowacourse-teams/2023-pium/discussions/358)\n\n우리가 서버 한 대로 서비스를 운영한다고 가정해보자.\n\n새로운 기능이 추가되고 배포되기 위해서는 애플리케이션을 빌드한 뒤 구동이 완료되어야한다.\n\n이 과정에서 기존의 실행 중인 서버 프로세스를 종료한 후 새롭게 실행해야 한다.\n\n이 때 서비스를 이용 중인 사용자는 그 시간 동안 서비스를 이용할 수 없게 된다.\n\nV1 버전이 종료되고 V2 버전이 실행되는 그 사이, 즉 유저가 서비스를 이용할 수 없는 시간을 **다운타임(downtime)** 이라고 한다.\n\n### 고가용성\n\n다음 자료는 페이지 로드 속도가 늦어짐에 따라 사용자의 이탈률이 증가한다는 지표다.\n\n![](.index_images/7e6334be.png)\n[출처 - Think with Google](https://www.thinkwithgoogle.com/marketing-strategies/app-and-mobile/mobile-page-speed-new-industry-benchmarks/)\n\n애플리케이션이 아닌 배포 과정이 이러한 다운타임에 대한 영향을 미치는 것이 올바르지 못한 상황이라고 여겨진다.\n\n### 무중단 배포 방식\n\n무중단 배포는 말 그대로 **서비스가 중단되지 않은 상태**로, 새로운 버전을 사용자들에게 다운 타임 없이 배포하는 것을 의미한다. 무중단 배포를 하기 위해서는 최소 서버 2대 이상을 확보해야한다.\n\n무중단 배포에는 크게 3가지 방식이 있다.\n\n1. 롤링(Rolling) 배포\n2. 블루/그린 배포\n3. 카나리(Canary) 배포\n\n#### 롤링 배포\n\n- 트래픽을 점진적으로 구버전에서 새로운 버전으로 옮기는 방식이다.\n- 점진적으로 트래픽을 어떻게 새로운 버전으로 옮길 수 있을까?\n    - 방식 1\n        - 기존에 V1 서버를 3대 운영 중이라고 하자.\n        - 우선 V2 인스턴스를 하나 추가한다. 로드 밸런서에 이 인스턴스를 연결한 후 기존 3대의 V1 서버를 하나씩 삭제한다.\n        - 서버 개수를 유연하게 조절할 수 있는 클라우드를 기반으로 서비스를 운영할 때 적합할 것 같다.\n    - 방식 2\n        - V1이 실행되고 있는 서버 하나를 로드 밸런서에서 떼어낸다.\n        - 해당 서버에는 트래픽이 도달하지 않으므로 이 상태에서 해당 서버의 어플리케이션을 V2로 교체한다.\n        - 이 과정을 반복하며 모든 서버를 V2로 교체한다.\n- 장점\n    - 롤링 배포 방식은 k8s, elastic beanstalk과 같은 많은 오케스트레이션 도구에서 지원하여 간편하다.\n    - 또한 많은 서버 자원을 확보하지 않아도 무중단 배포가 가능하다.\n- 단점\n    - 방식 2와 같은 경우 **배포 도중 서비스 중인 인스턴스의 수가 줄어드게 되어 각각의 서버가 부담하는 트래픽의 양이 늘어날 수 있다.**\n    - 구버전과 신버전의 어플리케이션이 동시에 서비스 되기 때문에 **호환성 문제**가 발생할 수 있다.\n\n---\n\n#### 블루/그린 배포\n\n- 트래픽을 한 번에 구버전에서 신버전으로 옮기는 방법이다.\n- 현재 운영 중인 서비스 환경을 블루라고 부르고, 새롭게 배포할 환경을 그린.\n- 블루와 그린의 서버를 동시에 나란히 구성해둔 상태로 배포 시점에 로드 밸런서가 트래픽을 블루에서 그린으로 일제히 전환시킨다.\n- 그린 버전 배포가 성공적으로 완료 되었고, 문제가 없다고 판단할 때 블루 서버를 제거할 수 있다.\n\n- 장점\n    - 롤링 배포와 달리 한번에 트래픽을 모두 새로운 버전으로 옮기기 때문에 호환성 문제가 발생하지 않는다.\n- 단점\n    - 실제 운영에 필요한 서버 리소스 대비 2배의 리소스를 확보해야 한다.\n\n#### 카나리(Canary) 배포\n\n- 점진적으로 구버전에 대한 트래픽을 신버전으로 옮기는 것은 롤링 배포 방식과 동일\n- 하지만 카나리 배포의 핵심은 새로운 버전에 대한 **오류를 조기에 감지**하는 것이다.\n- 소수 인원에 대해서만 트래픽을 새로운 버전에 옮겨둔 상태에서 서비스를 운영한다.\n- A/B 테스트를 진행할 수 있다.\n\n- 장점\n    - 새로운 버전으로 인한 위험을 최소화할 수 있다.\n- 단점\n    - 롤링 배포와 마찬가지로 호환성 문제가 발생할 수 있다.\n\n롤링 배포와 카나리 배포의 경우 서버의 자원을 많이 필요로 한다는 점이 적용하기 어려운 부분으로 다가왔다.\n\n따라서 현재 사용할 수 있는 자원을 고려해봤을 때 합당하다고 생각되는 **블루/그린 배포** 방식을 채택한다.\n\n### 서버 사양 확인\n\n무중단 배포 환경을 구성하기 이전에 서버 사용중인 리소스를 먼저 확인해보자.\n\n![](.index_images/2aa53eb9.png)\n\n메모리는 (스왑메모리 제외) 총 1.8 GB 중 742MB를 상시 사용중이고 이 중 애플리케이션은 534MB만큼의 메모리를 사용중이다.\n\n여유있게 애플리케이션은 상시 약 600MB 정도의 메모리를 사용한다고 생각해볼 수 있다.\n\n이에 새로운 애플리케이션을 서버에 한 대 더 띄울경우 약 600MB를 더 사용하게되어 1.4GB 메모리를 상시 사용할 것으로 예상된다.\n\n### 무중단 배포 시나리오 구성\n\n> `[블루, 그린 배포방식으로 진행한다]`\n\n![](.index_images/4cfcbb80.png)\n\n> [이미지 출처](https://github.com/jojoldu/springboot-webservice/blob/master/tutorial/7_NGINX_SSL_%EB%AC%B4%EC%A4%91%EB%8B%A8%EB%B0%B0%ED%8F%AC.md)\n\n- 8080, 8081 포트에 WAS를 구성한다.\n- 새롭게 배포될 때 nginx가 가리키는 포트를 정해준다.\n\n### 무중단 배포 스크립트 작성\n\n무중단 배포환경을 구성하기 위해 쉘 스크립트를 작성해보자. 보통 새롭게 배포할 PORT를 GREEN, 구버전 포트를 BLUE로 지칭하는데, 조금 더 이해하기 쉽게 GREEN을 NEW PORT, BLUE를\nCURRENT PORT로 지칭하도록 하겠다.\n\n#### 1. CURRENT PORT, NEW PORT 설정\n\n8080, 8081 포트가 살아있는지 확인한다.\n\n살아있는 포트를 `CURRENT_PORT`, 그렇지 않은 포트를 `NEW_PORT`로 판별한다.\n\n이 때 `NEW_PORT`로 지정하려는 포트에 이미 애플리케이션이 있으면 프로세스를 종료시켜 자리를 비워준다\n\n```bash\n#!/bin/bash\n\necho \"> 현재 구동중인 Port 확인\"\n\n# CURRENT & NEW 포트 확인\nif lsof -Pi :8080 -sTCP:LISTEN -t >/dev/null; then\n    echo \"현재 8080 포트가 사용 중입니다.\"\n    CURRENT_PORT=8080\n    NEW_PORT=8081\nelif lsof -Pi :8081 -sTCP:LISTEN -t >/dev/null; then\n    echo \"현재 8081 포트가 사용 중입니다.\"\n    CURRENT_PORT=8081\n    NEW_PORT=8080\nelse\n    echo \"8080과 8081 포트 모두 사용 중이지 않습니다.\"\n    exit 1\nfi\n# NEW 포트 사용중일 경우 종료\nif lsof -Pi :$NEW_PORT -sTCP:LISTEN -t >/dev/null; then\n  echo \"NEW_PORT가 사용중입니다.\"\n  PID=$(lsof -Pi :$NEW_PORT -sTCP:LISTEN -t)\n  kill -9 $PID\n  echo \"사용 중인 NEW_PORT 종료했습니다..\"\n  sleep 3\nelse\n  echo \"NEW_PORT가 사용 중이지 않습니다.\"\nfi\n```\n\n#### 2. NEW PORT에 프로젝트 구동\n\nNEW PORT에 새로운 프로젝트를 구동한다.\n\n```bash\n#!/bin/bash\n#Spring ON\nnohup java -jar \"pium.jar\" --spring.profiles.active=dev --server.port=$NEW_PORT > log.txt 2>&1 &\n\necho 백그라운드 모드로 애플리케이션 실행 성공 !!\n```\n\n### 3. Health Check 수행\n\n애플리케이션이 구동 완료되기까지의 시간을 고려하여 15초간 sleep을 실행한다.\n\nsleep 이후, NEW PORT 애플리케이션의 정상 실행을 확인하기 위해 열 번의 health check를 수행하도록 한다.(UP 문자열이 응답되지 않으면 배포 종료)\n\n> **health check**\n> 프로젝트 의존성에 스프링부트 actuator를 추가하여 health check를 수행할 수 있다.\n> `implementation('org.springframework.boot:spring-boot-starter-actuator')`\n> ![](.index_images/f3b48297.png)\n> 의존성만 추가하면 위와 같은 응답을 받을 수 있다.\n\n```bash\n#!/bin/bash\n\n# 새로 가동하는 서버 상태 확인\nsleep 15\n\nfor retry_count in {1..10}\ndo\n  response=$(curl -s http://localhost:$NEW_PORT/actuator/health)\n  up_count=$(echo $response | grep 'UP' | wc -l)\n\n  if [ $up_count -ge 1 ]\n then # $up_count >= 1 (\"UP\" 문자열이 있는지 검증)\n      echo \"> Health check 성공\"\n      break\n  else\n      echo \"> 새롭게 가동하는 서버의 상태가 UP이 아닙니다.\"\n      echo \"> Health check: ${response}\"\n  fi\n\n  if [ $retry_count -eq 10 ]\n  then\n    echo \"> Health check 실패. \"\n    echo \"> Nginx에 연결하지 않고 배포를 종료합니다.\"\n    exit 1\n  fi\n\n  echo \"> Health check 연결 실패. 재시도...\"\n  sleep 10\ndone\n```\n\n#### 3. nginx 포트 변경\n\nnginx 포트를 변수로 변경해준다.\n\n**nginx 포트를 동적으로 구성하기**\n\nnginx 설정은 현재 8080 포트에 대해서만 프록시를 구성하고있다.\n\n8080, 8081 포트를 동적으로 변경할 수 있도록 개선해보자.\n\n쉘 스크립트로 `/etc/nginx/conf.d/service-url.inc;` 파일을 생성한다.\n\n`sudo vim /etc/nginx/nginx.conf`\n\n```bash\n# service-url.inc 파일 내용\nset $service_url http://127.0.0.1:8080;\n```\n\n기존 nginx 설정에는 proxy pass 설정을 `http://127.0.0.1:8080` 로 구성했다면, 이제는 `$service_url` 환경변수가 proxy pass가 될 수 있도록 변경, 배포 시마다 이\n값이 동적으로 8080, 혹은 8081로 변경되도록 한다.\n\n- 기존 설정\n\n```bash\n# /etc/nginx/sites-available/backend\nserver {\n\n        server_name dev.api.pium.life;\n\n        location / {\n                if ($request_method = 'OPTIONS') {\n                        add_header 'Access-Control-Allow-Origin' 'https://dev.pium.life';\n                        add_header 'Access-Control-Allow-Methods' 'GET, POST, DELETE, PATCH, OPTIONS';\n                        add_header 'Access-Control-Allow-Headers' 'Content-Type, Authorization';\n                        add_header 'Access-Control-Max-Age' 86400;\n                        add_header 'Access-Control-Allow-Credentials' 'true';\n                        add_header 'Access-Control-Expose-Headers' 'Set-Cookie';\n                        return 204;\n                }\n                add_header 'Access-Control-Allow-Credentials' 'true' always;\n                add_header 'Access-Control-Allow-Origin' 'https://dev.pium.life' always;\n                add_header 'Access-Control-Expose-Headers' 'Set-Cookie' always;\n\n                proxy_set_header Host $host:$server_port;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n                proxy_pass http://127.0.0.1:8080;\n        }\n\n        location /docs {\n                proxy_pass http://127.0.0.1:8080;\n        }\n\n        location /admin {\n                proxy_pass http://127.0.0.1:8080;\n        }\n\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/dev.pium.life/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/dev.pium.life/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n\n}\n\nserver {\n    if ($host = dev.api.pium.life) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    server_name dev.api.pium.life;\n    listen 80;\n    return 404; # managed by Certbot\n}\n```\n\n- 개선 설정\n\n```bash\n# /etc/nginx/sites-available/backend\nserver {\n\n        server_name dev.api.pium.life;\n\n        # include 파일 추가하기\n        include /etc/nginx/conf.d/service-url.inc;\n\n        location / {\n                if ($request_method = 'OPTIONS') {\n                        add_header 'Access-Control-Allow-Origin' 'https://dev.pium.life';\n                        add_header 'Access-Control-Allow-Methods' 'GET, POST, DELETE, PATCH, OPTIONS';\n                        add_header 'Access-Control-Allow-Headers' 'Content-Type, Authorization';\n                        add_header 'Access-Control-Max-Age' 86400;\n                        add_header 'Access-Control-Allow-Credentials' 'true';\n                        add_header 'Access-Control-Expose-Headers' 'Set-Cookie';\n                        return 204;\n                }\n                add_header 'Access-Control-Allow-Credentials' 'true' always;\n                add_header 'Access-Control-Allow-Origin' 'https://dev.pium.life' always;\n                add_header 'Access-Control-Expose-Headers' 'Set-Cookie' always;\n                # add_header 'Content-Type' 'application/json' always;\n\n                proxy_set_header Host $host:$server_port;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n                # 이걸 변수로 지정\n                # proxy_pass http://127.0.0.1:8080;\n                proxy_pass $service_url;\n        }\n\n        location /docs {\n                # proxy_pass http://127.0.0.1:8080;\n                proxy_pass $service_url;\n        }\n\n        location /admin {\n                # proxy_pass http://127.0.0.1:8080;\n                proxy_pass $service_url;\n        }\n\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/dev.pium.life/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/dev.pium.life/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n\n}\n\nserver {\n    if ($host = dev.api.pium.life) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n    server_name dev.api.pium.life;\n    listen 80;\n    return 404; # managed by Certbot\n}\n\n```\n\n이제 쉘 스크립트에서 배포 시마다 해당 `NEW_PORT`에 맞춰 `$service_url` 환경변수를 변경하고, nginx를 재시작한다.\n\n```bash\n#!/bin/bash\n\necho \"> 전환할 Port: $NEW_PORT\"\n\necho \"> Port 전환\"\necho \"set \\$service_url http://127.0.0.1:${NEW_PORT};\" | sudo tee /etc/nginx/conf.d/service-url.inc\necho \"> NGINX Reload\"\nsudo service nginx reload\n```\n\n#### 4. CURRENT PORT 내리기\n\nnginx reload (포트포워딩 완료)가 완료되면 `CURRENT_PORT`를 내린다.\n\n```bash\necho \"> CURRENT_PORT를 종료합니다\"\nCURRENT_PID=$(lsof -Pi :$CURRENT_PORT -sTCP:LISTEN -t)\nkill -9 $CURRENT_PID\n```\n\n### 최종 배포 스크립트 형태\n\n```bash\n#!/bin/bash\n\necho \"> 현재 구동중인 Port 확인\"\n\n# CURRENT & NEW 포트 확인\nif lsof -Pi :8080 -sTCP:LISTEN -t >/dev/null; then\n    echo \"현재 8080 포트가 사용 중입니다.\"\n    CURRENT_PORT=8080\n    NEW_PORT=8081\nelif lsof -Pi :8081 -sTCP:LISTEN -t >/dev/null; then\n    echo \"현재 8081 포트가 사용 중입니다.\"\n    CURRENT_PORT=8081\n    NEW_PORT=8080\nelse\n    echo \"8080과 8081 포트 모두 사용 중이지 않습니다.\"\n    exit 1\nfi\n# NEW 포트 사용중일 경우 종료\nif lsof -Pi :$NEW_PORT -sTCP:LISTEN -t >/dev/null; then\n  echo \"NEW_PORT가 사용중입니다.\"\n  PID=$(lsof -Pi :$NEW_PORT -sTCP:LISTEN -t)\n  kill -9 $PID\n  echo \"사용 중인 NEW_PORT 종료했습니다..\"\n  sleep 3\nelse\n  echo \"NEW_PORT가 사용 중이지 않습니다.\"\nfi\n\n#Spring ON\nnohup java -jar \"pium.jar\" --spring.profiles.active=dev --server.port=$NEW_PORT > log.txt 2>&1 &\n\necho 백그라운드 모드로 애플리케이션 실행 성공 !!\n\n# 새로 가동하는 서버 상태 확인\nsleep 15\n\nfor retry_count in {1..10}\ndo\n  response=$(curl -s http://localhost:$NEW_PORT/actuator/health)\n  up_count=$(echo $response | grep 'UP' | wc -l)\n\n  if [ $up_count -ge 1 ]\n then # $up_count >= 1 (\"UP\" 문자열이 있는지 검증)\n      echo \"> Health check 성공\"\n      break\n  else\n      echo \"> 새롭게 가동하는 서버의 상태가 UP이 아닙니다.\"\n      echo \"> Health check: ${response}\"\n  fi\n\n  if [ $retry_count -eq 10 ]\n  then\n    echo \"> Health check 실패. \"\n    echo \"> Nginx에 연결하지 않고 배포를 종료합니다.\"\n    exit 1\n  fi\n\n  echo \"> Health check 연결 실패. 재시도...\"\n  sleep 10\ndone\n\n# 새로 가동하는 서버로 전환\n\necho \"> 전환할 Port: $NEW_PORT\"\n\necho \"> Port 전환\"\necho \"set \\$service_url http://127.0.0.1:${NEW_PORT};\" | sudo tee /etc/nginx/conf.d/service-url.inc\n\necho \"> NGINX Reload\"\nsudo service nginx reload\n\necho \"> CURRENT_PORT를 종료합니다\"\nCURRENT_PID=$(lsof -Pi :$CURRENT_PORT -sTCP:LISTEN -t)\nkill -9 $CURRENT_PID\n```\n\n### 시나리오 정리\n\n1. 현재 구동중인 포트(CURRENT)와 새롭게 배포할 포트(NEW)를 확인한다.\n2. NEW를 점유중인 프로세스가 있을 시 강제 종료한다.\n3. NEW에 애플리케이션을 구동한다\n4. 애플리케이션의 정상 작동을 확인한다(health check)\n5. nginx의 포트 포워딩 설정을 NEW PORT로 변경 후, nginx를 재가동한다.\n6. 기존 사용하전 구버전 포트(CURRENT)를 내린다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노, 참새'가 작성했습니다. 서론 피움 서비스를 이용하는 과정에서 불러오는 이미지의 크기가 너무 커서 사용자가 이미지 로드할 때 시간이 오래 걸린다. \n위와 같은 환경에서 이미지 로드 시간을 책정해봤다.\n \n사용자가 인터넷 환경이 느린 경우 1.8MB 사진을 로드하는 데 9.99초 이상의 로드시간이 걸린다. 피움 서비스는 …","fields":{"slug":"/image-resize/"},"frontmatter":{"date":"October 03, 2023","title":"서버에서 이미지 리사이징하기","tags":["javascript","이미지","webp"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho), [참새](https://github.com/WaiNaat)'가 작성했습니다.\n\n## 서론\n\n피움 서비스를 이용하는 과정에서 불러오는 이미지의 크기가 너무 커서 사용자가 이미지 로드할 때 시간이 오래 걸린다.\n\n> ![](.index_images/79ba487e.png)\n> 위와 같은 환경에서 이미지 로드 시간을 책정해봤다.\n> ![](.index_images/5020cb2e.png) \n> 사용자가 인터넷 환경이 느린 경우 1.8MB 사진을 로드하는 데 9.99초 이상의 로드시간이 걸린다.\n\n피움 서비스는 사진 업로드가 유의미할 정도로 많이 일어나지 않는다.\n\n따라서 다음과 같은 시나리오로 이미지를 압축하려고한다.\n\n```markdown\n1. S3에서 이미지 불러오기\n2. 이미지 압축\n3. S3로 이미지 보내기\n```\n\n서버에서 특정 시간마다 이미지 압축을 수행하고 이를 S3에 업로드하는 과정을 통해 이미지 최적화를 진행해보자.\n\n## 시작하기\n\n> WebP(웹피)는 손실/비손실 압축 이미지 파일을 위한 이미지 포맷이다.\n> [위키백과](https://ko.wikipedia.org/wiki/WebP)\n\n이미지 압축을 진행하기에 앞서 다음 두가지 방식을 고려했다.\n\n- `1. 서버에서 cwebp 프로그램을 이용해 이미지를 webp 형태로 변환하는 방법`\n- `2. node, python과 같은 언어를 이용해 이미지를 변환하는 방법`\n\n1번의 경우 서버에 직접 프로그램을 설치하고 관리해야하는 관리 소요가 있다\n\n현재 빌드서버에서 배치로 작업을 수행할 것이기 때문에 npm이 이미 설치되어있다는 가정하에 javascript로 개발하는 것이 적합하다고 판단하여 node.js로 이미지 최적화 코드를 작성헀다.\n\n추가로 해당 코드를 주기적으로 실행시키기 위한 쉘 스크립트도 함께 작성했다.\n\n## 코드\n\n코드를 하나씩 살펴보자.\n\n### 자바스크립트\n\n이미지 변환에는 [sharp](https://sharp.pixelplumbing.com/) 라이브러리를 사용하였다. [npm trends](https://npmtrends.com/@squoosh/lib-vs-imagemagick-vs-imagemin-vs-sharp)를 이용해 비교했을 때 다른 패키지들과의 다운로드 수 차이가 많이 나서 정보를 찾아봤었다. sharp는 [libvips](https://github.com/libvips/libvips)를 사용하기 때문에 다른 라이브러리에 비해서 속도가 훨씬 빠르고, 컴퓨터의 자원 자체를 최대한 조금만 사용하도록 설계했다는 이야기를 듣게 되었다. 이는 아마존 aws라는 굉장히 한정된 자원을 가진 서버에서 이미지 최적화를 해야 한다는 우리의 문제 해결에 가장 적합할 것이라는 생각이 들어 최종적으로 sharp를 선택하였다.\n\n![npm trends로 sharp, imagemin, imagemagick, squoosh의 다운로드 수를 비교한 결과 sharp가 압도적으로 많다](.index_images/npm-trends.png)\n\n사진은 압축률이 가장 좋은 최신 기술인 webp를 기본적으로 제공하되, 지원하지 않는 브라우저를 고려해 png 확장자로도 바꿔서 저장한다. 아래는 변환 함수 부분만 담은 코드이다. `outputPath`의 파일 이름을 정하는 방식은 피움 팀 프론트엔드의 회의를 통해 나온 규칙이며, [Github Discussions](https://github.com/woowacourse-teams/2023-pium/discussions/384)에서 확인 가능하다.\n\n```javascript\nimport { join } from 'path';\nimport sharp from 'sharp';\n\nconst convertImageTo = async (dir, filename, width, type, format) => {\n  const filenameWithoutExt = filename.split('.')[0];\n  const inputPath = join(dir, filename);\n  const outputPath = join(dir, `${filenameWithoutExt}.${type}.${format.toLowerCase()}`);\n\n  const image = sharp(inputPath);\n  const { width: imageWidth } = await image.metadata();\n  \n  // 이미지가 목표보다 이미 작을 경우 굳이 조정하지 않음\n  if (imageWidth <= width) {\n    await image\n      .withMetadata()\n      .toFile(outputPath);\n\n    return false;\n  }\n\n  await image\n    .resize(width)\n    .withMetadata()\n    .toFormat(format.toLowerCase(), { quality: 100 })\n    .toFile(outputPath);\n\n  return true;\n};\n```\n\n변환 이후 똑바로 일어나 있던 사진이 눕는 현상이 생겼었다. 이는 사진 파일들이 `orientation`이라는 메타데이터를 갖고 있는데 변환을 하면서 해당 자료를 잃어버려 생기는 현상이었다. sharp의 `withMetadata()`를 사용하면 간단하게 해결이 가능하다.\n\n커맨드 라인에서 받은 정보들을 갖고 이 함수를 이용해 하나씩 파일을 변환하는 방식이다.\n\n### 쉘 스크립트\n\n파일을 탐색하여 자바스크립트로 작성된 함수를 수행한다.\n\n```shell\n# 전처리\n\nconvert_image() {\n    local count_success=0\n    local count_failure=0\n    local count_skip=0\n\n    local target_dir=$1\n    local size=$2\n    \n    local total_files=$(find \"$target_dir\" -type f -iname \"*.jpeg\" -o -iname \"*.jpg\" -o -iname \"*.png\" -o -iname \"*.gif\" | wc -l)\n\n    echo \"$total_files 개의 파일에 대한 작업 시작\"\n\n    # 대, 소문자 구분 없이 탐색 활성화\n    shopt -s nocaseglob\n    for file in \"$target_dir\"/*.{jpeg,jpg,png,gif}; do\n\t    # 대, 소문자 구분 없이 탐색 비활성화\n\t    shopt -u nocaseglob\n\n\t    if [ -f \"$file\" ]; then # 파일인 경우에만 처리\n\t\t    filename_full=$(basename \"$file\")\n\t\t    extension=\"${filename##*.}\" # 확장자 추출\n\t\t    extension=$(echo \"$extension\" | tr '[:upper:]' '[:lower:]')  # 확장자 소문자로 변환\n\t\t    filename=\"${filename_full%.*}\"\n\n\t\t    output_file=\"$target_dir/${filename}.${nick_name}.webp\"\n\t\t    if [ -e \"$output_file\" ]; then # 이미 존재하는 파일이라면 생략\n\t\t\t    ((count_skip+=1))\n\t\t    else\n\t\t\t    # 여기에 이미지 압축!\n\t\t\t    node resize.js -d \"$target_dir\" -i \"$filename_full\" -s \"$size\" -n \"$nick_name\"\n\n\t\t\t    if [ $? -ne 0 ]; then\n\t\t\t\t    echo \"변환 실패: $file -> $output_file\" >> \"$error_log\"\n\t\t\t\t    ((count_failure++))\n\t\t\t    else\n\t\t\t\t    ((count_success++))\n\t\t\t    fi\n\t\t    fi\n\t    fi\n    done\n    echo \"\"\n    echo \"SIZE $width 변환 실패 파일 개수: $count_failure\"\n    echo \"SIZE $width 변환 성공 파일 개수: $count_success\"\n    echo \"SIZE $width 변환 생략 파일 개수: $count_skip\"\n}\n\n# 함수 실행\n```\n\n### crontab\n\n아래 쉘 스크립트를 crontab에 등록하여 이미지 저장 - 압축 - 등록 과정을 수행한다.\n\n```shell\n#!/bin/bash\n\n# S3에서 이미지를 받아온다\naws s3 sync s3://{S3 이미지 경로} {로컬 이미지 경로}\n\n# 최적화를 진행한다.\n/home/ubuntu/resize-image/bash.sh -s 512 -n \"small\" -d \"{로컬 이미지 경로}\"\n/home/ubuntu/resize-image/bash.sh -s 128 -n \"x-small\" -d \"{로컬 이미지 경로}\"\n\n# 최적화가 완료된 파일을 S3로 업로드한다.\naws s3 sync {로컬 이미지 경로} s3://{S3 이미지 경로}\n```\n\n## 정리\n\n간단하게 배포 시나리오와 별개로 이미지 리사이징을 수행해봤다.\n\n개발 일정이 이미 어느정도 잡혀있고 코드상으로 추가하기 어려운 부분이라면 서버에서 배치로 돌리는것도 고려해보면 좋을 것 같다.\n\n- [github 이미지 리사이징을 위한 코드](https://github.com/pium-official/resize-image)\n\n## Reference\n\n- https://certbot.eff.org/instructions\n- https://sharp.pixelplumbing.com\n"},{"excerpt":"이 글은 우아한테크코스 5기, 피움팀 크루 '쵸파'가 작성했습니다. 서론 피움 서비스는 기존에 EC2 + Nginx로 FE 정적 파일 서빙과 백엔드 API 응답을 하나의 EC2에서 하고 있었습니다. 백엔드와의 관심사 분리를 위해 EC2에서 S3로 FE 정적 파일을 옮기고,\n보안과 CDN 캐시를 적용하기 위해 CloudFront를 통해 배포하였습니다. 더 …","fields":{"slug":"/http-cache/"},"frontmatter":{"date":"September 21, 2023","title":"CloudFront와 HTTP 캐시","tags":["CloudFront","HTTP Cache"]},"rawMarkdownBody":"\n> 이 글은 우아한테크코스 5기, 피움팀 크루 '[쵸파](https://github.com/bassyu)'가 작성했습니다.\n\n## 서론\n\n피움 서비스는 기존에 EC2 + Nginx로 FE 정적 파일 서빙과 백엔드 API 응답을 하나의 EC2에서 하고 있었습니다.\n\n백엔드와의 **관심사 분리**를 위해 EC2에서 **S3**로 FE 정적 파일을 옮기고,\n**보안과 CDN 캐시**를 적용하기 위해 **CloudFront**를 통해 배포하였습니다.\n\n더 나아가서, 같은 리소스는 더 빠르게 접근하고 네트워크 트래픽을 줄이기 위해 **HTTP 캐시**를 적용했습니다.\n\n해당 과정을 어떻게 적용하였는지 공유하고자 합니다.\n\n## S3 + CloudFront 배포\n\n이미 S3와 CloudFront로 배포하는 방법은 [피움 블로그](https://blog.pium.life/aws-s3-apply/)에 잘 기록되어 있습니다!\n이외의 프론트엔드 배포를 위한 차이점만 다루었습니다.\n\n### S3 업로드 자동화 하기\n\n#### aws cli 설치\n\n가장 좋은 방법은 [공식 문서의 설치 방법](https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/getting-started-install.html)을 그대로 따라 하는 것이었습니다.\n피움의 빌드 EC2 환경은 Linux ARM 이어서 해당 명령어를 그대로 따라 해 설치했습니다.\n\n#### 파이프라인 구성\n\n아래의 명령어는 빌드 후 dist 폴더의 파일들을 S3 버킷에 동기화하는(옮기는) 명령어입니다.\n\n```\naws s3 sync ./dist s3://{프로젝트 S3 버킷 경로} --delete\n```\n\n이 1줄만 젠킨스 파이프라인에서 실행하면 됩니다.\n\n#### 빌드 EC2에 권한 주기\n\n이때, 빌드 EC2에서 S3에 업로드할 수 있도록 권한을 줘야 합니다.\n인스턴스 > 작업 > 보안 > IAM역할 수정\n에서 권한을 줄 수 있습니다.\n\n### CloudFront\n\n#### 오류 페이지\n\nSPA로 정적 페이지를 배포하면 최상위뿐만 아니라 다른 path로 이동할 때가 많습니다.\n다른 path에 접근하더라도 index.html을 받을 수 있도록 오류 페이지를 설정해 줍니다.\n\n![error_page](.index_images/1.png)\n\n## HTTP 캐시를 위한 응답 헤더 정책\n\nFE 배포는 성공적으로 마무리되었습니다.\n이제 더 나은 사용자 경험을 위해 **HTTP 캐시**를 적용하고자 합니다.\n\n### HTTP 캐시를 적용하는 이유\n\n똑같은 리소스는 웹페이지에 들어갈 때마다 새로 요청할 필요가 없습니다.\n디스크에 저장해 두었다가 새로운 버전만 요청하면 훨씬 더 빠르지 않을까요?\n네트워크 통신 비용도 크게 줄일 수 있습니다.\n\n[HTTP 캐시](https://developer.mozilla.org/ko/docs/Web/HTTP/Caching)를 활용하면 같은 리소스는 네트워크를 거치지 않고 디스크나 메모리에서 바로 사용할 수 있습니다!\n\n### Cache-Control 헤더\n\n리소스 **응답 헤더의 Cache-Control 필드**를 통해 HTTP캐시를 설정할 수 있습니다.\n그중 Cache-Control: \"max-age=<second>\" 값으로 몇 초간 캐시 할지 설정할 수 있습니다.\n\n피움 서비스에서는 '같은 리소스에 대해 한 달은 캐싱한다'는 정책을 주기 위해 아래와 같이 CloudFront의 응답 헤더 정책을 만들었습니다.\n\n![](.index_images/2.png)\n\n그리고 동작에 추가합니다.\n\n![](.index_images/3.png)\n\n이제 CDN이 보내주는 리소스 헤더에 Cach-Control이 더해져서 HTTP 캐시가 적용됩니다! 🎉\n\n### 캐시 버스팅\n\n클라이언트는 설정해 준 시간이 지나지 않으면 동일한 URL의 GET 요청에는 항상 캐시가 적용됩니다.\n새로운 버전이 업데이트되면 원래의 캐시를 어떻게 지울까요?\n\n피움은 **새로운 버전의 URL이 바뀔 수 있도록 번들에 해시**를 줘서 캐시 버스팅을 적용했습니다.\n\n아래는 번들 파일에 해시를 주는 webpack.config 설정입니다.\n\n```\n// webpack.config.js\n\noutput: {\n  path: path.join(__dirname, 'dist'),\n  filename: '[name].[contenthash].bundle.js',\n  chunkFilename: '[name].[chunkhash].chunk.bundle.js',\n}\n```\n\n이후 빌드된 번들에 해시가 들어간 모습입니다.\n\n![](.index_images/4.png)\n\n그런데 webpack.config를 자세히 보면 chunkhash와 contenthash가 있는 것을 알 수 있습니다.\n둘은 어떤 차이일까요?\n\n#### webpack 해시의 종류\n\n우선 하나의 엔트리 포인트에서 나온 파일들을 청크라고 부릅니다.\nwebpack 설정으로 청크를 어떻게 나눌지는 커스텀할 수 있습니다.\n\nchunkhash는 청크를 기준으로 해시값을 주고, contenthash는 청크가 같더라도 각 파일의 내용을 기준으로 해시값을 줍니다.\n\n만약 하나의 청크에 하나의 파일이 나온다면 chunkhash를 줘도 무방합니다.\n하나의 청크에서 .js .css 처럼 여러 파일이 나온다면 contenthash로 더욱 뾰족하게 해싱할 수 있습니다.\n\n### index.html은 예외\n\nindex.html 파일은 해시를 주면 안됩니다.\n유저가 저희의 서비스를 들어올 때 'www.pium.life/index.hash.html'이런 식으로 요청하지 않기 때문입니다.\n\n다른 리소스와는 달리 파일이 신선하지 않음을 URL로 판단하기 힘들기 때문에\nCloudFront의 동작으로 예외처리해서 해결했습니다.\n\n#### 새로운 동작 생성\n\nindex.html을 위한 동작을 위해 경로 패턴에 '/index.html'을 적어줍니다.\n\n![](.index_images/5.png)\n\n캐시 정책은 CachingDisabled를 사용합니다.\n그리고 응답 헤더 정책은 주지 않습니다.\n\n![](.index_images/6.png)\n\n완성된 모습입니다.\n\n![](.index_images/7.png)\n\n이제 index.html도 최신 버전으로 잘 받아옵니다. 🎉🎉\n\n#### s-maxage + 무효화\n\nHTML에 캐시를 안주는 방법만 있는 것은 아닙니다.\n응답 헤더 정책에서 Cache-Control의 s-maxage를 통해 CDN에서만 캐싱을 적용하고, 배포할 때 무효화를 해줄 수 있습니다.\n\n다만 무효화를 자동으로 하기 위해서는 추가적인 IAM 권한이 필요하고, 속도 차이가 크지 않음을 확인해서 아직 적용하지 않았습니다.\n\n### HTTP 캐시를 한 달로 설정한 이유\n\n설정을 한 달로 하면 한달 후에는 같은 리소스도 다시 검증해야합니다.\n왜 캐싱하는 시간을 최댓값인 1년으로 하지 않았을까요?\n\nHTTP 캐시는 사용자의 디스크나 메모리에 리소스를 저장합니다.\n만약 max-age를 최댓값인 1년으로 설정하면 새로운 버전의 해시가 배포되어도 사용자는 이전 버전의 리소스를 1년간 가지고 있게 됩니다.\n\n그래서 서비스 배포 주기에 맞는 적절한 기간을 논의하여 한 달로 설정하였습니다.\n소중한 고객님이 불필요한 리소스를 1년간 저장할 필요는 없으니까요!\n\n## 정리\n\nHTTP 캐시는 처음 접해보는 분야이고 index.html은 예외를 줘야 한다는 점이 흥미로웠습니다!\n캐시 기간도 길다고 무조건 좋은 게 아니었습니다.\n결과적으로 네트워크 트래픽을 줄이고 더 빠른 사용자 경험을 줄 수 있어서 뿌듯한 경험이 되었습니다.\n\n### 참고한 자료\n\n[웹 서비스 캐시 똑똑하게 다루기](https://toss.tech/article/smart-web-service-cache)\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. 이미지 최적화는 참새가 숟가락 얹었어요. 피움 최적화 하기 어찌어찌 서비스를 개시하긴 했지만, 여러 가지로 문제가 있었습니다. 첫 페이지 로드 시에 미리 이미지 크기를 설정해 놓지 않아서 layout shift가 발생하기도 하고 전체적인 번들 크기가 400kb가 넘기도 하며, 페이지 로드 후에 폰트가 …","fields":{"slug":"/optimize-frontend/"},"frontmatter":{"date":"September 21, 2023","title":"피움 최적화하기","tags":["웹 최적화","bundle","정적 배포"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다. 이미지 최적화는 참새가 숟가락 얹었어요.\n\n\n# 피움 최적화 하기\n\n\n어찌어찌 서비스를 개시하긴 했지만, 여러 가지로 문제가 있었습니다. 첫 페이지 로드 시에 미리 이미지 크기를 설정해 놓지 않아서 layout shift가 발생하기도 하고 전체적인 번들 크기가 400kb가 넘기도 하며, 페이지 로드 후에 폰트가 적용되어서 깜빡이는 현상이 발생하기도 했습니다. http 헤더 설정도 의도적으로 되어 있지 않아서 캐시 기간도 모르고, CDN 적용도 되어있지 않아서 캐시 역시 제대로 되지 않았습니다. 이러한 문제점들이 모여서 결국 사용자의 데이터를 많이 잡아먹고 폰트나 버벅거림 등은 분명한 사용자 경험 저해 요소이기 때문에 이를 해결하기 위한 방안을 세웠습니다.\n\n우선적으로 명시적인 문제를 먼저 해결해야 했습니다. Lighthouse를 통해서 측정을 해본 결과는 다음과 같습니다.\n\n![](./.index_images/before_optimize_lighthouse.png)\n\n생각보다 성능이 높게 나와서 놀랐습니다. 이제부터 세부 항목들을 찬찬히 살펴보겠습니다. \n\n1. 번들의 크기가 너무 크다. \n2. 폰트 로딩에 많은 시간이 소요된다.\n3. 파일들이 캐시되지 않는다\n\n![](./.index_images/before_optimize_detail.png)\n\n위 이미지에서 볼 수 있듯이, `bundle.js`의 파일 크기가 무려 444kib이고, 폰트의 크기 역시 합치면 1mb에 달합니다. 또한 캐시 + TTL에서 보면 알 수 있듯이 캐시를 전혀 사용하지 않고, 매번 새 파일을 요철하고 있습니다. 이는 사용자의 데이터를 그만큼 사용하고, 느린 네트워크 환경에서 속도를 저하시키는 요인이 됩니다. \n\n추가적으로 하나의 큰 문제로, 피움 서비스 내부에서 사용하는 아이콘들에 문제가 있습니다. 피움에서는 모든 아이콘들을 SVG-in-JS방식으로 사용하고 있는데, [SVG-in-JS와 결별 이라는 글](https://kurtextrem.de/posts/svg-in-js)에서 이 방법은 자바스크립트 컴파일 하는 코드의 양을 오히려 늘리는 안티 패턴이라는 글을 봤습니다. 해당 글에서 제안하는 해결 방법은 색 변화 등 props들의 영향을 받지 않는 이미지의 경우에 img태그를 통해 이미지를 로드하고, 그게 아닌 경우에는 `SVGSprite`라는 기술을 사용해서 이미지를 로드 하는 것을 추천하고 있습니다.\n\n따라서 피움은 아래 4가지를 개선함으로써 최적화를 진행했습니다.\n\n- 요청 크기 줄이기\n- 같은 요청 두번 하지 않기\n- 이미지 최적화\n- SVG-in-JS 개선\n\n## 요청 크기 줄이기\n\n### 정적 배포 방식 변경하기\n\n현재 `EC2`를 통해서 전달 받는 bundle의 크기는 444KB입니다. 문제는 이 파일을 어떻게 줄일 수 있을까? 입니다. webpack을 사용하면서 자동으로 압축을 해주기 때문에 analyze로 분석했을 때 줄일 수 있는 부분은 크게 보이지 않았습니다.\n\n![](./.index_images/before_optimize_bundle.png)\n \n현재 받는 parsed size가 pium에서 받는 444kb와 같다는 것을 알 수 있습니다. 아래 있는 Gzipped size가 있는데 이 사이즈는 141kb로 3배 좀 넘게 차이가 나는 크기입니다. 그렇다면 어떻게 Gzipped로 파일을 압축할 수 있을까요?\n\n> **gzip**은 [파일 압축](https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%BC_%EC%95%95%EC%B6%95)에 쓰이는 [응용 소프트웨어](https://ko.wikipedia.org/wiki/%EC%9D%91%EC%9A%A9_%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4)이다. gzip은 GNU zip의 준말이며, 초기 [유닉스](https://ko.wikipedia.org/wiki/%EC%9C%A0%EB%8B%89%EC%8A%A4) 시스템에 쓰이던 [압축](https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%95%95%EC%B6%95) 프로그램을 대체하기 위한 [자유 소프트웨어](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%9C%A0_%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4)이다. - [위키 백과](https://ko.wikipedia.org/wiki/Gzip)\n> \n\n피움은 Gzipped로 변경하는 방법을 서비스 정적 배포 방식의 변경으로 선택했습니다. 기존 배포 방식은 EC2에 빌드한 파일을 올리고 주소창에 도메인을 입력하면 DNS에서 일치하는 도메인을 찾은 뒤 Nginx에서 EC2 서버 인스턴스에 있는 프론트엔드 정적 파일을 배포하는 방식을 사용했었습니다. \n\n![](./.index_images/before_optimize_structure.png)\n\n(빌드 후에 폴더와 파일들을 gzip으로 압축해서 배포하는 방법도 있었지만, 이때는 생각하지 못했습니다)\n\n정적 배포 방식을 EC2에서 S3 + cloudfront로 변경하는 데에는 단순히 gzip만 있는 것도 아닙니다. s3를 사용했을 때 장점을 요약하자면 아래와 같습니다.\n\n- 자동 확장, 축소하기 때문에 특정 크기 할당 필요가 없음\n- 서버리스이기 때문에 파일이 저장되는 서버를 관리할 필요가 없음\n- 애플리케이션에 서버가 필요하더라도 정적 콘텐츠에 대한 요청 처리 필요가 없기 때문에 해당 서버의 크기 줄이기 가능.\n\nEC2는 결국 컴퓨터이기 때문에 만약에 메모리 초과가 발생하거나 모종의 이유로 서버가 다운된다면 피움 페이지로 들어온 사용자들은 흰 화면만 보게 됩니다. 아무것도 모르는 사용자는 어떠한 에러 페이지도 보지 못한 채 서비스를 떠나게 될 가능성이 있습니다. 또한, 단순히 정적 파일만을 제공하고 있는데 굳이 컴퓨터 한 대를 사용할 필요도 없습니다.\n\ns3 + cloudfront를 사용한다면 CDN 캐시를 통해 사용자에게 좀 더 빠르게 서비스 제공이 가능하고, http 캐시 설정이 가능합니다. 또한 앞에서 얘기한 gzip으로 자동 압축도 해주기 때문에 굳이 압축된 파일을 저장하고 있을 필요도 없습니다. (아래와 같이 cloudfront 설정할 때 자동으로 객체 압축을 설정해 주면 됩니다.)\n\n\n![](./.index_images/compress_cloudfront.png)\n\n> CDN(Contents Delivery Network)은 콘텐츠 전송 네트워크를 나타내는 약자로, 웹 콘텐츠와 웹 애플리케이션을 효율적으로 제공하기 위한 분산 네트워크입니다. CDN은 전 세계의 다양한 위치에 위치한 서버와 캐시 노드를 사용하여 웹 콘텐츠를 저장하고 전송함으로써 웹 성능을 최적화하고 가용성을 향상 시킵니다.\n> \n\n좀 더 자세한 내용은 쵸파가 작성한 [Cloudfront와 HTTP캐시](https://blog.pium.life/http-cache/)에 있습니다.\n\n### 코드 스플리팅\n\n다운로드 하는 번들의 크기를 줄인다고 해도 그 다운로도 된 번들의 코드 중에서 사용하지 않는 코드가 있다면 이 역시 불필요하게 번들의 크기를 늘리는 요인이 됩니다. `React`를 통해서 서비스를 개발한 피움은 `[SPA](https://en.wikipedia.org/wiki/Single-page_application)`라는 특징을 갖고 있기 떄문에 하나의 번들에 모든 JS 코드가 들어있습니다. 이 때문에 첫 로딩이 상대적으로 오래걸린다는 단점이 있는데, `l[azy`를 통한 동적 `import`](https://react-ko.dev/reference/react/lazy)를 통해서 url에 알맞는 컴포넌트만 동적으로 불러올 수 있습니다.\n\n여기서 “어떤 페이지를 동적으로 불러 올까?” 라는것에 대해서 여러 이야기를 나눴습니다. 저는 모든 페이지를 동적 `import`를 통해서 파일을 불러오는게 맞다라고 생각했습니다. 그 이유는 사용자들이 어떤 페이지를 들어갈 지 모르는데, 굳이 불필요한 리소스 다운이 될 수 있다는 것이었습니다. 하지만 다른 팀원은 그렇게 전부 나눠버리면 `SPA`의 장점이 사라지지 않는가, 결국 처음에 로딩이 좀 걸리더라도 다른 페이지에서는 빠르게 이동하는게 장점인데 오히려 모두 나눠버린다면 페이지 로드에 시간이 걸려서 사용자 경험을 해칠 수도 있고, `React`를 사용하는 이유가 없지 않나? 였습니다. \n\n따라서 사용자가 많이 접근할 것 같이 않은 페이지를 동적으로 불러오고, 핵심 페이지라고 생각되는 부분들은 `bundle`에 한번에 넣어서 오기로 했습니다. 아직 페이지 방문 횟수에 대한 믿을만한 데이터가 존재하지 않아서 사용자가 많이 왔다 갔다 할 것 같지 않은 `login`, `loginAuth`, `myPage` 페이지 만을 동적으로 불러오도록 설정했습니다.\n\n```tsx\n// router.tsx\nconst Login = lazy(() => import(/* webpackChunkName: \"Login\" */ 'pages/Login'));\nconst Authorization = lazy(\n  () => import(/* webpackChunkName: \"Authorization\" */ 'pages/Login/Authorization')\n);\nconst MyPage = lazy(() => import(/* webpackChunkName: \"MyPage\" */ 'pages/MyPage'));\n```\n\n### 폰트 용량 줄이기\n\n그 다음 용량을 많이 차지하는 파일이 폰트 용량이었습니다. 2가지 폰트([GmarketSans, GmarketSansBold](https://corp.gmarket.com/fonts/))를 사용했는데, 두 개 합쳐서 거의 1MB에 육박했습니다. 원래 폰트는 [NanumSquareRound](https://hangeul.naver.com/font) 였는데, 해당 폰트는 뒑, 뿗, 똟과 같이 사용되지 않는 한글을 지원하고 있지 않았습니다. 따라서 실제로 입력하게 된다면 공백이 나오기 때문에 사용성이 좋지 않다는 생각했습니다. 따라서 차선책이었던 GmarketSans를 적용했었습니다.\n\n![](./.index_images/nanumsquareround_fail.png)\n\n\n하지만, GmarketSans가 자주 사용하지 않는 한글을 지원한다는 것 자체가 쓸데없는 리소스가 많다는 의미기도 했습니다. 그래서 기본 폰트를 사용하되, `font-display:swap`을 사용해서, 폰트 로드를 했을 때 해당 폰트가 존재하지 않는다면 대체 폰트를 보여주는 방식을 사용했습니다. \n\nfont-display 속성은 폰트를 선언하는 @fontface 블록 안에서만 유효한 속성입니다. 따라서 사용자가 @fontface를 선언하는 css 파일에서 별도로 지정해줘야 합니다. 피움에서는 처음에 `cdn`으로 제공하는 폰트를 사용했었는데, 해당 폰트에 불필요한 폰트들도 존재하고 개인적으로 사용되는 폰트들도 아니어서 새로운 `.css`파일을 만들고 그 안에서 [폰트 확장자 설정](https://github.com/woowacourse-teams/2023-pium/pull/375/commits/a86981e758f85325cafede6cf315208daaa1da26#diff-48658425b08ff6807ec0e40ecb127dd74d3497168f1805453435d99baf78da1b)을 했습니다. \n\n하지만 `NanumSquareRound`에 큰 문제(?)가 있었습니다. `font-display:swap`을 설정했는데도 지원하지 않는 문자를 설정했는데 적용되지 않았습니다. 이게 무슨일인가… 내가 어떤 잘못을 했나? 라는 생각을 하면서 문제를 찾고 있었는데 옆에 있던 참새가 아주 엄청난 것을 알려줬습니다. 바로, 나눔 스퀘어 라운드에서는 지원하지 않는 글자들의 경우에 공백을 나타낸다는 것입니다…!!\n\n![](./.index_images/font_empty.png)\n\n\n(둘, 둠, 둡 등의 사용되는 단어들은 존재하는데, 제공하지 않는 단어는 존재하긴 하지만 공백을 제공합니다. [fontdrop](https://fontdrop.info/#/?darkmode=true))\n\n `NanumSquare`에도 같은 문제가 있었고, 그것을 해결한 방법이 있는 [블로그에서](https://github.com/blood72/NanumSquare) 제공한 방식을 따라서 참새가`NanumSquareRound`에서 제공하지 않는 공백들을 개인적으로 제거한 커스텀 폰트를 만들어서 사용할 수 있었습니다. (참새 고맙습니다.)\n\n![](./.index_images/font_success.png)\n\n\n참새 덕분에 지원하지 않는 폰트의 경우에 대체 폰트로 지정할 수 있었고, 용량이 더 작고 원래 선택 했었던 폰트로 되돌아 갈 수 있었습니다.\n\n### 폰트 preload 하기\n\n![](./.index_images/preload_fail.gif)\n\n\n용량과는 큰 상관이 없지만, 폰트 다운로드가 페이지 로드가 완료된 다음에 실행 되었기 때문에 영상과 같이 처음 로드 했을때 혹은 새로고침 했을 때 폰트가 깜빡하는 문제가 있었습니다. 이는 사용자 경험에 매우 좋지 못하다는 생각에 어떻게든 해결을 해야 하는 문제 중 하나 였습니다. \n\n폰트 `preload`설정은 생각보다 쉬웠습니다.(끝나고 보니 쉬웠습니다…) 처음에는 단순히 `link`태그에 `preload`만 추가하면 되는 줄 알았는데, css로 폰트를 다운 받고, 해당 폰트에 `preload`를 적용하면 되는 것이었습니다. 처음에는 폰트 CDN을 통해서 다운로드 받는 방식을 했었는데, 폰트의 캐시 설정도 되지 않고 저희의 통제를 벗어난 외부 폰트라는 생각에 한번에 처리할 수 있으면 좋겠다는 생각에 사용하는 폰트 자체를 다운받고, 해당 .css 확장자에 폰트 url을 설정함으로써 preload를 구현할 수 있었습니다.\n\n```html\n// index.html\n<link\n  href=\"/fonts/NanumSquareRoundMissingGlyph.woff2\"\n  rel=\"preload\"\n  as=\"font\"\n  type=\"font/woff2\"\n  crossorigin\n/>\n<link href=\"/fonts/font.css\" rel=\"stylesheet\" type=\"text/css\" />\n```\n\n```css\n@font-face {\n  font-family: 'NanumSquareRound';\n  font-weight: 500;\n  font-style: normal;\n  font-display: swap;\n  src: url('/fonts/NanumSquareRoundMissingGlyph.woff2') format('woff2');\n}\n```\n\n![](./.index_images/preload_success.gif)\n\n\n대체 폰트 설정과 폰트 `preload`를 통해서 용량을 줄이고 사용자 경험을 좀 더 끌어 올릴 수 있었습니다. 궁극적으로 gzip압축, 코드 스플리팅, 폰트 용량 줄이기 이 3가지를 통해서 압축한 용량은 다음과 같습니다.\n\n![](./.index_images/preload_fail.png)\n\n![](./.index_images/preload_success.png)\n\n455 + 487 + 658 = `1542` → 99.9 + 40.7 + 232 = `372.2`\n\n거의 4배 가까이 되는 용량을 줄 일 수 있었습니다. 여기에 추가적으로 `preload`를 통해서 사용자 경험 향상까지 시킬 수 있었습니다.\n\n## 같은 요청 두 번 하지 않기\n\n짐을 한번 가져왔는데, 그 짐을 다시 가져오는 것만큼 비효율적인 것은 없습니다. 이는 데이터 통신에도 똑같이 적용됩니다. 한번 데이터(짐)을 가져오고 나면 해당 데이터를 어딘가에 저장해 둘 수 있으면 리소스를 그만큼 줄일 수 있습니다. 그리고 프로그래밍에서는 이것을 캐시라고 합니다.\n\n> **캐시**(cache, [문화어](https://ko.wikipedia.org/wiki/%EB%AC%B8%ED%99%94%EC%96%B4): 캐쉬, 고속완충기, 고속완충기억기)는 [컴퓨터 과학](https://ko.wikipedia.org/wiki/%EC%BB%B4%ED%93%A8%ED%84%B0_%EA%B3%BC%ED%95%99)에서 데이터나 값을 미리 복사해 놓는 임시 장소를 가리킨다. 캐시는 캐시의 접근 시간에 비해 원래 데이터를 접근하는 시간이 오래 걸리는 경우나 값을 다시 계산하는 시간을 절약하고 싶은 경우에 사용한다. 캐시에 데이터를 미리 복사해 놓으면 계산이나 접근 시간없이 더 빠른 속도로 데이터에 접근할 수 있다.\n> \n\n저희는 캐시 설정을 할 수 있습니다. 코드 용량을 줄이기 위해 적용했던 s3+cloudFront를 통한 정적 배포에서 CDN 캐시 설정과 http 캐시 설정이 가능합니다.\n\n> **HTTP 캐시**는 `Cache-Control`에 값을 설정해 줌으로써 해당 리소스 파일의 캐시 주기를 설정해 줄 수 있습니다. HTTP 캐시에서는 `max-age`라는 설정 값을 통해 해당 리소스의 유효기간을 정할 수 있습니다. (Expires 역시 동일하지만, max-age가 우선시 됩니다.)만약에 설정한 캐시 만료 전에 같은 요청이 들어온다면, 메모리에 저장되어 있는 기존 값을 보여줌으로써 서버 요청을 최소화 할 수 있습니다.\n> \n\n여기서 **같은 요청**이란 같은 DNS에서 같은 파일 명을 요청하는 것을 의미합니다. 하지만, 버전을 업데이트 하거나 새롭게 배포하는 경우에 동일한 이름의 파일이 업로드 될 수 있는데, 저희는 배포를 할 때 `index.html`파일을 제외하고 모든 파일명과 확장자 사이에 랜덤한 해시값을 넣음 으로써 서로 다른 파일임을 알려주는 캐시 버스팅을 사용했습니다.\n\n```jsx\n// webpack.config.js\noutput: {\n  path: resolve(__dirname, 'dist'),\n  filename: '[name].[contenthash].bundle.js',\n  chunkFilename: '[name].[chunkhash].chunk.bundle.js',\n  assetModuleFilename: 'assets/[name][ext]',\n}\n```\n\nHTTP 캐시 정책은 각자 서비스에 맞춰서 설정하면 됩니다. 피움에서는 이 HTTP 캐시 설정일을 한 달로 설정했습니다. 즉, 한 달 동안은 브라우저에서 메모리에 파일들을 저장해 뒀다가 한 달 뒤에는 다시 CDN으로부터 정적 파일 정보를 받아오는 것입니다. 모든 파일에 대해서 HTTP 캐시 설정을 했지만 `index.html` 파일에 대해서는 캐시 설정을 하지 않았습니다.\n\n왜냐하면 `index.html` 파일에는 해시값을 추가할 수 없기 때문입니다. 따라서 데이터 요청을 할 때 `index`파일은 매번 S3에서 가져오고, 나머지 파일들은 HTTP 캐시를 통해 최적화했습니다. index 파일의 경우에 CDN 캐시도 설정할 수 있었지만, 현재 I AM 권한이 없어서 CDN 캐시 자동 무효화를 할 수 없습니다. 따라서 CDN 캐시 설정을 하는 대신에 배포할 때마다 수동으로 무효화하기 vs 매번 S3에서 새로운 요청 하기 둘 중 하나를 선택했어야 했는데, `index` 파일의 경우 성능상 문제가 있지 않아서 매번 S3에서 받아오는 형식을 선택했습니다.\n\n![](./.index_images/before_cache.png)\n\n![](./.index_images/after_cache.png)\n\n빠른 3G환경에서 캐시를 사용하지 않았을 때는 로드되는데 까지 3.36초가 걸린 반면에, 캐시를 사용했을 때 로드되는데 까지 0.752초가 걸리는 것을 볼 수 있습니다.\n\n## 이미지 최적화\n사진은 정말 여러 곳에서 사용됩니다. 크게는 사전 식물이나 반려 식물의 상세 정보부터 작게는 검색창 아래쪽에 나타나는 검색 결과 목록처럼요. 하지만 이 사진들은 전부 '원본 자료'를 요청하고 있었습니다. 가로세로 32픽셀 크기인 곳에서도 원본 사진을 받아왔죠. 그래서 사이트 첫 로딩에 사용하는 자원이 많았습니다.\n\n따라서 작은 사진이 필요한 곳에서는 작은 해상도의 사진을 받아와서 통신 비용을 절약해보자! 라는 생각을 하게 되었죠. 처음에는 Amazon CloudFront의 Lambda@Edge 기능을 활용해서 클라이언트 측에서 특정 해상도의 사진을 요청하면 원본을 변형해서 주는 방법을 적용하고 싶었습니다. 하지만 아쉽게도 Lambda 권한을 허락받지 못해 이 방법을 사용할 수는 없었어요. 그래서 임시로 지금 저장되어있는 사전 식물의 사진들만이라도 수동으로 처리해보자는 결론을 내렸습니다.\n\n방법은 크게 세 단계로 나눌 수 있습니다.\n\n1. S3에 저장된 식물 사진을 로컬로 다운로드\n2. 로컬에서 `node.js`를 이용해서 사진 변환\n3. 변환한 사진을 다시 S3에 업로드\n\n저희는 권한이 없어서 S3에서 바로 자료를 받아올 수는 없어서 EC2를 중간 다리로 활용해 S3에서 EC2로, EC2에서 다시 로컬로 옮기는 방법을 썼어요.\n\n사진 변환은 가장 많이 사용되는 `sharp` 라이브러리를 썼습니다. 가로세로 64픽셀의 초소형과 256픽셀의 소형 사진을 png, webp 형식 두 가지로 변환했어요. 사용한 코드는 아래와 같습니다.\n\n```js\nimport Sharp from 'sharp'; // 이 패키지는 npm으로 설치가 필요해요\nimport fs from 'fs';\nimport { join } from 'path';\n\nconst SMALL = {\n  width: 256,\n};\n\nconst X_SMALL = {\n  width: 64,\n};\n\nconst INPUT_DIR = './static';\nconst OUTPUT_DIR = './output';\n\nconst getFileNames = (dir) => fs.readdirSync(dir);\n\nconst convertImageTo = async (filename, width, type, format) => {\n  const inputPath = join(INPUT_DIR, filename);\n  const filenameWithoutExt = filename.split('.')[0];\n\n  const sharp = new Sharp(inputPath);\n  const { width: imageWidth } = await sharp.metadata();\n\n  if (imageWidth <= width) {\n    console.log(`${filename} image width(${imageWidth}) is smaller than target size(${width}).`);\n    return false;\n  }\n\n  await sharp\n    .resize(width)\n    .toFormat(format.toLowerCase(), { quality: 100 })\n    .toFile(join(OUTPUT_DIR, `${filenameWithoutExt}.${type}.${format.toLowerCase()}`));\n\n  return true;\n};\n\nconst filenames = getFileNames(INPUT_DIR);\n\nfilenames.forEach(async (filename) => {\n  await convertImageTo(filename, SMALL.width, 'small', 'png');\n  await convertImageTo(filename, SMALL.width, 'small', 'webp');\n  \n  await convertImageTo(filename, X_SMALL.width, 'x-small', 'png');\n  await convertImageTo(filename, X_SMALL.width, 'x-small', 'webp');\n});\n```\n\n정말 단순무식하긴 하지만 가장 확실한 방법이었습니다.\n\n![](./.index_images/image-resize.png)\n\n그 결과 파일의 용량을 어마무시하게 줄일 수 있었어요!\n\n하지만 이 방법은 어디까지나 임시방편이라 나중에는 백엔드와의 협업으로 새로운 방법을 사용하는 게 맞겠다는 생각이 듭니다.\n\n\n## SVG-in-JS 개선\n피움에서는 여러 가지 아이콘들을 사용하고 있습니다. 그래서 처음에는 `react-icons`를 통해서 아이콘을 좀 편하게 사용하려고 했었는데 [다음과 같은 문제](https://blog.pium.life/bundle-analyze/)가 있었습니다. 지금 돌이켜 보면 `typescript` 컴파일 옵션을  CommonJS로 했기 때문에 icons에 있는 하나의 모듈이 전부 다운로드 돼서 사용하지 않는 아이콘들까지 같이 import 하는 문제였습니다. 웹팩에서는 자동으로 트리 쉐이킹을 해주기 때문에 일어날 수 없는 문제인데 CommonJS로 받아오게 되면서 트리 쉐이킹이 적용되지 않았던 것입니다. 즉, `react-icons`를 삭제하지 않고 `tsconfig` 파일에서 `module`을 `esnext`나 `target`에 맞춰서 넣으면 되는 해결할 수 있는 문제였는데, 당시에는 이것을 몰라서 라이브러리를 삭제하고 `[icons](https://icones.js.org/collection/all?s=p)`라는 사이트에서 아이콘들을 리액트 컴포넌트로 받아서 렌더링 하는 방식으로 적용했습니다.\n\n그렇게 작업을 하고 있던 와중에 [SVG-in-JS와의 결별](https://kurtextrem.de/posts/svg-in-js) 이라는 글을 봤습니다. 글의 요지는 다음과 같습니다.\n\n- SVG 코드가 JS로 들어있다면 불필요한 번들의 크기가 커진다. 즉 파싱 & 컴파일할 게 많아진다.\n- 자바 스크립트가 실행되는 동안 파싱된 내용이 힙 메모리 내에 존재할 텐데, 다른 어플들을 실행하고 있다면 힙 메모리에 영향을 미칠 수 있다.\n\nsvg는 javascript코드가 아닌 xml코드에 가깝습니다. 하지만 그 코드를 JS 파서로 파싱하는 것 자체가 어색한 일 입니다. 따라서 bundle 크기 축소와 사용자 메모리 최적화를 위해서 리액트 컴포넌트 형식으로 선언한 icon 들을 svg로 따로 분리했습니다.\n\n피움에서 사용하는 아이콘의 개수는 거의 40개에 육박합니다. 그렇다고 40개가 넘는 아이콘들은 전부 정적 파일로 저장하게 된다면 파일 용량이 상당히 커지게 됩니다. 그것 또한 사용자 성능에 좋지 못한 요소가 될 수 있습니다.\n\n![](./.index_images/svg_algorithm.png)\n\n위 의사 결정 표(?)를 보면 SVG를 사용할 때 그 용도에 따라 어떻게 설정하는지에 대한 가이드라인이 나와 있습니다. 4kb 미만이면서 폴더 안에 있는 경우에는 inline으로 쓰지만, 4kb 이상이고 정적인 색을 가지고 있다면 img 태그, 그게 아니라면 `SVG sprite`와 `use` 키워드를 사용하라고 했습니다.\n\n`SVG sprite`의 사용법은 간단합니다.\n\n```tsx\n<svg xmlns=\"http://www.w3.org/2000/svg\">\n    <defs>\n      <symbol id=\"account-circle\" viewBox=\"0 0 24 24\">\n\t\t\t//...\n\t\t\t</symbol>\n\t\t\t// + 필요한 만큼 존재\n\t\t</defs>\n</svg>\n```\n\n사용하고자 하는 `svg` 파일들의 `path`를 고유 `id`를 가진 `symbol`들로 선언합니다.\n\n```tsx\nconst SvgFill = ({ icon, size = 24, color = theme.color.gray, ...rest }: SvgIconsProps) => {\n  return (\n    <svg fill={color} width={size} height={size} {...rest}>\n      <use href={`#${icon}`} />\n    </svg>\n  );\n};\n```\n\n그다음에 `svg` 태그 안에 `<use />`태그를 넣고, `href`에 `symbol`로 지정한 `id` 값을 할당하면 끝입니다. 여기서 해당 `svg`의 `props`들을 인자로 받아서 설정할 수 있습니다. 만약에 사용하고자 하는 색이나 크기가 있다면 다음과 같이 선언할 수 있습니다.\n\n```tsx\n<SvgFill\n  icon=\"flowerpot\"\n  color={primaryColor}\n  aria-label=\"화분\"\n  aria-describedby=\"반려 식물이 담긴 화분의 재질\"\n  size={20}\n/>\n```\n\n`svg`를 리액트 컴포넌트처럼 사용하기 위해서는 `webpack`에 플러그인을 하나 추가해 줘야 하는데, 바로 `svgr`입니다. \n\n```jsx\n// webpack.config.js\n{\n  test: /\\.svg$/,\n  include: [resolve(__dirname, 'src'), resolve(__dirname, 'src', 'types', 'module.d.ts')],\n  issuer: /\\.[jt]sx?$/,\n  use: ['@svgr/webpack', 'url-loader'],\n},\n```\n\n또한, `typescript`에서 `svg`의 타입이 보통은 `string`이기 때문에 해당 타입이 `ReactComponent`라고 모듈을 선언해 줘야 합니다.\n\n```tsx\ndeclare module '*.svg' {\n  import React from 'react';\n  export const ReactComponent: React.FunctionComponent<React.SVGProps<SVGSVGElement>>;\n  const src: string;\n  export default src;\n}\n```\n\n하지만 위 코드를 보면 약간 의아한 게 있습니다. `React Component`와 `src`를 같이 반환하고 있다는 것입니다. SVG Sprite를 사용하기 위해서는 svg 확장자를 리액트 컴포넌트처럼 사용해야 하는데, 그 반환 값을 커스텀 타입으로 지정해 준 것입니다. 하지만 아예 `ReactCompnent만`을 반환하게 한다면 `<img/>`태그에 `src` 속성에 정적인 `svg` 주소를 넣을 수 없는 문제가 있었습니다. 따라서 `React Component`와 `src` 모두 지원하기 위해서 2가지를 `export` 하도록 설정했습니다.\n\n하지만, 위처럼 2가지를 export 하도록 바꾸니 코드 스플리팅이 제대로 되지 않는 것 같았습니다. 만약에 정적인 데이터를 불러오기 위해서 `assets` 내부에 있는 `.svg`확장자를 불러오고 번들 크기 분석을 해봤는데, 평소보다 2배가 더 큰 용량을 차지하고 있는것을 볼 수 있었습니다.\n\n사용하지 않는 이미지를 불러오는 것을 보고 왜 트리 쉐이킹 안되지? 하는 생각을 했습니다 (애초에 이게 트리 쉐이킹이 맞는지도 잘 모르겠습니다.) 하지만 이미지를 svg가 아닌 png로 변경하니 바로 문제 해결이 됐습니다. 이미지 품질 자체에 문제가 있을까 걱정했지만, 정적으로 제공하는 이미지들이 해상도가 아주 중요한 이미지들이 아니기 때문에 문제가 없을 것으로 생각해서 png로 제공하기로 했습니다.\n\n그다음 문제는 SVG symbol들을 저장하는 코드의 위치였습니다. SVG Sprite를 사용하는 이유가 번들 크기를 줄이기 위해서인데, symbol들을 저장하는 소스 코드 파일 자체가 리액트 컴포넌트 형식으로 작성되어 있어서 이게 맞냐고 생각을 했습니다. 그래서 svg 확장자로 설정하고 CDN으로 받아오려고 시도해 봤지만, cors 문제로 인해 불가능했습니다.\n\n> SVG `<use>` elements don’t currently have any way to ask for cross-origin permissions. They just don’t work cross-origin, at all. - [cors](https://oreillymedia.github.io/Using_SVG/extras/ch10-cors.html#:~:text=SVG%20%3Cuse%3E%20elements%20don%E2%80%99t%20currently%20have%20any%20way%20to%20ask%20for%20cross-origin%20permissions.%20They%20just%20don%E2%80%99t%20work%20cross-origin%2C%20at%20all.)\n> \n\n따라서 html에 직접 삽입하는 방법밖에 없었는데, 그렇게 된다면 icon이 추가될 때마다 `index`파일을 바꿔야 하는 번거로움이 있었습니다. SPA로 구현하고 있는 상황에서 불특정한 상황에 따라 `index` 파일이 바뀌는 게 괜한 거부감이 있어서 현재는 리액트 컴포넌트로 삽입하고 있는데, 이 방법이 맞는지는 여전히 의문이긴 합니다. (icon 같은 경우에는 그렇게 많이 바뀌는 것이 아니라 상관이 없을 것 같다는 생각이 있어서 그냥 넣어도 되지 않을까 생각하기도 합니다.)\n\n또 하나의 아쉬운 점은 SVG Sprite를 사용함으로써 파싱할 코드의 양이 줄어들어서 사용자에게 감동을 주는 정도로 변화가 있을까 생각했지만, 번들 크기는 30kb가 줄었고 (압축 전) 기능상 큰 변화는 없는 것 같아서 약간 아쉬운 결과를 남겼습니다. 최적화 수업 때 망치를 들면 뭐든지 못으로 보인다고 했는데 저는 오함마를 들고 서비스를 다지려고 하지 않았나 생각이 들었습니다."},{"excerpt":"이 글은 우테코 피움팀 크루 '그레이'가 작성했습니다. 서론 피움 서비스의 런칭 페스티벌을 무사히 마쳤습니다. 런칭 일정에 맞춰 빠르게 개발을 진행하다 보니 꼼꼼하지 못한 부분이 존재했고, 다음 개발에 착수하기 전 부족한 부분을 개선하고 넘어가기로 했습니다. 먼저 서비스 내에서 발생하는 모든 쿼리를 정리해보았습니다. 현재 서비스의 모든 기능을 인수 테스트…","fields":{"slug":"/query-analysis/"},"frontmatter":{"date":"September 21, 2023","title":"서비스 내에서 발생하는 쿼리를 분석하고 개선하기","tags":["쿼리 분석","SQL"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[그레이](https://github.com/kim0914)'가 작성했습니다.\n\n\n## 서론\n피움 서비스의 런칭 페스티벌을 무사히 마쳤습니다.\n\n\n\n런칭 일정에 맞춰 빠르게 개발을 진행하다 보니 꼼꼼하지 못한 부분이 존재했고, 다음 개발에 착수하기 전 부족한 부분을 개선하고 넘어가기로 했습니다.\n\n\n\n먼저 서비스 내에서 발생하는 모든 쿼리를 정리해보았습니다.\n\n현재 서비스의 모든 기능을 인수 테스트로 작성했기 때문에 인수 테스트를 하나씩 돌려보며 발생하는 쿼리를 추적했습니다.\n\n\n\nJPA를 사용하는 환경에서 스프링 properties 설정 중 아래 옵션을 설정하면 쿼리를 확인할 수 있습니다.\n\n```properties\nspring.jpa.show-sql=true\nspring.jpa.properties.hibernate.format_sql=true\n```\n\n![](.index_images/img.png)\n\n현재 서비스에서 사용하고 있는 API에서 발생하는 쿼리를 위와 같이 정리하였고 개선이 필요한 부분, 개선이 불필요한 부분을 먼저 구분했습니다.\n\n\n\n개선이 필요한 원인들은 아래와 같습니다.\n\n\n- batch로 진행할 수 있는 작업을 단건 질의 여러 번 실행\n\n- DB 인덱스를 적절히 사용하고 있지 않음\n\n- PK를 이용해 조회/삭제/수정을 할 수 있음에도 PK를 사용하지 않고 다른 컬럼을 이용해 질의\n\n- N+1 발생\n\n- Pagable 사용 시 불필요한 count 쿼리 발생\n\n- 애플리케이션 단에서 처리할 수 있는 로직을 DB로 질의\n\n\n\n생각보다 개선할 수 있는 부분이 많았습니다.\n\n개발 당시에 꼼꼼하게 코드 리뷰를 했다고 생각했었지만...!! 쿼리를 더 꼼꼼히 확인하지 못했던 점이 아쉬웠습니다.\n\n위와 같은 문제가 발생한 이유를 하나씩 알아보았습니다.\n\n##  batch로 진행할 수 있는 작업을 단건 질의 여러 번 실행\n![](.index_images/img_1.png)\n사용자가 서비스를 탈퇴하면 사용자의 반려 식물을 모두 삭제하고 있습니다. 반려 식물을 삭제하는데 반려 식물 개수만큼 delete 쿼리가 발생한다는 점을 파악했습니다. 또한 반려 식물이 삭제되는 경우 해당 반려 식물의 히스토리까지 함께 삭제하고 있기 때문에 히스토리 개수 만큼 delete 쿼리가 발생한다는 점도 함께 파악했습니다. 히스토리는 서비스 내에서 가장 많이 생성되는 정보이고 생성 주기가 짧은 만큼 많은 데이터가 저장되기 때문에 반드시 개선이 필요한 부분입니다.\n\n\n\n이 문제의 원인은 JPA의 deleteAll 메서드에 있었습니다. JPA의 deleteAll은 iterator 방식으로 동작하며 엔티티를 삭제하기 전 DB에 조회 쿼리를 실행한 후 삭제합니다. 즉, 50개의 데이터를 삭제한다고 했을 때 50개의 조회 쿼리, 50개의 삭제 쿼리가 발생한다는 것입니다. 또한 히스토리도 함께 삭제하고 있기 때문에 훨씬 더 많은 쿼리가 발생하고 있습니다.\n\n\n\n삭제 시 조회가 발생하는 이유는 영속성 컨텍스트와 관련 있다는 생각이 들었습니다. 만약 엔티티가 cascade나 orphanRemoval과 같은 다른 엔티티의 생명 주기와 관련이 되어 있는 경우, 해당 엔티티의 영속화가 필요하기 때문에 조회를 한다고 생각합니다.\n\n\n\n현재 반려 식물 엔티티는 다른 엔티티의 생명 주기와 관련이 없으므로 굳이 영속성 컨텍스트에 로드할 필요가 없습니다. 그러므로 **JPA에서 제공하는 delete를 사용하지 않고 queryDsl의 batch delete를 이용해 한번에 삭제하도록 쿼리를 개선**했습니다.\n\n## DB 인덱스를 적절히 사용하고 있을까?\n서비스 내에서 발생하는 쿼리들을 조사하며 인덱스는 적절히 타고 있는지도 함께 알아보았습니다.\n\nEXPLAIN 명령어를 이용해 쿼리를 하나씩 실행해 보면 해당 쿼리의 실행 계획을 확인할 수 있습니다.\n\n```sql\n+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+\n|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra      |\n+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+\n|1 |SIMPLE     |d1_0 |null      |ALL |null         |null|null   |null|297 |11.11   |Using where|\n+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+\n```\n\n위 실행 계획은 **사전 식물 검색 쿼리의 실행 계획**입니다.\n\n사전 식물 검색 쿼리는 `like`를 이용해 다음과 같이 작성되고 있고 DB full scan을 하고 있습니다. \n\n![](.index_images/img_2.png)\n\nFull scan을 하기 때문에 데이터가 많아지면 조회 성능이 느려질 수 있다고 판단했습니다. 또한 검색 서비스는 비로그인 사용자도 할 수 있고 서비스의 메인 페이지이기 때문에 개선이 필요하다고 생각했습니다.\n\n\n\n검색 기능에서 적용할 수 있는 인덱스인 FullText 인덱스를 적용한 후 테스트를 진행했습니다.\n\n```sql\nCREATE FULLTEXT INDEX plant_name ON dictionary_plant(name) WITH PARSER ngram;\n```\n\n```sql\n+----------------+----------+----------+------------+-----------+---------+-----------+--------+------+----+----------+-------+-------------+-------+----------+\n|Table           |Non_unique|Key_name  |Seq_in_index|Column_name|Collation|Cardinality|Sub_part|Packed|Null|Index_type|Comment|Index_comment|Visible|Expression|\n+----------------+----------+----------+------------+-----------+---------+-----------+--------+------+----+----------+-------+-------------+-------+----------+\n|dictionary_plant|0         |PRIMARY   |1           |id         |A        |317        |null    |null  |    |BTREE     |       |             |YES    |null      |\n|dictionary_plant|1         |plant_name|1           |name       |null     |317        |null    |null  |YES |FULLTEXT  |       |             |YES    |null      |\n+----------------+----------+----------+------------+-----------+---------+-----------+--------+------+----+----------+-------+-------------+-------+----------+\n```\n\n인덱스를 적용하고 동일한 쿼리를 실행했습니다. 현재 사전 식물은 약 350개입니다.\n\n![](.index_images/img_3.png)\n\n위가 FULLTEXT `INDEX`로 조회한 결과이고, 아래가 `like` 쿼리로 조회한 결과입니다. \n\n인덱스를 설정했음에도 불구하고 like 쿼리의 실행 결과와 큰 차이가 나지 않음을 확인했습니다. 데이터 개수가 많지 않기 때문에 큰 차이가 발생하지 않았습니다.\n\n\n\n`0.0002s`의 차이는 무의미한 차이라고 판단했고, **사전 식물의 경우 데이터가 아무리 많아도 1000개 이상 생길 수 없다고 판단**했습니다. \n\n\n또한 인덱스를 사용하는 경우 **DB 내에서 인덱스를 위한 별도의 페이지(저장 공간)가 필요하다는 오버헤드도 존재하기 때문에 like 쿼리를 그대로 사용하는 쪽으로 결정**했습니다.\n\n## N+1\n\n![](.index_images/img_4.png)\n\n반려 식물을 조회하는 과정에서 사전 식물 정보를 함께 제공해주고 있습니다. 반려 식물이 사전 식물을 Lazy 하게 가지고 있기 때문에 반려 식물을 조회하고 정보를 응답하는 과정에서 사전 식물을 조회하는 추가적인 쿼리가 발생했습니다.\n\n\n\n조회 기능이 다른 기능보다 압도적으로 많이 발생하는 저희 서비스에서는 반드시 해결하고 넘어가야 하는 문제였습니다. \n위와 같은 N+1이 발생했던 `히스토리 단건 조회-(기본, 필터링, 페이징)`, `반려 식물 조회`, `리마인더 조회` API에서 사용하는 조회 쿼리를 **fetch join을 이용해 한 번에 조회하여 가져올 수 있도록 변경**했습니다.\n\n\n## PK를 사용하지 않고 다른 컬럼을 이용해 질의\n![](.index_images/img_6.png)\n![](.index_images/img_5.png)\n\n회원을 삭제 기능에서 회원 ID로 삭제하지 않고 unique 컬럼인 kakaoId로 조회한 후 회원을 삭제하고 있음을 확인했습니다.\n\n\n\n회원 객체가 이미 영속화되어 있는 상황에서 PK가 아닌 unique 컬럼으로 조회 후 삭제 질의를 날렸기 때문에 추가적인 select가 발생했고 **PK를 이용해 삭제하도록 변경**했습니다. (OSIV가 켜져 있기 때문에 withdraw 메서드가 호출되는 시점에 member는 영속화가 되어 있습니다)\n\n## Pagable 사용 시 불필요한 count 쿼리 발생\n\n![](.index_images/img_7.png)\n![](.index_images/img_8.png)\n피움 서비스의 물 주기 기능에서 validation을 진행하고 있습니다. 해당 반려 식물의 물주기 히스토리에서 마지막 물주기 이전, 즉 최신 물주기 히스토리 중 2번째 기록을 이용해 잘못된 요청을 하지 않도록 검증하고 있습니다. 그러므로 두 번째 히스토리를 조회하는 쿼리를 질의하고 있습니다. Pagable 객체를 이용해 이 쿼리를 질의하고 반환 타입을 Pagable로 반환받습니다.\n\n\n\n그렇기 때문에 Pagable 객체에 필요한 총 데이터 수를 가져오는 count 쿼리가 발생합니다.\n\n페이징이 필요 없는 기능에서 페이징 객체를 반환받고 있으므로 **Pagable 대신 Slice 객체를 반환받도록 변경하여 count 쿼리가 발생하지 않도록 개선**했습니다.\n\n## 애플리케이션 단에서 처리할 수 있는 로직을 DB로 질의\n\n![](.index_images/img_9.png)\n![](.index_images/img_10.png)\n\n반려 식물을 생성, 환경 변화, 물 주기 등 반려 식물과 관련된 이벤트가 발생했을 때 히스토리를 저장하고 있고, 한 동작에서 여러 히스토리가 생성되고 있습니다. 히스토리를 저장할 때 어떤 히스토리인지 히스토리 카테고리를 이용해 구분합니다. 이때 해당 카테고리가 DB에 존재하는지 확인하고 저장하기 때문에 카테고리 조회가 발생합니다. 현재 카테고리가 총 6개 존재하고 요구사항으로 카테고리가 추가적으로 생길 가능성이 많고 그에 따라 발생하는 쿼리는 증가할 것입니다.\n\n```java\nHistoryCategory historyCategory = historyCategoryRepository.findByHistoryType(\n                historyEvent.getHistoryType())\n        .orElseThrow(() -> new NoSuchElementException(\"일치하는 카테고리가 없습니다\"));\n```\n\n그러므로 히스토리를 생성하는 시점에 모든 카테고리를 조회한 후, 애플리케이션 단에서 검증하도록 변경했습니다.\n```java\nList<HistoryCategory> categories = historyCategoryRepository.findAll();\n\nHistoryCategory historyCategory = categories.stream()\n                    .filter(it -> it.getHistoryType() == historyEvent.getHistoryType())\n                    .findAny()\n                    .orElseThrow(() -> new NoSuchElementException());\n```\n\n![](.index_images/img_11.png)\n\n현재 서비스에서 사용하는 쿼리를 정리하고 개선이 필요한 쿼리는 모두 개선을 마쳤습니다.\n\n\n\n또한 각 쿼리에서 사용하는 인덱스 설정을 확인하였고 인덱스 설정이 필요한 쿼리는 실행 계획을 통해 확인했습니다."},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 우아한테크코스 레벨4 5차 데모데이 요구사항으로부터 Tomcat Thread의 적절한 설정값에 대해 고민하게 되었다.  현재 피움 서비스는 Tomcat의 Thread 값을 어느정도로 설정해야할까? 다양한 상황을 고려해보고 적절한 개수로 설정해보자. 알아보기 요구사항에서 주어진 Tomcat의 설정값…","fields":{"slug":"/tomcat-thread-config/"},"frontmatter":{"date":"September 20, 2023","title":"톰캣 스레드 설정하기","tags":["tomcat","thread"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n우아한테크코스 레벨4 5차 데모데이 요구사항으로부터 Tomcat Thread의 적절한 설정값에 대해 고민하게 되었다.\n\n![](.index_images/b06631fa.png)\n\n현재 피움 서비스는 Tomcat의 Thread 값을 어느정도로 설정해야할까?\n\n다양한 상황을 고려해보고 적절한 개수로 설정해보자.\n\n## 알아보기\n\n요구사항에서 주어진 Tomcat의 설정값이 각각 무엇을 의미하는지 알아보자.\n\n### server.tomcat.threads.max\n\nHTTP 요청을 처리하기 위해 사용할 수 있는 최대 작업 스레드 수를 설정한다.\n기본값은 **200**으로 200개의 스레드를 사용하여 동시에 요청을 처리한다는 의미다.\n\n### server.tomcat.max-connections\n\nTomcat이 동시에 처리할 수 있는 최대 클라이언트 연결 수를 정의한다.\n기본값은 **8192**이다.\n\n### server.tomcat.accept-count\n모든 작업 스레드가 사용 중일 때 Tomcat이 수용 가능한 상태에서 대기 중인 TCP 연결의 최대 수를 나타낸다.\n기본값은 **100**이다.\n\n각 설정값이 어떤 부분인지 그림으로 표현하면 다음과 같다.\n\n![](.index_images/e91ada8d.png)\n\n## 서비스 분석하기\n\n피움 서비스는 트래픽이 몰리는 서비스가 아니다.\n사용자의 개인화된 서비스를 제공하고 있기 때문에 다량의 데이터를 동시에 처리할 일이 거의 없다.\n\n대량의 트래픽, 동시성에 대한 여지가 있는 커뮤니티 기능도 존재하지 않는다.\n\n하지만 실제로 운영한다면 달라지지 않을까?\n\n현재 운영중인 유사한 서비스를 분석해보자\n\n### MAU 분석\n\n> 이어지는 내용은 다음과 같은 용어를 사용합니다.\n> \n> `MAU(Monthly Active User)`: 30일 동안 앱을 사용하는 순 유저 수\n> \n> `DAU(Daily Active User)`: 하루동안 앱을 사용하는 순 유저 수\n> \n> `MCU(Max Current User)`: 최대 동시 접속자\n> \n> `ACU(Average Current User)`: 평균 접속자\n\n![](.index_images/e43c9ccc.png)\n\n[유사 서비스 MAU 참고 사이트](https://www.similarweb.com/website/groo.pro/#traffic)\n\n해당 지표를 확인했을 때 월 평균 방문자수는 약 3만 8천명으로 책정된다.\n\n분당, 초당 요청 횟수에 대해서는 자세하게 확인할 수 없으니 대략적으로 값을 판단해보자.\n\n#### 추정값 계산\n\n![](.index_images/3c4d6f9c.png)\n\n한달에 3만 8천명 정도의 트래픽이 발생한다고 가정했을 때 단순 계산을 해보면 초당 0.015개의 요청이 발생한다고 생각해볼 수 있다...\n\n24시간 내내가 아니라 하루 12시간으로 쳐줘도 초당 0.03개의 요청이 발생한다고 유추해볼 수 있다.\n\n## 분석하기\n\n기존 서비스의 사용자 추이를 분석해봐도 초기에 생각한 것 처럼 많은 트래픽은 몰리지 않음을 확인할 수 있었다.\n\n만약 위에서 계산했던 값을 토대로 하루 평균 요청량이 1분 내로 몰린다고 하더라도 초당 21건의 요청이 오는 꼴이다.\n\n이 값은 현재 Tomcat의 기본 사양으로도 충분한 요청이다.\n(maxThreads = 200)\n\n그래도 실제로 요청이 왔을 때 정상적으로 처리하는지 확인해볼 필요는 있다.\n\n## Jmeter\n\nApache jmeter를 사용하여 서버가 요청을 처리하는 성능을 알아보자.\n\n피움 서비스에서 비로그인 상태로 이용할 수 있는 서비스로 식물 검색기능이 있다.\n\n로그인 이후 수행되는 서비스 중에서 트래픽이 빈번하게 발생할 것으로 우려되는 기능은 없다고 판단했다. (반려 식물 등록, 수정, 삭제, 타임라인 확인, 물주기 등)\n\n따라서 서비스에서 가장 빈번하게 일어날 것이라고 예상되는 식물 검색기능을 기준으로 처리량을 분석하고자 한다.\n\n### TPS\n\n> TPS란 Transaction per Second의 약자로서, 1초당 처리할 수 있는 트랜잭션의 개수를 의미한다.\n다시말해 1초에 서비스가 처리할 수 있는 요청의 양의 개수를 의미한다.\n\n현재 가장 직관적으로 판별할 수 있는 값이라고 판단하여 TPS를 성능의 지표로 사용하기로 했다.\n\n![](.index_images/d86c4efb.png)\n\n1건의 요청에 대해 TPS가 2.3/sec라는 결과가 나왔다.\n\nTPS는 `요청개수 / 처리시간(초)` 로 계산되며 위 경우 요청 개수가 1개이므로 1개의 요청을 처리하는데 약 0.4초가 걸린다고 볼 수 있다.\n(원래는 네트워크 상태에 의한 지연도 고려해야하지만 지금은 고려하지않는다)\n\n#### TPS를 기준으로 목표 지표 생각해보기\n\n현재 실 서비스의 접속량을 기반으로 다음과 같이 서비스에 요구되는 TPS를 생각해봤다.\n\n- 1분당 접속자수: 0.3명\n- 평균 접속 유지시간: 1분\n- (예상) 트래픽이 몰릴 경우 1분에 1명 꼴로 예상\n\n\nTPS = 동시 활성 사용자수 / 평균 응답 시간\n\n동시 활성 사용자수 = 1\n\n평균 응답 시간 = 0.4s\n\n목표 TPS = 4\n\n> ![](.index_images/86d97246.png)\n> 위에서 이야기했다시피 현재 Tomcat의 기본 사양으로 충~분한 요청이다.\n\n### What if..?\n\n만약 서비스가 인기가 많아져서 요청이 많이 발생한다고 가정해보자\n\n#### 시나리오\n\n사용자 유치 목표 100명의 2배인 200명의 충성고객을 유치했다고 가정한다.\n해당 고객들은 매일같이 서비스를 열정적으로 이용한다.\n`스레드 수: 200`\n\n검색기능의 접근성이 단순한 점과 트래픽이 몰리는 경우를 상정하여 1초에 최대 200건의 요청이 들어온다고 가정한다.\n`Ramp up 시간: 1`\n\n각 사용자는 10회 반복요청을 보낸다.\n`루프 카운트: 10`\n\n![](.index_images/8c7c5612.png)\n\n위 시나리오를 기준으로 요청을 보내본다.\n\n![](.index_images/eba81d9d.png)\n\nTPS가 231.1/sec라는 결과가 나왔다.\n\nNIO방식으로 동작하기 때문에 TPS가 200을 넘어가는 것으로 예상된다.\n\n> NIO에 대한 자세한 이야기는 여기에서 다루지는 않겠다.\n\n## 결론\n\nTomcat의 기본 설정값으로 사용자 트래픽을 충분히 감당할 수 있을 것으로 판단했기 때문에 현재로서는 Tomcat의 스레드 설정을 기본 값으로 유지한다.\n\n## Reference\n\n- https://junuuu.tistory.com/799\n- https://hudi.blog/tomcat-tuning-exercise/"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 현재 피움 프로젝트는 properties, env 파일과 같은 민감정보 내용을 서버 내부에 저장, 관리하고있다. 이렇게 파일을 관리했을 때 다음과 같은 문제점이 생각나 팀원들과 discussion에서 의견을 나눠봤다. 캠퍼스 외부에서 properties / env 파일 관리 불가 현재 서버 접속 …","fields":{"slug":"/submodule-apply/"},"frontmatter":{"date":"September 13, 2023","title":"Git submodule을 이용한 민감정보 관리","tags":["git","submodule"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n현재 피움 프로젝트는 properties, env 파일과 같은 민감정보 내용을 서버 내부에 저장, 관리하고있다.\n\n이렇게 파일을 관리했을 때 다음과 같은 문제점이 생각나 팀원들과 discussion에서 의견을 나눠봤다.\n\n- 캠퍼스 외부에서 properties / env 파일 관리 불가\n> 현재 서버 접속 권한이 캠퍼스 내부 IP로 한정되어있기 때문에 발생한 문제점이다.\n- properties 설정을 파악하기가 어려움 (vim, cat 명령어로 확인)\n- 분업화 진행 간 properties / env 파일 변경에 대한 공유가 어려움\n\n\n> 자세한 내용은 [discussion - submodule 도입에 대해](https://github.com/woowacourse-teams/2023-pium/discussions/324) 에서 확인\n\n![](.index_images/59c3159b.png)\n\n팀원들 모두 동의해줬고 이에 프로젝트에 submodule을 도입하는 과정을 정리해보려고한다.\n\n## submodule이란?\n\n서브모듈은 하나의 저장소(레포지토리)에서 다른 저장소를 포함하고 관리할 수 있게 해주는 기능이다.\n주로 의존성 관리 혹은 외부 프로젝트 통합에 사용되곤 한다.\n\n피움에서는 private 레포지토리를 이용해 민감정보를 관리하기 위해 submodule을 적용해보려고한다.\n\n## 시작하기\n\nproperties, env 파일 등 다양한 민감정보를 가지는 설정파일들을 private 레포지토리에 저장하고 사용해보자.\n\n### Private Repository 생성\n\n민감정보를 관리할 Private Repository를 생성한다.\n\n![](.index_images/0658737b.png)\n\n### submodule 시작하기\n\n우선 백엔드 프로젝트를 기준으로 submodule을 적용시켜보자.\n\n`git submodule add {서브모듈 Repo URL}`\n\n![](.index_images/effc0cc4.png)\n\ngit submodule add 명령어를 수행하면 루트 경로에 `.gitmodules` 파일이 생성된 것을 확인할 수 있다.\n\n![](.index_images/20ec3a3b.png)\n\n![](.index_images/a8d5f20f.png)\n\n파일 내용을 살펴보면 submodule이 적용된 path와 submodule을 가져온 url이 표기되어있는 것을 확인할 수 있다.\n\n![](.index_images/0185252a.png)\n\n\n> 추가로 branch 정보를 넣으면 해당 branch를 기준으로 submodule 정보를 가져온다.\n![](.index_images/0ebe1978.png)\n\n.gitsubmodule 파일과 config 폴더가 생성되고 git stage에 올라가있는 상태가 되어있을것이다.\n\ncommit 후 push를 해보자.\n\n![](.index_images/2e9366b0.png)\n\n다음과 같이 서브모듈 폴더가 추가된 것을 볼 수 있다.\n\n![](.index_images/8a331998.png)\n\n### submodule 적용하기\n\n내 컴퓨터에서의 submodule 적용은 완료했다.\n\n다른 개발자들이 submodule을 어떻게 적용해야할지 가이드를 제공하자.\n\n우선 git clone부터 시작해보자.\n\n```shell\ngit clone {Project}\n```\n\n![](.index_images/632999b1.png)\n\nsubmodule을 초기화하고 update한다.\n**해당 작업은 처음 1번만 수행하면 된다.**\n\n```shell\ngit submodule init\n\ngit submodule update\n```\n\n![](.index_images/3a3b7bf7.png)\n\n### submodule 업데이트 반영하기\n\n프로젝트를 진행하는 과정에서 submodule 프로젝트에 수정사항이 생길 수 있다. 이를 반영하기 위해서는 다음과 같은 작업을 수행하면 된다.\n\n> 아래와 같이 새로운 커밋(변경사항)이 발생했다고 가정해보자.\n![](.index_images/8fe95451.png)\n\n```shell\ngit submodule update --remote --merge\n\n## .gitmodules 파일에 정의되어 있는 브랜치의 최신 버전으로 업데이트 (혹은 default 브랜치)\ngit submodule update --remote\n\n## 로컬에서 작업 중인 부분과 원격에 작업된 부분이 다른 경우 머지까지 진행\ngit submodule update --remote --merge\n```\n\n![](.index_images/34d677ff.png)\n\nsubmodule이 update된 정보에 대해 기존 프로젝트에도 반영을 해줘야한다.\n\n![](.index_images/dc253c7e.png)\n\n```shell\ngit add {file}\n\ngit commit -m \"build: 서브모듈 최신사항 반영\"\n\ngit push\n```\n\n![](.index_images/0a7b7407.png)\n\n## 정리\n\n민감정보를 관리하기위해 submodule 도입방법을 정리해봤다.\n생각보다 간단했다 👍\n\n## Reference\n\n- https://tecoble.techcourse.co.kr/post/2021-07-31-git-submodule/\n\n### Reference\n\n- https://medium.com/dream-youngs/lets-encrypt-dns-%EB%A1%9C-%EB%93%B1%EB%A1%9D%ED%95%98%EA%B8%B0-fdc3efda36af\n- https://techblog.woowahan.com/6217/"},{"excerpt":"이 글은 우테코 피움팀 크루 '하마드'가 작성했습니다. 개요 피움 서비스 개발을 진행하면서, JPA Method Naming 기능을 활용하여  쿼리를 생성해 검색 기능을 개발한 과정을 기록한다. 과정 먼저 개발하려는 기능은 다음과 같다 식물사전 데이터베이스에 포함된 식물의 이름을 기준으로, 해당 검색어를 포함한 식물의 목록을 조회해 온다. 따라서 나는 다…","fields":{"slug":"/search_in_spring_data_jpa/"},"frontmatter":{"date":"August 31, 2023","title":"Spring Data Jpa를 활용한 검색 기능 개발하기","tags":["jpa","spring","springDataJpa"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[하마드](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 개요\n---\n피움 서비스 개발을 진행하면서, JPA Method Naming 기능을 활용하여 `%Like%` 쿼리를 생성해 검색 기능을 개발한 과정을 기록한다.\n## 과정\n---\n먼저 개발하려는 기능은 다음과 같다\n> 식물사전 데이터베이스에 포함된 식물의 이름을 기준으로, 해당 검색어를 포함한 식물의 목록을 조회해 온다.\n\n따라서 나는 다음과 같은 쿼리를 생성하고자 했다.\n\n```sql\nselect *\n    from\n        dictionary_plant\n    where\n        dictionary_plant.name like '%검색어%'\n```\n현재 dictionary_plant의 entity는 다음과 같다.\n\n```java\n@Entity\n@Getter\n@Table(name = \"dictionary_plant\")\n@NoArgsConstructor(access = AccessLevel.PROTECTED)\npublic class DictionaryPlant extends BaseEntity {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    @Column(name = \"name\")\n    private String name;\n\n    @Column(name = \"image_url\")\n    private String imageUrl;\n\n    @Column(name = \"family_name\")\n    private String familyName;\n\n    @Column(name = \"smell\")\n    private String smell;\n\n    @Column(name = \"poison\")\n    private String poison;\n\n    @Column(name = \"manage_level\")\n    private String manageLevel;\n\n    @Column(name = \"grow_speed\")\n    private String growSpeed;\n\n    @Column(name = \"require_temp\")\n    private String requireTemp;\n\n    @Column(name = \"minimum_temp\")\n    private String minimumTemp;\n\n    @Column(name = \"require_humidity\")\n    private String requireHumidity;\n\n    @Column(name = \"posting_place\")\n    private String postingPlace;\n\n    @Column(name = \"special_manage_info\")\n    private String specialManageInfo;\n\n    @Embedded\n    private WaterCycle waterCycle;\n}\n```\n\n### Like 기능을 하는 Repository 메소드 생성\n\nspring data jpa는 메소드 네이밍만으로 Like 쿼리를 생성할 수 있는 기능을 제공한다.\n\n자세한 내용은 [이곳](https://www.baeldung.com/spring-jpa-like-queries) 을 참고하자.\n\n`Like` 쿼리는 총 네 가지의 네이밍으로 생성할 수 있다.\n예를 들어\n`SELECT * FROM movie WHERE title LIKE '%in%';` 라는 쿼리를 생성하려 한다면,\n```java\nList<Movie> findByTitleContaining(String title);\nList<Movie> findByTitleContains(String title);\nList<Movie> findByTitleIsContaining(String title);\n```\n위와 같이 `Containing`, `Contains`, `IsContaining` 이라는 네이밍을 활용해 생성할 수 있다. 세 가지 네이밍이 동작한 결과는 완전히 같다.\n\n스프링은 또한 메소드에`Like` 키워드를 활용해 적용하도록 할 수도 있는데,이 경우 와일드카드 키워드(%)를 문자열에 포함시켜야 한다.\n\n```java\nList<Movie> findByTitleLike(String title);\n```\n위와 같이 메소드를 작성한다면\n```java\nresults = movieRepository.findByTitleLike(\"%in%\");\n```\n위처럼 % 를 포함한 문자열을 파라미터로 넣어야 한다는 뜻이다.\n\n본인의 경우 Contains 키워드가 가장 가독성이 높다고 판단하여 이를 적용하였다.\n\n**DictionaryPlantRepository.java**\n\n```java\npublic interface DictionaryPlantRepository extends JpaRepository<DictionaryPlant, Long> {\n\n    List<DictionaryPlant> findDictionaryPlantsByNameContains(String name);\n}\n```\n따라서 위와 같이 이름에 파라미터로 들어온 검색값(name)이 포함된 사전식물을 검색하는 메소드를 만들었다.\n\n## 결과\n---\n![](.index_images/jpa1.png)\n테스트를 진행시켜보니 원하는 검색어가 포함된 Like 쿼리 (여기서는 \"%스투%\")가 날아가는 모습을 볼 수 있다."},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론  피움 팀에서 비로그인 시 로그인을 수행하라는 페이지와 함께 이미지를 띄우려고 한다.\n이 때 코드를 배포하지 않고도 이미지를 변경할 수 있는 방법이 없을까 생각했다. 현재 이미지를 프론트엔드의 assets에 넣어서 배포를 하게 된다면 이미지가 변경하나에도 배포가 이뤄져야한다는 번거로움이 존재한다.…","fields":{"slug":"/aws-s3-apply/"},"frontmatter":{"date":"August 30, 2023","title":"AWS S3로 정적 이미지 배포하기","tags":["AWS","S3","CloudFront"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n![img.png](.index_images/img.png)\n\n피움 팀에서 비로그인 시 로그인을 수행하라는 페이지와 함께 이미지를 띄우려고 한다.\n이 때 코드를 배포하지 않고도 이미지를 변경할 수 있는 방법이 없을까 생각했다.\n\n현재 이미지를 프론트엔드의 assets에 넣어서 배포를 하게 된다면 이미지가 변경하나에도 배포가 이뤄져야한다는 번거로움이 존재한다.\n\nCDN을 이용하여 이미지를 배포하고 해당 파일을 관리할 수 있다면 재배포하는 번거로움은 줄어들 것이다.\n\n다시말해 `https://이미지경로.이미지이름.png`라는 경로는 동일하나 해당 서버에 있는 자원을 변경하면 이미지가 변경될 수 있다.\n\n이러한 효과를 내기 위해 AWS의 S3를 이용해볼 수 있다.\n\n## AWS S3란?\n\n아마존 웹 서비스에서 제공하고있는 객체 스토리지 서비스(Simple Storage Service)다.\n쉽게 말해 클라우드 환경에 저장소를 제공하는 서비스를 의미한다.\n\n![img_1.png](.index_images/img_1.png)\n\n## 시작하기\n\n> [사례별로 알아본 안전한 S3 사용 가이드](https://techblog.woowahan.com/6217/)를 참고하여 작성된 글입니다.\n개인 AWS 계정으로 실습을 진행하는점 참고 부탁드립니다~\n\n지금부터 AWS S3를 이용해 정적 웹 호스팅을 수행해보자.\n\n### 웹 호스팅 준비하기\n\n![img_2.png](.index_images/img_2.png)\n\n버킷 생성부터 시작해보자.\n\n#### 버킷 생성\n\n![img_3.png](.index_images/img_3.png)\n\n우선 S3 서비스에 들어가서 버킷을 생성한다.\n\n![img_4.png](.index_images/img_4.png)\n\n`ACL 비활성화됨`, `모든 퍼블릭 엑세스 차단` 옵션을 선택해준다.\n\n![img_5.png](.index_images/img_5.png)\n\n기본 암호화는 `Amazon S3 관리형 키(SSE-S3)를 사용한 서버 측 암호화`를 선택하고 버킷을 생성한다.\n\n![img_6.png](.index_images/img_6.png)\n\n위와같이 버킷이 생성된 모습을 확인할 수 있다.\n\n![img_7.png](.index_images/img_7.png)\n\n버킷 탭에 들어가서 이미지를 하나 업로드하자.\n\n![img_8.png](.index_images/img_8.png)\n\nfavicon.png 파일을 업로드했다.\n\n![img_9.png](.index_images/img_9.png)\n\n이 상태로 객체 URL에 접근해보면\n\n![img_10.png](.index_images/img_10.png)\n\n다음과 같이 Access Denied가 뜬다.\n버킷을 생성할 때 Public으로 생성하지 않았기 때문이다.\n\n### 비공개인 버킷에 접근하기\n\n버킷을 Private으로 만들었기 때문에 사용자가 자원에 직접적으로 접근하지 못한다.\n\n![img_32.png](.index_images/img_32.png)\n\n> 출처: https://techblog.woowahan.com/6217/\n\n사용자가 CloudFront를 우회하여 접근할 수 있도록 구성해보자.\n\n#### CloudFront 설정\n\n![img_1.png](.index_images/img_34.png)\n\nCloudFront 배포 생성에서 원본 도메인을 위에서 생성한 S3로 선택한다.\n\n![img_2.png](.index_images/img_35.png)\n\n이후 원본액세스 제어 설정을 선택하고 `제어 설정 생성` 탭을 선택한다.\n\n![img_3.png](.index_images/img_36.png)\n\n위와같이 제어 설정을 구성한다.\n\n![img_4.png](.index_images/img_37.png)\n\n기본 캐시 동작은 `Redirect HTTP to HTTPS` 를 선택하고 새 배포를 생성한다.\n(WAF 설정은 적용하지 않았다)\n\n#### 버킷 정책 업데이트\n\n![img_5.png](.index_images/img_38.png)\n\nCloudFront를 생성하면 위처럼 S3 버킷 정책을 업데이트하도록 나온다.\n`정책 복사` 를 투르고 `정책을 업데이트하려면 S3 버킷 권한으로 이동합니다.` 를 누른다.\n\n![img_6.png](.index_images/img_39.png)\n\n버킷 - 퀀합 탭에서 버킷 정책 편집으로 들어간다.\n\n![img_7.png](.index_images/img_40.png)\n\n복사한 내용을 버킷 정책에 붙여넣고 변경사항을 저장한다.\n\n#### 접속하기\n\n![img_8.png](.index_images/img_41.png)\n\n이제 CloudFront 탭에 다시 들어가서 배포 도메인 이름을 확인한다.\n\n`배포도메인/{S3경로파일명}`로 호출했을 때 파일이 정상적으로 호출되는지 확인해본다.\n\n![img_9.png](.index_images/img_42.png)\n\n#### 대체 도메인 설정하기\n\n> 해당 과정을 수행하기 위해서는 도메인을 구입이 선행되어야합니다.\n도메인 구입은 [피움의 배포과정](https://pium-official.github.io/pium-deploy-step/)을 참고해주세요\n\n매번 `배포도메인/{S3경로파일명}`으로 접속하기에는 도메인의 길이도 너무 길고 복잡하다.\n\n`image.pium.life/{S3경로파일명}`로 접속하도록 구성해보자.\n\n#### 인증서 발급\n\n우선 인증서를 발급받아야한다.\n\n특정 웹서버에 인증서를 붙이는것이 아니므로 image.pium.life에 대해 SSL 인증서만 발급받아보자.\n\n- **certbot으로 SSL 인증서 발급**\n\n> certbot에 대한 내용은 [피움 팀블로그 - 내 서버에 HTTPS 설정하기](https://pium-official.github.io/apply-https/)를 참고\n\n맥북에서 image.pium.life에 대한 SSL 인증서를 발급받아보자.\ncertbot을 받아준다.\n\n```shell\n# certbot 설치\nbrew install certbot\n\n# dns challenge 방식의 인증을 수행한다\nsudo certbot certonly --manual --preferred-challenges dns -d image.pium.life\n```\n\n![img_10.png](.index_images/img_43.png)\n\n서비스에 대한 동의를 진행한다.\n\n![img_11.png](.index_images/img_11.png)\n\n여기서 해당 도메인의 소유자인지 검증하기위해 DNS 서비스에 위 값을 등록해야한다.\n**(아직 엔터를 누르면 안된다!!!)**\n\n![img_12.png](.index_images/img_12.png)\n\n위처럼 DNS 관리 툴에서 TXT 레코드 정보를 추가해준다.\n이후 해당 도메인이 정상적으로 배포되었는지 확인하기 위해 `Admin Toolbox: https://...`에 쓰여있는 링크로 접속한다.\n\n> https://toolbox.googleapps.com/apps/dig/#TXT/_acme-challenge.image.pium.life.\n\n![img_13.png](.index_images/img_13.png)\n\n도메인이 아직 배포되지 않았다면 `Record not found!`가 뜬다.\n\n![img_14.png](.index_images/img_14.png)\n\n잠시 기다리면 이렇게 도메인 조회가 된다.\n\n![img_15.png](.index_images/img_15.png)\n\n배포 확인 후 다시 이 화면으로 넘어와서 엔터를 누르면 된다.\n\n![img_16.png](.index_images/img_16.png)\n\n![img_18.png](.index_images/img_18.png)\n\n`/etc/letsencrypt/live/image.pium.life` 경로에 SSL 인증서가 발급되었다.\n\n> 인증서 발급이 완료되었으므로 DNS 서비스에 등록했던 TXT 레코드는 삭제해도된다.\n\n#### AWS에 인증서 넣기\n\n이제 이 인증서를 AWS로 가져가보자.\n\n![img_19.png](.index_images/img_19.png)\n\nAWS의 Certificate Manager 서비스를 이용한다.\n\n![img_20.png](.index_images/img_20.png)\n\n> CloudFront의 SSL 인증서 적용 과정에서 미국 동부(버지니아 북부) 리전의 인증서를 요구하기 때문에 리전을 버지니아-북부로 설정해두고 작업을 진행해야한다!\n>\n> ![img_21.png](.index_images/img_21.png)\n>\n> ![img_22.png](.index_images/img_22.png)\n\n\n인증서 가져오기를 수행한다.\n\n![img_23.png](.index_images/img_23.png)\n\n인증서 세부 정보를 입력하는 칸이 나오는데 각각 해당하는 정보를 적어주면 된다.\n\n![img_24.png](.index_images/img_24.png)\n\n```\nsudo cat cert.pem\nsudo cat privkey.pem\nsudo cat fullchain.pem\n```\n\n> ![img_25.png](.index_images/img_25.png)\n>\n> `-----BEGIN CERTIFICATE-----`부터\n> `-----END CERTIFICATE-----`까지\n> 다 복사해서 넣어줘야한다.\n\n![img_26.png](.index_images/img_26.png)\n\n등록을 누르면 다음과 같이 인증서를 가져올 수 있다.\n\n![img_27.png](.index_images/img_27.png)\n\n#### CloudFront 별칭 설정\n\n![img_28.png](.index_images/img_28.png)\n\nDNS 호스팅 사이트에 가서 CNAME 레코드를 추가해준다.\n호스트는 image이고 값은 CloudFront의 배포 주소를 넣어준다.\n\n![img_29.png](.index_images/img_29.png)\n\nCloudFront 탭으로 와서 편집탭으로 들어간다.\n\n![img_30.png](.index_images/img_30.png)\n\n대체 도메인을 작성하고 추가한 SSL 인증서를 선택한 뒤 저장한다.\n\n![img_31.png](.index_images/img_31.png)\n\n이제 image.pium.life로 cloudfront로 접속할 수 있게 되었다.\n\n## 후기\n\n단순한 내용도 많았지만 중간에 인증서 발급 등과 같은 세세한 부분에서 헤맬 수 있다고 생각하여 최대한 자세하게 기록했다.\n\n## Reference\n\n- https://medium.com/dream-youngs/lets-encrypt-dns-%EB%A1%9C-%EB%93%B1%EB%A1%9D%ED%95%98%EA%B8%B0-fdc3efda36af\n- https://techblog.woowahan.com/6217/"},{"excerpt":"이 글은 우테코 피움팀 크루 '조이'가 작성했습니다.   시작하기 전에 피움 프로젝트에서는 OAuth 2.0을 이용해 로그인을 구현하였습니다. 즉, 로그인 과정이 외부 API에 의존적이라고 볼 수 있는 것이죠. 서비스를 제공할 때 문제가 되는 부분은 아니지만, 로그인에 대한 테스트 코드를 작성할 때는 문제가 될 수 있습니다. 그렇다면 어떻게 외부 API에…","fields":{"slug":"/OAuth-test-backend/"},"frontmatter":{"date":"August 29, 2023","title":"외부 API 를 의존하는 OAuth 로그인 테스트하기","tags":["OAuth2.0","로그인","테스트"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[조이](https://github.com/yeonkkk)'가 작성했습니다.  \n\n\n\n## 시작하기 전에\n---\n피움 프로젝트에서는 OAuth 2.0을 이용해 로그인을 구현하였습니다.   \n즉, 로그인 과정이 외부 API에 의존적이라고 볼 수 있는 것이죠.\n\n서비스를 제공할 때 문제가 되는 부분은 아니지만, 로그인에 대한 테스트 코드를 작성할 때는 문제가 될 수 있습니다.\n\n그렇다면 어떻게 외부 API에 의존하지 않고 테스트 코드를 작성할 수 있을까요?\n피움 프로젝트를 통해서 그 방법을 살펴보겠습니다.\n\n글을 읽기 전에 몇 가지 사항들을 체크해 봅시다.\n\n<br><br>\n\n### 1.  테스트\n> 이 글은 테스트 방법에 대해서 다루지만, 특정 테스트 종류들에 대해서 설명하지 않습니다.   \n> 특정 테스트에 대한 지식이 필요하시거나 궁금하시다면 아래 자료들을 참고해주시면 감사하겠습니다.\n\n- [단위 테스트 vs 통합 테스트 vs 인수 테스트](https://tecoble.techcourse.co.kr/post/2021-05-25-unit-test-vs-integration-test-vs-acceptance-test/)\n- [Spring Boot 슬라이스 테스트](https://tecoble.techcourse.co.kr/post/2021-05-18-slice-test/)\n\n<br><br>\n\n### 2. 개발 환경\n> 이 글은 아래와 같은 환경에서 진행한 내용을 다룹니다.\n\n- spring boot 3.1.1\n\n- java 17\n\n\n<br><br><br><br>\n\n## 테스트할 코드 파악하기\n---\n> 이 글은 OAuth 2.0을 이용한 로그인 구현 방법이 아닌 테스트 방법에 대해서만 설명합니다.    \n> 그렇기 때문에 테스트 코드와 밀접한 관련이 있는 코드만 가볍게 확인하고 넘어가겠습니다.  \n> 만약 자세한 구현 과정에 대해 알고 싶다면 아래 자료를 참고해주세요!\n\n- [피움블로그 - OAuth 2.0 로그인 구현하기 (카카오)](https://blog.pium.life/OAuth2.0-backend/)\n\n- [피움 깃허브 레포지토리](https://github.com/woowacourse-teams/2023-pium)\n\n<br><br><br><br>\n\n\n\n![피움-로그인-테스트-이미지.png](.index_images/img.png)\n\n그림을 통해 코드의 흐름을 먼저 생각해보겠습니다.\n\n로그인 요청이 들어오면 `AuthController`가 요청을 확인한 후 `AuthService`를 호출합니다.\n\n`AuthService`에서는 `OAuthProvider`를 호출하여 `Kakao server`로 필요한 정보를 요청합니다.\n\n`OAuthProvider`는 `Kakao server`에 액세스 토큰 발급을 요청하고 사용자 정보를 요청합니다.\n\n흐름을 파악했다면 코드를 간단히 확인한 후 테스트 방법에 대해 알아봅시다!  \n<br><br>\n\n### AuthController\n\n\n```java\n@Validated\n@RestController\n@RequiredArgsConstructor\npublic class AuthController {\n\n\t// ...\n    \n    private final AuthService authService;\n\n    @PostMapping(\"/login\")\n    public ResponseEntity<Void> login(\n            @RequestParam(name = \"code\") @NotBlank String code,\n            HttpServletRequest request) {\n        Member loginMember = authService.login(code);\n\n\t\t// ...\n    }\n\t// ...\n}\n```\n\n<br><br>\n\n### AuthService\n\n```java\n@Service\n@Transactional(readOnly = true)\n@RequiredArgsConstructor\npublic class AuthService {\n\n    private final MemberRepository memberRepository;\n    private final OAuthProvider provider;\n\n    @Transactional\n    public Member login(String authorizationCode) {\n        KaKaoAccessTokenResponse accessTokenResponse = provider.getAccessToken(authorizationCode);\n        String accessToken = accessTokenResponse.getAccessToken();\n\n        KakaoMemberResponse kakaoMemberResponse = provider.getMemberInfo(accessToken);\n        Long kakaoId = kakaoMemberResponse.getId();\n\t\t\n        // ...\n    }\n}\n\n```\n\n<br><br>\n\n### OAuthProvider\n\n```java\n@Component\n@RequiredArgsConstructor\npublic class OAuthProvider {\n\n    public static final String AUTHORIZATION_HEADER = \"Authorization\";\n    public static final String GRANT_TYPE = \"authorization_code\";\n    public static final String TOKEN_TYPE = \"Bearer \";\n\n    @Value(\"${auth.kakao.token-request-uri}\")\n    private String tokenRequestUri;\n\n    @Value(\"${auth.kakao.member-info-request-uri}\")\n    private String memberInfoRequestUri;\n\n    @Value(\"${auth.kakao.client-id}\")\n    private String clientId;\n\n    @Value(\"${auth.kakao.admin-id}\")\n    private String adminId;\n\n    @Value(\"${auth.kakao.redirect-uri}\")\n    private String redirectUri;\n\n    @Value(\"${auth.kakao.unlink-uri}\")\n    private String unLinkUri;\n\n    private final RestTemplate restTemplate;\n\n    public KakaoMemberResponse getMemberInfo(String accessToken) {\n        try {\n            HttpHeaders httpHeaders = new HttpHeaders();\n            httpHeaders.set(AUTHORIZATION_HEADER, TOKEN_TYPE + accessToken);\n            httpHeaders.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n\n            HttpEntity<Object> request = new HttpEntity<>(httpHeaders);\n\n            return restTemplate.postForEntity(memberInfoRequestUri, request, KakaoMemberResponse.class)\n                    .getBody();\n        } catch (HttpClientErrorException e) {\n            throw new KaKaoMemberInfoRequestException(e.getMessage());\n        } catch (HttpServerErrorException e) {\n            throw new KakaoServerException(e.getMessage());\n        }\n    }\n\n    public KaKaoAccessTokenResponse getAccessToken(String authorizationCode) {\n        try {\n            HttpHeaders httpHeaders = new HttpHeaders();\n            httpHeaders.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n\n            MultiValueMap<String, String> body = new LinkedMultiValueMap<>();\n            body.add(\"grant_type\", GRANT_TYPE);\n            body.add(\"client_id\", clientId);\n            body.add(\"redirect_uri\", redirectUri);\n            body.add(\"code\", authorizationCode);\n\n            HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(body, httpHeaders);\n\n            return restTemplate.postForEntity(\n                    tokenRequestUri, request, KaKaoAccessTokenResponse.class).getBody();\n        } catch (HttpClientErrorException e) {\n            throw new KakaoTokenRequestException(e.getMessage());\n        } catch (HttpServerErrorException e) {\n            throw new KakaoServerException(e.getMessage());\n        }\n    }\n\t// ...\n}\n```\n\n<br><br><br><br>\n\n\n## Mockito, MockMvc 이용하기\n---\n> 먼저 `Mockito`, `MockMvc` 등을 통한 mocking을 생각해볼 수 있습니다.\n\n`AuthController`에 대한 슬라이스 테스트에 적용한 코드를 통해 그 방법을 알아보겠습니다.\n\n`AuthService`를 `MockBean`으로 주입한 후 카카오 서버에 요청을 하는 로직이 담긴 service를 mocking 하고, 예상한 대로 작업이 수행되었는지 테스트합니다.\n\nservice 레이어에서도 `OAuthProvider`를 동일한 방법으로 mocking 하여 테스트할 수 있습니다.\n\n\n### AuthControllerTest\n\n```java\n@WebMvcTest(controllers = AuthController.class)\nclass AuthControllerTest extends UITest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @MockBean\n    private AuthService authService;\n\n    @Nested\n    class 로그인_ {\n\n        @Test\n        void 정상_요청_시_200_반환() throws Exception {\n\n           given(authService.login(anyString())).willReturn(Member.builder()\n                    .kakaoId(12345L)\n                    .build()); \n\n            mockMvc.perform(post(\"/login\")\n                            .queryParam(\"code\", \"authorization code\")\n                            .contentType(MediaType.APPLICATION_JSON_VALUE))\n                    .andExpect(status().isOk())\n                    .andDo(print());\n        }\n\n\t\t// ...\n    }\n    // ...\n}\n\n```\n\n<br><br>\n\n\n### 한계점\n이 방법은 특정 레이어에 대한 테스트를 수행하기에 적합하고 효율적일 수 있습니다.\n하지만 실제 운영 환경과 같은 조건에서 사용자 시나리오를 테스트해야하는 인수테스트를 진행하는 경우 적합하지 않을 수 있습니다.\n\n\n<br><br><br><br>\n\n\n## MockRestServiceServer 이용하기\n---\n> 두번째 방법으로는 Http 통신을 가능하게 하는 `RestTemplate`를 mocking 하는 방법입니다.  \n> `RestTemplate`을 mocking하기 위해 `MockRestServiceServer`를 이용할 수 있습니다.  \n> [MockRestServiceServer](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/web/client/MockRestServiceServer.html)는 `RestTemplate`를 테스트 하기 위한 Spring에서 제공하는 테스트 라이브러리입니다.\n\n\n### OAuthProviderTest\n\n`OAuthProvider` 테스트를 통해 그 방법을 알아보겠습니다.  \n전체 코드를 확인 후 세부적인 내용을 확인해보겠습니다.\n\n```java\n@SpringBootTest\nclass OAuthProviderTest {\n\n    @Value(\"${auth.kakao.token-request-uri}\")\n    private String TOKEN_REQUEST_URI;\n\n    @Value(\"${auth.kakao.member-info-request-uri}\")\n    private String MEMBER_INFO_REQUEST_URI;\n\n    @Autowired\n    private OAuthProvider supporter;\n\n    @Autowired\n    private RestTemplate restTemplate;\n\n    private MockRestServiceServer mockServer;\n\n    @BeforeEach\n    void setUp() {\n        mockServer = MockRestServiceServer.createServer(restTemplate);\n    }\n\n    @Test\n    void 사용자_정보_조회() {\n        String accessToken = \"access token\";\n        Long kakaoId = 12345L;\n\n        String response = String.format(\"{\\\"id\\\":\\\"%d\\\"}\", kakaoId);\n\n        mockServer.expect(requestTo(MEMBER_INFO_REQUEST_URI))\n                .andExpect(content().contentType(\"application/x-www-form-urlencoded\"))\n                .andExpect(method(HttpMethod.POST))\n                .andExpect(header(AUTHORIZATION_HEADER, TOKEN_TYPE + accessToken))\n                .andRespond(withSuccess(response, MediaType.APPLICATION_JSON));\n\n        KaKaoMemberInfoResponse memberInfoResponse = supporter.getMemberInfo(accessToken);\n\n        assertThat(memberInfoResponse.getId()).isEqualTo(kakaoId);\n    }\n\n    @Test\n    void 액세스_토큰_조회() {\n        String authorizationCode = \"authorization code\";\n        String accessToken = \"access token\";\n        String response = String.format(\"{\\\"access_token\\\":\\\"%s\\\"}\", accessToken);\n\n        mockServer.expect(requestTo(TOKEN_REQUEST_URI))\n                .andExpect(content().contentType(\"application/x-www-form-urlencoded;charset=utf-8\"))\n                .andExpect(method(HttpMethod.POST))\n                .andRespond(withSuccess(response, MediaType.APPLICATION_JSON));\n\n        KaKaoAccessTokenResponse tokenResponse = supporter.getAccessToken(authorizationCode);\n\n        assertThat(tokenResponse.getAccessToken()).isEqualTo(accessToken);\n    }\n}\n\n```\n\n<br><br>\n\n\n- 동일한 URL에 대한 응답 값이 다를 경우를 고려해 서버를 새로 생성해주도록 합니다.\n```java\n@SpringBootTest\nclass OAuthProviderTest {\n    // ... \n    @BeforeEach\n    void setUp() {\n        mockServer = MockRestServiceServer.createServer(restTemplate);\n    }\n    // ... \n}\n```\n\n<br><br>\n\n- 요청 조건과 응답 값을 설정해줍니다.\n\n    - `requestTo(String URI)` : 요청할 URI를 설정합니다.\n    - `method(HttpMethod method)`: 요청할 메서드를 설정합니다. 기본 값은 `GET` 입니다.\n    - `header()`: 요청 시 보내야하는 헤더를 설정합니다. key-value 순서로 작성합니다.\n    - `andRespond()`: 돌아와야하는 응답 값을 설정합니다. 응답은 아래와 같이 String으로 작성합니다.\n    - 이외에도 status code, contentType 등도 설정할 수 있습니다.\n\n```java\n@SpringBootTest\nclass OAuthProviderTest {\n    // ... \n    @Test\n    void 사용자_정보_조회() {\n        // ...\n        \n        String response = String.format(\"{\\\"id\\\":\\\"%d\\\"}\", kakaoId);\n\n        mockServer.expect(requestTo(MEMBER_INFO_REQUEST_URI))\n                .andExpect(content().contentType(\"application/x-www-form-urlencoded\"))\n                .andExpect(method(HttpMethod.POST))\n                .andExpect(header(AUTHORIZATION_HEADER, TOKEN_TYPE + accessToken))\n                .andRespond(withSuccess(response, MediaType.APPLICATION_JSON));\n        // ...\n    }\n    // ... \n}\n```\n\n<br><br>\n\n이렇게 직접 `MockRestServiceServer`를 생성해주지 않고 `@RestClientTest`를 사용하는 방법도 있습니다.\n\n자세한 내용은 [기억보단 기록을(이동욱님) - Spring Boot에서 외부 API 테스트하기](https://jojoldu.tistory.com/341) 를 참고하시면 좋을 것 같습니다.\n\n\n<br><br>\n\n\n### 한계점\n이 방법은 mock server를 이용하기 때문에 앞선 방법과 다르게 인수테스트에서도 사용이 가능합니다.  \n하지만 테스트 시 `RestTemplate`과 `MockRestServiceServer`를 매번 생성해주어야 하고, 적용되는 테스트들에 다수의 중복 코드가 발생할 수 있습니다.  \n뿐만 아니라 응답 값을 문자열로 작성해주어야 하기 때문에 가독성이 떨어집니다.\n\n\n<br><br><br><br>\n\n\n## Kakao Server Mocking하기\n---\n> 마지막 방법으로 가짜 카카오 서버를 만들어서 mocking하는 방법입니다.  \n> 즉, 카카오 서버 대신 요청을 수행해줄 controller를 생성하는 것입니다.\n\n### AuthApiTest\n아래와 같이 로그인 요청을 테스트하는 코드를 작성합니다.  \n이 때, `@SpringBootTest(webEnvironment = WebEnvironment.DEFINED_PORT)` 를 사용하여 원하는 포트에서 테스트가 진행되도록 설정해주어야 합니다.\n\n그 이유는 카카오 서버를 대신해 요청을 수행해줄 controller로 우리가 요청을 보낼 때 해당 포트를 기재해주어야 하기 때문입니다.\n\n(아래 `application.properties`의 `auth.kakao.token-request-uri`와 `auth.kakao.member-info-request-uri` 참고)\n\n\n```java\n@SpringBootTest(webEnvironment = WebEnvironment.DEFINED_PORT)\npublic class AuthApiTest {\n\n    @Test\n    void 로그인_정상_요청_시_200_반환() {\n        ExtractableResponse<Response> response = RestAssured\n                .given()\n                .log().all()\n                .queryParam(\"code\", \"authorizationCode\")\n                .when()\n                .post(\"/login\")\n                .then()\n                .log().all()\n                .statusCode(HttpStatus.OK.value())\n                .extract();\n\n        // ...\n    }\n    \n    // ...\n}\n```\n\n<br><br>\n\n### application.properties\nmain의 `application.properties` 와 동일하게 카카오 서버에 정보를 요청하기 위한 정보들(redirect uri, client id 등)을 test 시 사용되는 `application.properties`에 작성해줍니다.\n\n여기서 = `server.port`로 지정한 포트에서 `@SpringBootTest(webEnvironment = WebEnvironment.DEFINED_PORT)` 테스트가 진행됩니다.\n\n이 때, `server.port`로 지정한 포트와 요청 URI(`auth.kakao.token-request-uri`, `auth.kakao.member-info-request-uri`) 의 포트가 동일해야 요청이 정상적으로 수행됩니다.\n\n```properties\nserver.port=8888\nauth.kakao.token-request-uri=http://localhost:8888/oauth/token\nauth.kakao.member-info-request-uri=http://localhost:8888/user/me\nauth.kakao.redirect-uri=http://localhost:8888/authorization\nauth.kakao.client-id=clientId\n\n```\n\n<br><br>\n\n\n### MockKakaoServerController\n카카오 서버 대신 요청을 처리해 줄 컨트롤러를 작성하여, 요청 조건과 원하는 응답을 설정해줍니다.\n\n```java\n@RestController\npublic class MockKakaoServerController {\n\n// ...\n\n    @PostMapping(path = \"/oauth/token\", consumes = MediaType.APPLICATION_FORM_URLENCODED_VALUE)\n    public ResponseEntity<KaKaoAccessTokenResponse> getAccessToken(HttpEntity<String> request) {\n    \n\t\t// ...\n        \n        KaKaoAccessTokenResponse response = KaKaoAccessTokenResponse.builder()\n                .accessToken(\"access token\")\n                .build();\n\n        return ResponseEntity.ok(response);\n    }\n\n    @PostMapping(path = \"/user/me\", consumes = MediaType.APPLICATION_FORM_URLENCODED_VALUE)\n    public ResponseEntity<KakaoMemberResponse> getMemberInfo(@RequestHeader(HttpHeaders.AUTHORIZATION) String token) {\n \t\t\n        // ...\n\n        KakaoMemberResponse response = KakaoMemberResponse.builder()\n                .id(54321L)\n                .build();\n\n        return ResponseEntity.ok(response);\n    }\n}\n\n```\n\n\n<br><br><br><br>\n\n\n## 참고 자료\n---\n\n[PROLOG - OAuth를 사용하는 로직의 테스트를 외부 API에 의존하지 않게 해보자](https://prolog.techcourse.co.kr/studylogs/2500)\n\n[외부 API Mocking하여 테스트하기](https://velog.io/@jurlring/%EC%99%B8%EB%B6%80-API-Mocking%ED%95%98%EC%97%AC-%ED%85%8C%EC%8A%A4%ED%8A%B8%ED%95%98%EA%B8%B0#%EC%8B%9C%EB%8F%84-2---resttemplate-mocking)  \n"},{"excerpt":"이 글은 우테코 피움팀 크루 '조이'가 작성했습니다. 시작하기 전에 피움 프로젝트에서 OAuth 2.0을 이용해 카카오 로그인을 구현하여 그 과정을 기록해보려 합니다. 그럼 본격적으로 글을 읽기 전에 몇 가지 사항들을 체크해 봅시다.  1.  OAuth 2.0 기초 개념 이 글은 독자가 OAuth 2.0 개념을 알고 있다는 가정 하에 작성 되었습니다. 만…","fields":{"slug":"/OAuth2.0-backend/"},"frontmatter":{"date":"August 24, 2023","title":"OAuth 2.0 로그인 구현하기 (카카오)","tags":["OAuth2.0","로그인"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[조이](https://github.com/yeonkkk)'가 작성했습니다.\n\n## 시작하기 전에\n\n피움 프로젝트에서 **OAuth 2.0**을 이용해 **카카오 로그인**을 구현하여 그 과정을 기록해보려 합니다.\n\n그럼 본격적으로 글을 읽기 전에 몇 가지 사항들을 체크해 봅시다.\n\n<br><br>\n\n### 1.  OAuth 2.0 기초 개념\n> 이 글은 **독자가 OAuth 2.0 개념을 알고 있다는 가정 하**에 작성 되었습니다.  \n> 만약 OAuth 2.0에 대한 기본적인 개념을 파악하고 싶다면 아래 자료를 참고해주세요.\n\n\n- [Tecoble - OAuth 개념 및 동작 방식 이해하기](https://tecoble.techcourse.co.kr/post/2021-07-10-understanding-oauth/)\n- [Tecoble - OpenID와 OAuth2.0](https://tecoble.techcourse.co.kr/post/2022-10-24-openID-Oauth/)\n\n<br><br>\n\n### 2. 개발 환경\n> 이 글은 아래와 같은 환경에서 구현한 내용을 설명하고 있습니다.\n\n- spring boot 3.1.1\n\n- java 17\n\n\n<br><br>\n\n### 3. 설정하기\n> 카카오 로그인 흐름을 이해하기 위해 카카오에서 제공하는 공식 문서를 숙지합니다.  \n> 카카오 공식 문서를 참고하여 서비스 애플리케이션을 생성 및 설정 합니다.  \n> 원하는 정보를 요청하기 위해 API를 파악합니다.\n\n- 카카오 로그인 이해하기: [Kakao Developers - 카카오 로그인 이해하기](https://developers.kakao.com/docs/latest/ko/kakaologin/common)\n\n- 카카오 애플리케이션 생성 및 설정하기: [Kakao Developers - 카카오 로그인 설정하기](https://developers.kakao.com/docs/latest/ko/kakaologin/prerequisite)\n\n- 카카오 REST API 확인하기: [Kakao Developers - 카카오 로그인 REST API](https://developers.kakao.com/docs/latest/ko/kakaologin/rest-api)\n\n<br><br><br><br>\n\n\n## 환경 구성하기\n>우리는 클라이언트의 입장으로 카카오 서버에 요청을 보내야 합니다.  \n> 이를 위해 먼저 내부에서 요청을 보낼 클라이언트 객체를 생성해 줍니다.\n\n<br><br>\n\n### RestTemplate\n> 스프링 어플리케이션에서 HTTP 요청할 때 사용하는 방법으로 `RestTemplate`과 `WebClient`가 있습니다.  \n> 우리는 그 중 `RestTemplate`를 이용할 것입니다.   \n> 관련 내용은 글의 마지막에 다루도록 하겠습니다.\n\n기본 스프링 부트 의존성을 추가하면 `RestTemplate` 관련 의존성은 자동으로 추가됩니다.\n\n```groovy\nimplementation 'org.springframework.boot:spring-boot-starter-web'\n```\n\n\n<br><br>\n\n`RestTemplate`은 자동으로 빈 등록이 되지 않기 때문에 아래와 같이 직접 등록해주어야 합니다.\n\n```java\n@Configuration\npublic class AuthConfig {\n\n    @Bean\n    public RestTemplate restTemplate(RestTemplateBuilder restTemplateBuilder) {\n        return restTemplateBuilder.build();\n    }\n}\n```\n\n<br><br>\n\n\n\n### 클라이언트 객체 생성하기\n\nHTTP 통신을 위한 `RestTemplate` 세팅이 끝났으니 클라이언트 역할을 할 객체를 생성합니다.  \n우리는 이 클라이언트 객체를 이용하여 액세스 토큰 요청, 사용자 정보 요청, 회원 탈퇴 요청 등 카카오 서버로 다양한 요청을 보낼 것입니다.\n\n```java\n@Component\n@RequiredArgsConstructor\npublic class OAuthProvider {\n\n    public static final String AUTHORIZATION_HEADER = \"Authorization\";\n    public static final String GRANT_TYPE = \"authorization_code\";\n    public static final String TOKEN_TYPE = \"Bearer \";\n\n    @Value(\"${auth.kakao.token-request-uri}\")\n    private String tokenRequestUri;\n\n    @Value(\"${auth.kakao.member-info-request-uri}\")\n    private String memberInfoRequestUri;\n    \n    @Value(\"${auth.kakao.redirect-uri}\")\n    private String redirectUri;\n\n    @Value(\"${auth.kakao.client-id}\")\n    private String clientId;\n\n    private final RestTemplate restTemplate;\n\n}\n\n```\n\n\n\n- 액세스 토큰 요청, 사용자 정보 요청, 회원 탈퇴 요청 등의 요청 시 `RestTemplate`을 반복적으로 사용해야하기 때문에 필드로 두었습니다.\n\n- 카카오 서버로 요청을 보낼 때 반복적으로 사용되는 정보들을 필드로 두었습니다.\n\n<br><br>\n\n\n### 민감 정보 관리\n> 요청에 사용되는 정보들 중 REST API 키나 SECRET과 같은 민감한 정보들은 `@Value` 애너테이션과 `application.properties` 파일을 이용해서 분리하여 관리할 수 있습니다.\n\n\n`application.properties`는 아래와 같이 작성할 수 있습니다.  \n(피움 프로젝트에서는 `properties` 파일을 사용하였지만, `yml` 파일도 동일한 방식으로 사용할 수 있습니다)\n\n```properties\nauth.kakao.token-request-uri= 액세스 토큰을 요청할 카카오 URI\nauth.kakao.member-info-request-uri= 사용자 정보를 요청할 카카오 URI\nauth.kakao.redirect-uri= kakao developers에서 설정한 리다이렉트 URI\nauth.kakao.client-id= kakao developers에서 애플리케이션 생성 시 발급 받은 REST API 키\n```\n\n\n\n해당 내용에 대해 더 알아보고 싶으시다면 아래 글을 참고하시면 좋을 것 같습니다.\n\n- [Tecoble - Spring Boot에서 properties 값 주입받기](https://tecoble.techcourse.co.kr/post/2020-09-29-spring-properties-binding/)\n\n\n<br><br><br><br>\n\n\n## 로그인 구현하기\n카카오 로그인은 OAuth 2.0 프로토콜 중 `Authorization Code Grant` 방식을 사용합니다.  \n그렇기 때문에 우리가 수행해야 할 요청은 아래와 같이 3개로 나눠볼 수 있습니다.\n\n- `authorization code` 요청\n\n- `access token` 요청\n\n- 사용자 정보 요청\n\n<br><br>\n\n### 동작 과정\n> 피움 서비스에서 구현한 로그인은 아래와 같이 동작합니다.  \n> 피움의 경우 사용자 인증 방식으로 세션을 사용하고 있습니다.  \n> 이 글에서는 OAuth 2.0에 대해 다루고 있으므로 관련 내용은 생략합니다.\n\n \n![pium-oauth-arc.png](.index_images/img.png)\n\n\n위 그림과 같이 카카오 서버에 요청해야하는 정보 중 하나인 `authorization code`는 피움의 클라이언트에서 카카오 서버에 요청한 후 피움 서버로 로그인 요청 시 사용합니다.   \n즉, **피움 서버에서는 카카오 서버로 액세스 토큰 요청과 사용자 정보 요청만 진행**할 것입니다.\n\n<br><br>\n\n### 액세스 토큰 요청하기\n> [KAKAO REST API - 토큰받기](https://developers.kakao.com/docs/latest/ko/kakaologin/rest-api#request-token)를 참고하여 액세스 토큰을 요청합니다.\n\n#### 요청\n\n```\ncurl -v -X POST \"https://kauth.kakao.com/oauth/token\" \\\n -H \"Content-Type: application/x-www-form-urlencoded\" \\\n -d \"grant_type=authorization_code\" \\\n -d \"client_id=${REST_API_KEY}\" \\\n --data-urlencode \"redirect_uri=${REDIRECT_URI}\" \\\n -d \"code=${AUTHORIZE_CODE}\"\n```\n\n#### 응답\n\n```\nHTTP/1.1 200 OK\nContent-Type: application/json;charset=UTF-8\n{\n    \"token_type\":\"bearer\",\n    \"access_token\":\"${ACCESS_TOKEN}\",\n    \"expires_in\":43199,\n    \"refresh_token\":\"${REFRESH_TOKEN}\",\n    \"refresh_token_expires_in\":5184000,\n    \"scope\":\"account_email profile\"\n}\n```\n\n<br><br>\n\n#### KaKaoAccessTokenResponse Dto\n\n액세스 토큰만 받을 것이기 때문에 `String`으로 받을수도 있으나, 이후에 추가적인 정보를 받는 것으로 명세가 변경될 수 있기에 DTO를 작성하였습니다.\n\n```java\n@Getter\n@Builder\n@NoArgsConstructor(access = AccessLevel.PRIVATE)\n@AllArgsConstructor(access = AccessLevel.PRIVATE)\npublic class KaKaoAccessTokenResponse {\n\n    @JsonProperty(\"access_token\")\n    private String accessToken;\n}\n```\n\n<br><br>\n\n#### 카카오 서버로 액세스 토큰 요청하기\n\n```java\n@Component\n@RequiredArgsConstructor\npublic class OAuthProvider {\n\n    // ...\n\n    public KaKaoAccessTokenResponse getAccessToken(String authorizationCode) {\n        try {\n            HttpHeaders httpHeaders = new HttpHeaders();\n            httpHeaders.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n\n            MultiValueMap<String, String> body = new LinkedMultiValueMap<>();\n            body.add(\"grant_type\", GRANT_TYPE);\n            body.add(\"client_id\", clientId);\n            body.add(\"redirect_uri\", redirectUri);\n            body.add(\"code\", authorizationCode);\n\n            HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(body, httpHeaders);\n\n            return restTemplate.postForEntity(\n                    tokenRequestUri, request, KaKaoAccessTokenResponse.class).getBody();\n        } catch (HttpClientErrorException e) {\n            throw new KakaoTokenRequestException(e.getMessage());\n        } catch (HttpServerErrorException e) {\n            throw new KakaoServerException(e.getMessage());\n        }\n    }\n\n    // ...\n}\n\n```\n\n- 헤더 생성\n\n    - 카카오 REST API 명세에 따라 `HttpHeaders`를 이용해 header를 생성하여 Content Type을 `application/x-www-form-urlencoded` 로 설정합니다.\n\n- 바디 생성\n\n    - `MultiValueMap`을 이용하여 요청 시 사용될 body 를 `key-value` 형태로 만들어줍니다.\n    - 카카오 REST API 명세에 따라 grant_type, client_id, redirect_uri, code 를 파라미터로 넣어줍니다.\n\n- 리퀘스트 생성\n\n    - 앞서 설정한 header와 body로 `HttpEntity`를 생성합니다.\n\n- 카카오 서버에 요청하기\n\n    - 카카오 REST API 명세에 따라 POST 메서드를 이용해서 요청을 보낼 것 입니다.\n    - [RestTemplate](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/client/RestTemplate.html)에서 제공하는 POST 메서드는 총 3가지로 목적에 따라 `postForObject()`, `postForEntity()`, `postForLocation()` 중에 선택해서 사용할 수 있습니다.\n    - 우리는 응답 값을 `ResponseEntity`로 받을 것이므로`RestTemplate`의 `postForEntity()` 메서드를 이용하여 요청을 보냈습니다.\n    - 인자 값으로는 `URI`, `Request`, `ResponseType` 을 넣어줍니다.\n\n<br><br>\n\n\n\n### 사용자 정보 요청하기\n> [KAKAO REST API - 사용자 정보 가져오기](https://developers.kakao.com/docs/latest/ko/kakaologin/rest-api#req-user-info)를 참고하여 응답으로 받은 액세스 토큰으로 사용자 정보를 요청합니다.\n\n#### 요청\n\n```\ncurl -v -X POST \"https://kapi.kakao.com/v2/user/me\" \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -H \"Authorization: Bearer ${ACCESS_TOKEN}\" \\\n```\n\n#### 응답\n응답 값은 수집 항목에 따라 달라질 수 있습니다.  \n피움 프로젝트의 경우 사용자의 회원 ID만 수집하므로 다른 정보는 생략하였습니다.\n\n```\nHTTP/1.1 200 OK\n{\n    \"id\":123456789,\n    \"connected_at\": \"2022-04-11T01:45:28Z\"\n    ...\n}\n```\n\n<br><br>\n\n#### KakaoMemberResponse Dto\n\n회원 ID 역시 Long으로 바로 받을수도 있으나, 이후에 추가적인 정보를 받는 것으로 명세가 변경될 수 있기에 DTO를 작성하였습니다.\n\n```java\n@Getter\n@Builder\n@NoArgsConstructor(access = AccessLevel.PRIVATE)\n@AllArgsConstructor(access = AccessLevel.PRIVATE)\npublic class KakaoMemberResponse {\n\n    private Long id;\n}\n```\n\n<br><br>\n\n#### 카카오 서버로 사용자 정보 요청하기\n\n```java\n@Component\n@RequiredArgsConstructor\npublic class OAuthProvider {\n\n    // ...\n\n    public KakaoMemberResponse getMemberInfo(String accessToken) {\n        try {\n            HttpHeaders httpHeaders = new HttpHeaders();\n            httpHeaders.set(AUTHORIZATION_HEADER, TOKEN_TYPE + accessToken);\n            httpHeaders.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n\n            HttpEntity<Object> request = new HttpEntity<>(httpHeaders);\n\n            return restTemplate.postForEntity(memberInfoRequestUri, request, KakaoMemberResponse.class)\n                    .getBody();\n        } catch (HttpClientErrorException e) {\n            throw new KaKaoMemberInfoRequestException(e.getMessage());\n        } catch (HttpServerErrorException e) {\n            throw new KakaoServerException(e.getMessage());\n        }\n    }\n\n    public KaKaoAccessTokenResponse getAccessToken(String authorizationCode) {\n        // ...\n    }\n\n    // ...\n}\n\n```\n\n\n- 헤더 생성\n\n    - 카카오 REST API 명세에 따라 `HttpHeaders`를 이용해 header를 생성하여 Content Type을 `application/x-www-form-urlencoded` 로 설정합니다.\n    - 카카오 REST API 명세에 따라 authorization 헤더를 추가하여 액세스 토큰 값을 넣어줍니다.\n\n- 리퀘스트 생성\n\n    - 앞서 설정한 header로 `HttpEntity`를 생성합니다.\n\n- 카카오 서버에 요청하기\n\n    - 이번에도 카카오 REST API 명세에 따라 POST 메서드를 이용해서 요청을 보낼 것 입니다.\n    - 이번에도 응답을`ResponseEntity`로 받을 것이므로`RestTemplate`의 `postForEntity()` 메서드를 이용하여 요청을 보냅니다.\n    - 인자 값으로는 `URI`, `Request`, `ResponseType` 을 넣어줍니다.\n\n<br><br>\n\n\n### controller 구현하기\n> 피움 클라이언트에서 로그인 요청을 보낼 수 있도록 controller 및 api를 구현합니다.  \n> controller의 경우 각 서비스의 명세에 맞게 작성하시면 됩니다.\n\n#### 요청\n\n```\nPOST \"https://pium.life/login?code={AUTHORIZATION_CODE}\"\n```\n\n#### 응답\n\n```\nHTTP/1.1 200 OK\n```\n\n<br><br>\n\n#### controller\n```java\n@Validated\n@RestController\n@RequiredArgsConstructor\npublic class AuthController {\n\n    private final AuthService authService;\n\n    @PostMapping(\"/login\")\n    public ResponseEntity<Void> login(\n            @RequestParam(name = \"code\") @NotBlank String code) {\n        Member loginMember = authService.login(code);\n        \n        // ...\n        \n        return ResponseEntity.ok().build();\n    }\n}\n\n```\n\n<br><br>\n\n\n### service 구현하기\n> 앞서 구현한 `OAuthProvider`를 호출하여 로그인을 처리합니다.\n> 해당 레이어에서 각 서비스의 규정에 따라 서비스 내 인증 방식(토큰, 세션 등)을 적용할 수 있습니다.   \n> (이 글에서는 OAuth 2.0를 이용한 로그인만 다루고 있기 때문에 해당 내용은 생략합니다)\n\n\n```java\n@Service\n@Transactional(readOnly = true)\n@RequiredArgsConstructor\npublic class AuthService {\n\n    private final OAuthProvider provider;\n\n    @Transactional\n    public Member login(String authorizationCode) {\n        KaKaoAccessTokenResponse accessTokenResponse = provider.getAccessToken(authorizationCode);\n        String accessToken = accessTokenResponse.getAccessToken();\n\n        KakaoMemberResponse kakaoMemberResponse = provider.getMemberInfo(accessToken);\n        \n        // ...\n    }\n}\n```\n\n<br><br><br><br>\n\n## 추가로 고려해 볼 사항\n\n### RestTemplate vs WebClient\n스프링 어플리케이션에서 HTTP 요청할 때 사용하는 방법으로 [RestTemplate과 WebClient](https://tecoble.techcourse.co.kr/post/2021-07-25-resttemplate-webclient/)가 있습니다.\n\n현재 스프링 부트를 사용하고 있기 때문에 RestTemplate을 사용한다면 별도의 의존성 추가가 필요하지 않습니다. 하지만 WebClient를 사용하기 위해서는 webflux 의존성을 추가해주어야 합니다.\n또한 로그인 처리를 위해 카카오 서버의 응답이 필수적이기 때문에 WebClient의 특징이 큰 이점이 되지 않는다고 느껴 `RestTemplate`을 사용하였습니다.\n\n만약 이후 서비스 규모가 커지고, deprecated 소식이 있다면 `WebClient`로의 대체를 고려할 수도 있을 것 같습니다.\n\n다른 대체 방안으로 Spring Boot3.2에 새롭게 추가될 [RestClient](https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-3.2.0-M1-Release-Notes) 도 고려해볼 수 있겠습니다.\n\n<br><br>\n\n### 다수의 Provider 고려하기\n현재 피움 서비스에서는 카카오 로그인만을 제공하고 있습니다.\n만약 이후에 구글 로그인, 네이버 로그인 등 다른 provider를 이용하게 된다면 현재의 코드로는 적용이 불가합니다.  \n이 점을 고려하여 추상화를 통해 유연한 로그인 서비스를 제공하는 방법을 생각해볼 수 있겠습니다.\n\n<br><br><br><br>\n\n## 끝으로\n\nOAuth 2.0 을 통해 카카오 로그인을 구현하는 방법을 알아보았습니다.  \n다음 글에서는 구현한 로그인에 대한 테스트 방법을 다뤄보도록 하겠습니다.  \n글을 읽어주셔서 감사합니다!  \n\n\n\n\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. 문제: 무한 스크롤 시에 데이터 fetch가 제대로 이루어지지 않는다. 에러 구현 방법: 특정 식물 상세보기 접속 → 타임라인 페이지 접속 → 뒤로가기 → 해당 식물 정보 수정 → 타임라인 페이지 접속 서론 ‘피움’ 서비스 에서는 식물의 관리 이력(식물 물 주기, 물 주기 Cycle 변화, 식물의 환경…","fields":{"slug":"/tanstack-query-cache-trouble-shooting/"},"frontmatter":{"date":"August 22, 2023","title":"InfinityQuery에서 fetch가 제대로 이루어지지 않는다?!?!","tags":["Tanstack Query","Cache","useInfititeQuery"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다.\n\n### 문제: 무한 스크롤 시에 데이터 fetch가 제대로 이루어지지 않는다.\n\n### 에러 구현 방법: 특정 식물 상세보기 접속 → 타임라인 페이지 접속 → 뒤로가기 → 해당 식물 정보 수정 → 타임라인 페이지 접속\n\n## 서론\n\n‘피움’ 서비스 에서는 식물의 관리 이력(식물 물 주기, 물 주기 Cycle 변화, 식물의 환경 변화 등)을 확인할 수 있는 ‘타임라인’ 서비스를 제공하고 있습니다. 이 타임라인 기능에서는 다른 페이지로 이동할 필요 없이 아래로만 스크롤 하면 다음 데이터가 나오는 ‘무한 스크롤’ 기능을 사용자들에게 제공하고 있는데, 이 무한 스크롤을 좀 더 편리하게 사용하기 위해서 React-Query에서 제공하는 [useInfiniteQuery](https://tanstack.com/query/v5/docs/react/reference/useInfiniteQuery)를 통해 무한 스크롤을 구현하고 있습니다. \n\n### 본론\n\n사건의 발단은 다음과 같습니다. 식물의 관리 이력을 수정하고 변경 내역이 타임라인에 기록되었는지를 확인하기 위해 타임라인을 클릭 했더니 **수정하기 전 타임라인을 제공하고 있던 것이었습니다.** 당시에 운영 서비스 배포하기로 결정한 당일 이었기 때문에 상당히 당황한 기억이 있습니다. \n\n문제를 찾아보기 위해 `console`을 통해서 데이터 fetch 순서를 한번 봤습니다. 처음에 접속해서 캐시되어 있던 데이터가 먼저 찍히는 것을 볼 수 있습니다. 그 다음에 타임라인에 사용될 데이터를 보여주고, 데이터 통신이 완료가 됩니다. `GET /history` 이력을 보면 데이터 fetch가 완료된 것을 볼 수 있는데, 데이터 fetch를 하고 끝이 납니다. 마지막으로 fetch한 데이터가 가장 최신의 데이터이고 타임라인에 적용되어야 할 데이터인데, 전혀 적용되지 않고 fetch만 하고 끝이 됩니다. 이 때문에 타임라인에는 이전 내용이 적용되어서 데이터가 나타나는 것입니다. 문제는 언제는 또 제대로 작동이 된다는 것 이었습니다. 이렇게 일관성 없는 fetch 동작에 상당히 많은 혼란이 있었습니다.\n\n- `Response Data`: 처음 fetch 되었을 때 나온 데이터\n- `Select Data`: fetch된 데이터를 가공하기 위해 `select` 옵션을 사용한 과정에서 찍히는 데이터\n- `TimeLine Data`: 실제 타임라인에 사용될 데이터\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/3b122e0d-2404-41a6-856e-0f937de9938c)\n\n문제의 원인을 ‘캐시된 데이터’라고 추정한 가운데 생각한 해결책은 ‘데이터를 fetch 할 때마다 캐시를 하지 않게 하려면 어떻게 해야 하나?’ 였습니다. React-Query에 대해서 좀 더 자세히 알고 있었다면 금방 해결 됐을 문제이지만, 옵션 들에 대해 그렇게 자세하게 알지 못했고, 애꿎은 `queryKey`와 `enable` 등을 건드렸다. [Dependent Queries](https://tanstack.com/query/v4/docs/react/guides/dependent-queries)를 사용하면서, enable 옵션에 상태 값을 할당하는데, 바로 호출하는 방식이 클로저에 갇혀서 계속 같은 값으 제공하나? 이런 생각도 하고, queryKey가 같아서 계속 캐싱을 하나… filter를 없애야 하나 하는 생각도 했습니다. \n\n이렇게 여러 가지를 시도해 보다가 한 가지 원인을 알게 되었는데, React-Query가 제공하는 [캐시 예제](https://tanstack.com/query/v5/docs/react/guides/caching)에 있는 것과 동일합니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/8686d632-4fba-4dad-a303-d9ec16342890)\n\n\n> 캐시의 시간이 완료되기 전에 똑같은 인스턴스가 마운트 된다면. 해당 쿼리는 즉시 사용 가능한 캐시 데이터를 반환하고, `queryFn`은 background에서 실행이 됩니다. 그리고 **해당 fetch가 성공적으로 완료된다면, 방금 전에 fetch한 데이터가 캐시 데이터로 들어가는 것입니다.**\n> \n\n즉, 저희가 겪었던 문제는 기본적으로 설정되어 있는 옵션들 (`staleTime:0, gcTime:5분`)에서는 자연스럽게 발생하는 문제들인 것이었습니다. 최초로 요청한 인스턴스가 가비지 콜렉션에 들어가기 전에 계속해서 같은 인스턴스를 요청하였고, 동작 원리에 따라 5분간은 같은 결과 (2 - 3 - 1 순서, 캐시 → fetch)로 작동하게 된 것이었습니다.\n\n결국에 이를 해결하기 위해서는 캐시 시간을 짧게 설정하면 되는 문제였고, 타임라인의 경우에는 캐시를 하지 않겠다는 의미로 `gcTime`을 0으로 설정하여 해결할 수 있는 문제였습니다.\n\n### 결론\n\n이번 트러블 슈팅을 하면서 크게 2가지 과정에서 문제가 있지 않았나 하는 생각이 들었습니다. \n\n첫 번째는 “잘 알지 못하는 라이브러리를 도입했다. 이 때문에 배포 당일 날에 문제를 마주했고, 쫄깃한 경험을 하게되었다.” 입니다. 물론 학습하는 입장에서 매우 좋은 경험이었지만, 실제 상황이었다면 살짝 아찔한 순간이었을 것 같습니다. \n\n두 번째는 이러한 문제를 개발 서버에서 확인했다는게 약간 치명적이지 않았나 라는 생각이 들었습니다. msw를 통해서 mockAPI를 통해서 모든 처리를 하고 있는데, 해당 API 통신에 대한 빡빡한 검증이 부족했던 것 같기도 하고, 테스트의 범위를 어디까지 정해야 하는 지도 살짝 애매한 느낌이 들긴 했습니다. 타임라인 이력을 불러오는 API을 구현한다면 잘 작성한 mockAPI이지만, e2e테스트로 생각을 한다면 살짝 부족한 과정이지 않았나를 생각하면서 e2e테스트의 중요성과 msw를 작성하는데 공식적으로 사용할만한 DB가 없었다는게 살짝 아쉬워 지는 트러블 슈팅이었습니다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '그레이'가 작성했습니다. 사건의 발단 현재 구동 중인 개발 서버와 운영 서버를 캠퍼스외의 장소에서 빌드해 배포했을 때, 내부 로그를 확인할 수 없는 문제가 있었습니다. 인스턴스 내부에 발생한 로그나, nginx에 남겨진 로그를 확인하기 위해서는 캠퍼스에서 직접 ssh 연결을 통해 접속해서 확인해야 합니다. 서버 외부에서도 인…","fields":{"slug":"/server-monitoring/"},"frontmatter":{"date":"August 17, 2023","title":"CloudWatch를 이용한 모니터링 환경 구성","tags":["AWS","CloudWatch","로그 모니터링","자원 모니터링"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[그레이](https://github.com/Kim0914)'가 작성했습니다.\n\n## 사건의 발단\n현재 구동 중인 개발 서버와 운영 서버를 캠퍼스외의 장소에서 빌드해 배포했을 때, 내부 로그를 확인할 수 없는 문제가 있었습니다.\n\n인스턴스 내부에 발생한 로그나, nginx에 남겨진 로그를 확인하기 위해서는 캠퍼스에서 직접 ssh 연결을 통해 접속해서 확인해야 합니다.\n\n서버 외부에서도 인스턴스 상태를 확인해야 하는 필요성이 느껴졌고, 모니터링 툴을 도입하기로 결정했습니다.\n\n\nCloudWatch는 AWS에서 제공하는 모니터링 툴입니다.\n\n\n\n기존에 프로메테우스와 그라파나를 활용해 모니터링을 할 수 있었지만, 현재 보유 중인 서버가 모두 가용 중이고 가용 중인 서버에 모니터링 툴을 설치하는 경우 다른 서버들이 영향을 받을 수 있기 때문에 별도의 툴이 필요했습니다.\n\n\n\n그래서 EC2 인스턴스를 추가로 생성해 모니터링용 서버를 구축할 수 있었으나, 비용(금전)적으로 cloud watch를 사용하는 것이 훨씬 비용 절감이 되고 적용과정도 비교적 간편하기 때문에 AWS CloudWatch를 사용하기로 결정했습니다.\n\n\n## CloudWatch 적용 과정\n클라우드 와치를 인스턴스에 적용하기 위해서는 권한이 필요합니다.\n\n\n\nIAM을 발급 받는 방법은 다른 블로그에서 쉽게 찾을 수 있기 때문에 생략하고, 적용하는 방법부터 알아보겠습니다.\n\n\n\n*1. 인스턴스 대시보드로 이동해  인스턴스 우클릭 -> 보안 -> IAM 역할 수정을 누르고 발급받은 IAM을 추가합니다.**\n\n![](.index_images/img.png)\n\n**2. CloudWatch 탭으로 이동해 대시보드 생성을 누르고 원하는 이름을 자유롭게 설정하면 됩니다.**\n\n\n![](.index_images/img_1.png)\n\n\n\n대시보드가 처음 생성되면 아래와 같은 화면을 확인할 수 있습니다.\n\n여기서 왼쪽 하단에 있는 로그, 지표를 활용하여 인스턴스의 상태를 확인할 수 있습니다.\n\n![](.index_images/img_2.png)\n\n*3. 지표 추가하기**\n\n지표 -> 모든 지표를 누르면 아래와 같이 인스턴스 탭을 확인할 수 있습니다. 인스턴스 이외에도 다른 탭도 확인할 수 있습니다.\n![](.index_images/img_3.png)\n\nEC2 탭을 누르고 들어오면 아래와 같은 화면을 볼 수 있고 인스턴스별 지표를 확인해야 하기 때문에 인스턴스별 지표를 누릅니다.\n\n![](.index_images/img_4.png)\n\n인스턴스 지표를 누르면 현재 AWS에 가동 중인 모든 인스턴스를 확인할 수 있습니다.\n\n상단에 있는 검색 탭에 관측하고 싶은 인스턴스의 ID를 입력합니다. 예를 들면 i-0123456789 가 됩니다.\n\n![](.index_images/img_5.png)\n\n**4. 원하는 지표 확인하기**\n\n![](.index_images/img_6.png)\n\n**5. 대시보드에 지표 추가하기**\n\n다시 대시보드로 돌아와서 관측하고 싶은 지표를 추가합니다.\n\n우측 상단에 있는 + 버튼을 누르면 지표나 로그를 추가하는 화면을 볼 수 있습니다.\n\n![](.index_images/img_7.png)\n\n어떤 그래프로 확인할 지 그래프를 선택한 후 다음을 눌러 앞 서 모든 지표에서 확인했던 방법과 동일하게 지표를 선택합니다.\n\n![](.index_images/img_8.png)\n\n위젯 생성 버튼을 눌러 위젯을 생성합니다.\n\n![](.index_images/img_9.png)\n\n추가된 위젯을 확인하고 저장버튼을 눌러 저장합니다.\n\n![](.index_images/img_10.png)\n\n## CloudWatch Agent 설치\nCloudWatch에서 기본적으로 제공하는 정보는 CPU, Network I/O 등과 같은 기본적인 정보만 제공합니다.\n\n인스턴스의 메모리, 스왑 메모리, 디스크 I/O 등 보다 자세한 정보를 확인하기 위해서는 `CloudWatch Agent`를 설치해야 합니다.\n\n\n\nAWS CloudWatch 공식 문서에 설치 방법이 자세히 잘 나와있습니다.\n\n\n\n저희 팀은 `Ubuntu22.04 ARM`을 사용 중이므로, Ubuntu22.04 ARM 기준의 설치 방법을 알려드리겠습니다.\n\nCloudAgent 패키지를 다운로드하는 과정 이외에는 다른 환경도 동일하게 적용됩니다.\n\n**1.ARM or AMD 확인**\n\n\n우선 사용하고 있는 인스턴스의 아키텍처를 확인합니다.\n\n```java\nlscpu | grep \"Vendor ID\"\n```\n\n![](.index_images/img_11.png)\n\n**2. Cloudwatch Agent 다운**\n\n\n다음 명령어를 통해 Agent를 다운로드합니다.\n\n```shell\nsudo wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/arm64/latest/amazon-cloudwatch-agent.deb\n```\n\n**여기서 주의할 점 !**\n\n\n\n클라우드 와치를 다운받은 후 설정을 마치고 적용하는 과정에서 알 수 없는 **403 예외가 발생**했었습니다.\n\n\n\n[processors.ec2tagger] ec2tagger: unable to describe ec2 tags for initial retrieval: unauthorizedoperation: you are not authorized to perform this operation.\n\n\n\n구글링을 해보니 IAM 권한과 관련된 예외인 것 같았는데, IAM에 접근할 수 있는 권한이 없어서 몇 시간 동안 헤매었습니다..\n\n\n\n결론은 `sudo`와 함께 패키지를 설치하니 해결되었습니다. sudo ! \n\n![](.index_images/img_12.png)\n\n\n**3. 패키지를 디렉토리로 압축 해제**\n\n\n```shell\nsudo dpkg -i -E ./amazon-cloudwatch-agent.deb\n```\n\n**4. CloudWatch Agent시작**\n\n\n압축 해제까지 모두 마치면 클라우드 와치를 적용하는 일만 남았습니다.\n\n여기서 클라우드 와치를 적용할 수 있는 방법은 2가지가 있는데 직접 config.json 파일을 생성하거나 wizard를 이용하여 시작할 수 있습니다. Wizard로 시작하게 되면 기본 config.json 파일을 생성해 주기 때문에 저는 wizard로 시작했습니다.\n\n\n`Wizard`를 실행하는 명령어입니다.\n\n```shell\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard\n```\n\n명령어를 입력하면 아래와 같이 질문에 대한 답을 입력하는 문구가 나옵니다.\n\n![](.index_images/img_13.png)\n\n웬만한 설정은 default 설정을 따라기시면 됩니다.\n\n한 번씩 읽어보고 상황에 맞는 설정을 해주는 것이 좋습니다.\n\n![](.index_images/img_14.png)\n\n설정을 진행하다 보면 **현재까지 작성된 config 파일 정보를 확인할 수 있습니다.**\n\n![](.index_images/img_15.png)\n\n계속해서 진행하다 보면 `Do you want to monitor any log files?`이라는 질문이 나옵니다.\n\n인스턴스 내에 **로그를 클라우드 워치로 전송하기를 원하면 1, 그렇지 않으면 2**를 누르면 됩니다.\n\n\n\n**지금 설정하지 않아도, 이후에 파일로 가서 직접 수정할 수 있기 때문에 크게 중요하지 않습니다 !!**\n\n\n\nDo you want to store the config in the SSM parameter store?\n\n\n\n질문에는 `2. no` 를 선택하시면 됩니다.\n\n\n\n설정이 끝나면 `/opt/aws/amazon-cloudwatch-agent/bin/` 위치에 `config.json` 파일이 생성됩니다.\n\n\n\nconfig.json 파일을 열어봅시다.\n\n![](.index_images/img_16.png)\n\n크게 `agent`, `logs`, `metrics`로 나뉘는데 agent는 계정과 권한에 관련된 부분이고 logs는 로그와 관련된 부분, metrics는 인스턴스 내의 모니터링과 관련된 부분입니다. 자세한 설정은 CloudWatch docs를 참고하시면 됩니다 !\n\n\n\nmetrics에는 mem 이외에 cpu, disk, swap 등이 있습니다.\n\n\n\nhttps://docs.aws.amazon.com/ko_kr/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html#CloudWatch-Agent-Configuration-File-Metricssection\n\n관측하고 싶은 자원에 대한 설정을 모두 마친 후 아래의 명령어를 실행해 CloudWatch Agent를 실행합니다.\n\n```shell\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s\n```\n\n/opt/aws/amazon-cloudwatch-agent/logs에 있는 amazon-cloudwatch-agent.log를 확인하면 정상적으로 실행되는지 로그를 확인할 수 있습니다.\n\n![](.index_images/img_17.png)\n\n**5. 대시보드에 추가**\n\n\n대시보드로 돌아와 위젯 추가를 눌러 지표를 확인합니다.\n\n아래와 같이 사용자 지정 네임스페이스 구역을 확인합니다. 저는 config.json을 설정할 때 네임스페이스를 따로 설정하지 않았으므로 기본인 CWAgent에 추가됩니다.\n\n![](.index_images/img_18.png)\n\nCWAgent -> host로 이동해 hostname을 보면 인스턴스 아이디로 설정되어 있습니다.\n\n본인의 인스턴스 아이디를 찾아서 위젯으로 추가하면 됩니다.\n\n**6. CloudWatch로 로그 관측하기**\n\n인스턴스 내에서 남기고 있는 로그를 클라우드 워치를 이용해 관측할 수 있습니다. 예를 들어 스프링 서버에서 남기고 있는 로그, NginX에서 남기고 있는 로그등을 클라우드 워치로 관측할 수 있습니다.\n\n\n\n지표를 설정했던 것과 동일하게 config.json 파일에 원하는 로그 파일을 설정하면 됩니다.\n\nlogs -> logs_collected -> files -> collect_list의 value 값에 로그 파일 위치, 로그 그룹 이름, 로그 스트림 이름을 설정합니다.\n\n![](.index_images/img_19.png)\n로그 그룹은 클라우드 워치 왼쪽의 로그 -> 로그 그룹에서 생성할 수 있습니다.\n\n로그 스트림 이름은 config.json 파일에 명시한 log_stream_name이 로그 그룹에 존재하지 않으면 자동으로 생성합니다.\n\n\n\n- file_path: 인스턴스 내에 로그 파일의 경로\n\n- log_group_name: CloudWatch에서 생성한 로그 그룹 이름\n\n- log_stream_name: 원하는 로그 스트림 이름. 로그 스트림으로 클라우트 와치에서 필터링할 수 있습니다.\n\n```json\n{\n        \"agent\": {\n                \"metrics_collection_interval\": 60\n        },\n        \"logs\": {\n                \"logs_collected\": {\n                        \"files\": {\n                                \"collect_list\": [\n                                        {\n                                                \"file_path\": \"/home/ubuntu/log/pium-prod-**.log\",\n                                                \"log_group_name\": \"2023-pium-prod\",\n                                                \"log_stream_name\": \"spring-prod-log\"\n                                        },\n                                        {\n                                                \"file_path\": \"/var/log/nginx/access.log\",\n                                                \"log_group_name\": \"2023-pium-prod\",\n                                                \"log_stream_name\": \"nginx-access-log\"\n                                        },\n                                        {\n                                                \"file_path\": \"/var/log/nginx/error.log\",\n                                                \"log_group_name\": \"2023-pium-prod\",\n                                                \"log_stream_name\": \"nginx-error-log\"\n                                        }\n                                ]\n                        }\n                }\n        }\n}\n```\n\nconfig.json 파일을 수정한 후 다시 **에이전트 실행 명령어를 입력해 수정된 config 파일 내용을 적용시킵니다.**\n\n```shell\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s\n```\n\n정상적으로 적용되면 로그 그룹 내에 로그 스트림이 생성되는 것을 확인할 수 있습니다.\n\n![](.index_images/img_20.png)\n\n**생성된 로그 스트림을 바탕으로 대시보드에 로그 위젯을 추가할 수 있습니다.**\n\n지표 대신 로그를 선택해서 추가할 수 있습니다.\n\n**로그 별로 위젯을 구분하려면 쿼리를 실행할 때 filter 옵션을 추가**해 주시면 됩니다.\n\n![](.index_images/img_21.png)\n![](.index_images/img_22.png)\n![](.index_images/img_23.png)\n\n로그가 정상적으로 출력되는 것을 확인할 수 있습니다.\n\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '그레이'가 작성했습니다. 로깅이란 ? 우리가 처음 개발을 할 때 System.out.println(), cout << \"hello world\" << endl, print() 등으로 원하는 대로 동작하고 있는지 출력하곤 했을 것입니다. 또한 스프링 부트와 같은 프레임워크를 사용하면서 Log 객체를 이용해 로그를 남겨본 경험이 …","fields":{"slug":"/server-logging/"},"frontmatter":{"date":"August 16, 2023","title":"Logback을 이용해 운영 환경 별 로그 남기기","tags":["Spring","Logback"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[그레이](https://github.com/Kim0914)'가 작성했습니다.\n\n## 로깅이란 ?\n\n우리가 처음 개발을 할 때 System.out.println(), cout << \"hello world\" << endl, print() 등으로 원하는 대로 동작하고 있는지 출력하곤 했을 것입니다. 또한 스프링 부트와 같은 프레임워크를 사용하면서 Log 객체를 이용해 로그를 남겨본 경험이 있을 것입니다.\n\n\n\n이런 과정을 로깅이라고 할 수 있는데, 쉽게 말해 프로그램 동작시 발생하는 모든 일을 기록하는 행위를 말합니다.\n\n\n\n로깅을 통해 시스템 내부에 문제가 발생한 경우 쉽게 파악할 수 있고, 사용자 로그의 경우 분석의 도구로 사용될 수 있습니다.\n\n\n\n하지만 인터넷에서 발생하는 모든 요청에 대해 로그를 남기면 어떻게 될까요?\n\n로그의 양이 너무나 방대해지며 원하는 로그를 찾는 비용이 점점 커질 것입니다.\n\n\n\n어떻게 로깅을 효율적으로 할 수 있을까요?\n\n\n\n개발자가 원하는 정보를 알맞은 수준의 로그로 남길 수 있다면 효율적으로 로깅을 할 수 있습니다.\n\n또한 가용하고 있는 서버의 환경에 따라 적절한 로깅 전략을 가져간다면, 개발 과정에서도 로깅을 효율적으로 사용할 수 있을 것입니다.\n\n\n## Logback vs Log4j2\n\n스프링 환경의 대표적인 로깅 프레임워크는 `Logback`과 `Log4j2`가 있습니다.\n각각의 장단점이 있지만 현재 스프링 부트에서는 기본적으로 logback을 사용하기 때문에 주어진 기간 내에 빠르게 적용할 수 있고, \n현재 프로젝트의 규모에서는 logback과 log4j2 성능 차이가 크지 않을 것이라고 판단해 `logback`을 사용하기로 결정했습니다.\n\n\n## 스프링 부트에서 로그 남기기\n\n스프링 부트에서는 로그를 쉽게 남길 수 있도록 편의를 제공합니다.\n\n초기의 스프링은 JCK(Jakarta Commons Logging)을 사용해서 로깅을 구현했는데, 요즘에는 Log4j, Logback으로 스프링 부트의 로그 구현체를 사용합니다.\n\n![](.index_images/img_0.png)\n![](.index_images/img_0-1.png)\n\n\n스프링 부트는 기본적으로 Logback 설정이 되어 있습니다. 라이브러리를 살펴보면 slf4j를 사용하고 있음을 확인할 수 있습니다.\n\nSlf4j는 Simple Logging Facade for Java라는 뜻으로 다양한 로깅 프레임워크에 대한 추상화(인터페이스) 역할을 합니다.\n\nLogback은 Slf4j의 구현체이며 Log4j를 토대로 만든 프레임워크입니다.\n\n스프링 프레임워크에서도 slf4j와 Logback을 채택해서 사용하고 있습니다.\n\n![](.index_images/img_1.png)\n\n컨트롤러에서 LoggerFactory를 직접 생성해 로그를 남길 수도 있고\n\n![](.index_images/img_2.png)\n\n롬복에서 제공해 주는 @Slf4j 어노테이션을 통해 log 객체를 사용할 수 있습니다.\n\n\n\n롬복으로 @Slf4j 어노테이션을 사용하면 컴파일 타임에 다음과 같은 코드를 생성해 준다.\n\n```java\nprivate static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class);\n```\n\n지금까지 설명한 방법으로 로그를 남기면 실행 중인 애플리케이션의 콘솔에 로그가 남게 됩니다.\n\n즉 애플리케이션을 껐다 켜게 되면 별도의 output으로 빼서 처리하지 않는 한 해당 로그들이 사라지게 됩니다.\n\n\n\n또한 원하는 레벨 별로 로그를 확인하기 제한이 있고, 별도의 파일로 저장해 로그를 관리하기에도 제한이 있습니다.\n\n\n\n이런 상황을 해결할 수 있는, logback을 활용한 효과적으로 로깅할 수 있는 방법을 알아봅시다.\n\n## Logback을 활용해 효과적으로 로깅하기\n스프링 부트에서 콘솔 로그의 수준을 변경하는 방법은 `application.properties`나 `logback-spring.xml` 에서 설정하는 방법이 있습니다.\n\napplication.properties로 설정하는 방법이 있지만 세세한 설정을 하는데 제한이 있습니다.\n\n그러므로 logback-spring.xml로 관리하여 세부적인 설정을 할 수 있습니다.\n\n\n\n스프링 부트는 로깅 설정 파일의 네이밍에 관한 규칙을 가지고 있습니다.\n\n\n\n스프링 부트가 로딩되는 시점에 로그 설정 파일이 프로젝트 내에 존재하는지 스캔하는데\n\n\n\n1. logback-spring.xml\n\n2. logback-spring.groovy\n\n3. logback.xml\n\n4. logback.grooby\n\n\n\n등의 파일을 스캔하고 발견하면 해당 파일에 정의된 로그 설정을 적용합니다.\n\n\n\n여기서 logback-spring.xml은 Spring Boot에 특화된 설정 파일입니다.\n\n이 파일을 사용하면 Spring Boot의 로깅 확장 기능을 활용할 수 있습니다. 예를 들면, 프로파일에 따른 로그 레벨 설정이나 Spring 환경 변수를 로깅 설정에 사용하는 것 같은 기능들을 활용할 수 있습니다.\n\n\n\n만약 다른 파일 이름으로 된 로그 설정 파일을 사용하고 싶으면 logging.config 값을 변경한 후 프로그램 실행 시 JVM 인자로 설정하면 됩니다.\n\n\n\nLogback 설정을 하기 전, 구조를 간단하게 살펴보면 Logback은 크게 3가지 `Logger`, `Appender`, `Layout`의 구성 요소를 가지고 있습니다.\n\n![](.index_images/img_3.png)\n`Logger`는 로깅을 수행하는 주요 객체입니다. \n로거는 이름을 가지며 주로 패키지 이름 또는 클래스 이름과 일치하게 설정합니다. \n또한 로거는 계층 구조를 가지는데 ROOT 로거는 모든 로거의 상위 로거입니다.\n\n![](.index_images/img_4.png)\n`Appender`는 로깅 이벤트를 특정 출력 대상으로 전송하는 구성 요소입니다. \n쉽게 말해 로그 메세지가 출력할 대상을 결정한다고 할 수 있습니다. \n콘솔에 출력할지, 파일에 출력할지 등등.\n\n**각 로거는 하나 이상의 Appender에 연결될 수 있습니다.**\n\n![](.index_images/img_5.png)\n`Layout`은 로깅 이벤트가 어떻게 포맷되어 출력될지 결정하는 구성 요소입니다. \n사용자가 지정한 형식으로 로그 메세지를 변환한다는 것을 의미합니다. \n예를 들어 자바에서 문자열 포맷팅과 비슷하다고 할 수 있습니다.\n  \n  \n이제 로그를 남기는 방법을 알아보겠습니다.\n\n## logback-spring.xml 작성 & 환경 별 로그 작성\n\n### 1. **logback-spring.xml 생성**\n```html\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n <!-- 추가할 기능을 작성할 곳. -->\n</configuration>\n```\n### 2. **appender와 pattern을 이용해 로그 출력 위치와 출력되는 형식 지정**\n```html\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\"/>\n\n    <property name=\"CONSOLE_LOG_PATTERN\"\n              value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %clr(%5level) %cyan(%logger) - %msg%n\"/>\n\n    <appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>${CONSOLE_LOG_PATTERN}</pattern>\n        </encoder>\n    </appender>\n\n    <root level=\"INFO\">\n        <appender-ref ref=\"CONSOLE\"/>\n    </root>\n</configuration>\n```\n\nproperty를 이용해 변수처럼 선언할 수 있습니다. 마치 자바에서 상수화를 시키는 것과 같습니다.\n\n콘솔에는 색을 포함해 이쁘게 출력하기 위해 스프링 부트에서 제공하는 Logback 컨버터 ColorConverter 태그를 선언합니다.\n\n\n\n작성한 로그 패턴 \"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %clr(%5level) %cyan(%logger) - %msg%n\"을 살펴보면\n\n\n\n- %d{yyyy-MM-dd HH:mm:ss.SSS}: 로그 이벤트가 발생한 날짜와 시간을 yyyy-MM-dd HH:mm:ss.SSS 형식으로 표시.\n\n\n\n- [%thread]: 현재 로그 이벤트를 발생시킨 스레드의 이름을 표시.\n\n\n\n- %clr(%5level): clr 변환 키워드를 사용하여 로그 레벨 (INFO, ERROR 등)을 적절한 색상으로 출력.\n\n\n\n- %cyan(%logger): 로거 이름 (%logger)을 시안색 (cyan)으로 출력.\n\n\n\n- %msg: 로그 메시지의 내용을 표시.\n\n\n\n- %n: 줄 바꿈 문자.\n\n\n  \nappender의 이름은 CONSOLE로 지정하고 pattern은 앞서 property에서 정의한 CONSOLE_LOG_PATTERN을 사용합니다.\n\n\n\n마지막으로 로거 레벨을 INFO로 설정하고 사용할 appender를 appender-ref 태그를 이용해 명시합니다.\n\n![](.index_images/img_6.png)\n위와 같이 출력됨을 확인할 수 있습니다.\n\n  \n   \n### 3. **환경 별로 로그 남기는 방법 나누기**\n   \n현재 저희 팀은 운영 서버, 개발 서버를 나누어 가용 중에 있고 팀원들이 로컬로 개발하는 환경까지 총 3개의 환경에서 서버를 운영하고 있습니다.\n\n\n\n로컬 서버의 경우 DB로 날아가는 쿼리를 모두 확인할 필요가 있고, 필요한 경우 DEBUG 수준의 로그도 확인하는 상황이 생깁니다.\n\n개발 서버의 경우 실제 사용자를 기반으로 한 로그가 아닌, 개발 단계에서 필요한 쿼리 확인, 개발자가 의도한 예외가 발생하는지 등의 정보가 필요합니다.\n\n운영 서버의 경우 쿼리나 의도한 예외를 확인하는 로그보다는, 예상하지 못한 에러(NPE, ISE)등이 발생했을 때 에러에 대한 로그가 필요합니다.\n\n\n\n또한 개발 서버, 운영 서버는 콘솔 output이 아닌 별도의 파일에 로그를 저장해야 합니다.\n\n\n\n그러므로 각 환경마다 로그 전략을 다르게 가져가야 합니다.\n\n```html\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\"/>\n\n    <property name=\"CONSOLE_LOG_PATTERN\"\n              value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %clr(%5level) %cyan(%logger) - %msg%n\"/>\n    <property name=\"FILE_LOG_PATTERN\" value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %5level %logger - %msg%n\"/>\n    \n    <appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>${CONSOLE_LOG_PATTERN}</pattern>\n        </encoder>\n    </appender>\n    \n    <appender name=\"FILE-INFO\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <encoder>\n            <pattern>${FILE_LOG_PATTERN}</pattern>\n        </encoder>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\">\n            <fileNamePattern>./log/pium-dev-%d{yyyy-MM-dd}.%i.log</fileNamePattern>\n            <maxFileSize>50MB</maxFileSize>\n            <maxHistory>30</maxHistory>\n            <totalSizeCap>1GB</totalSizeCap>\n        </rollingPolicy>\n    </appender>\n\n    <appender name=\"FILE-ERROR\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>ERROR</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n        <encoder>\n            <pattern>${FILE_LOG_PATTERN}</pattern>\n        </encoder>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\">\n            <fileNamePattern>./log/pium-prod-%d{yyyy-MM-dd}.%i.log</fileNamePattern>\n            <maxFileSize>50MB</maxFileSize>\n            <maxHistory>30</maxHistory>\n            <totalSizeCap>3GB</totalSizeCap>\n        </rollingPolicy>\n    </appender>\n    \n    <root level=\"INFO\">\n        <appender-ref ref=\"CONSOLE\"/>\n    </root>\n</configuration>\n```\n\n우선 파일로 출력하기 위한 property와 appender를 추가합니다. \nProperty는 콘솔에서 지정한 방법과 동일하지만 파일에 저장하는 경우에는 색이 들어갈 필요가 없고, 색 코드가 들어가면 글자가 깨지는 현상이 발생합니다. \n그러므로 색을 제거한 패턴을 작성했습니다.\n\nAppender의 경우 File로 로그를 남기기 위해 RollingFileAppender를 사용했습니다. \n`INFO`와 `ERROR`로 appender를 구분해 레벨 수준에 따라 별도의 appender로 분리했습니다. \nRollingFileAppender는 **maxFileSize**와 **maxHistory**, **totalSizeCap** 설정을 통해 한 로그 파일의 최대 크기, 며칠 동안 보관할지, 모든 로그 파일(./logs/..)의 크기를 설정할 수 있습니다.\n\n![](.index_images/img_7.png)\n\n환경 별로 분리하기에 앞서, **각 appender 들은 분리 하여 설정 파일로 저장해 두면 재사용을 할 수 있습니다.**\n\n위에서 작성한 console, file-info, file-error appender 들을 각각의 xml 파일로 분리합니다.\n\n자바에서 클래스로 분리하는 것과 동일한 맥락입니다.\n\n**console-appender.xml**\n```html\n<included>\n    <appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>${CONSOLE_LOG_PATTERN}</pattern>\n        </encoder>\n    </appender>\n</included>\n```\n\n**file-info-appender.xml**\n```html\n<included>\n    <appender name=\"FILE-INFO\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <encoder>\n            <pattern>${FILE_LOG_PATTERN}</pattern>\n        </encoder>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\">\n            <fileNamePattern>./log/pium-dev-%d{yyyy-MM-dd}.%i.log</fileNamePattern>\n            <maxFileSize>50MB</maxFileSize>\n            <maxHistory>30</maxHistory>\n            <totalSizeCap>1GB</totalSizeCap>\n        </rollingPolicy>\n    </appender>\n</included>\n```\n\n**file-error-appender.xml**\n```html\n<included>\n    <appender name=\"FILE-ERROR\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n            <level>ERROR</level>\n            <onMatch>ACCEPT</onMatch>\n            <onMismatch>DENY</onMismatch>\n        </filter>\n        <encoder>\n            <pattern>${FILE_LOG_PATTERN}</pattern>\n        </encoder>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\">\n            <fileNamePattern>./log/pium-prod-%d{yyyy-MM-dd}.%i.log</fileNamePattern>\n            <maxFileSize>50MB</maxFileSize>\n            <maxHistory>30</maxHistory>\n            <totalSizeCap>3GB</totalSizeCap>\n        </rollingPolicy>\n    </appender>\n</included>\n```\n\n이제 환경별로 로그 전략을 분리하겠습니다.\n\n```html\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<configuration>\n    <include resource=\"org/springframework/boot/logging/logback/defaults.xml\"/>\n    <conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\"/>\n\n    <property name=\"CONSOLE_LOG_PATTERN\"\n              value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %clr(%5level) %cyan(%logger) - %msg%n\"/>\n    <property name=\"FILE_LOG_PATTERN\" value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %5level %logger - %msg%n\"/>\n\n    <!--local-->\n    <springProfile name=\"local\">\n        <include resource=\"console-appender.xml\"/>\n\n        <root level=\"INFO\">\n            <appender-ref ref=\"CONSOLE\"/>\n        </root>\n    </springProfile>\n\n    <!--dev-->\n    <springProfile name=\"dev\">\n        <include resource=\"file-info-appender.xml\"/>\n\n        <root level=\"INFO\">\n            <appender-ref ref=\"FILE-INFO\"/>\n        </root>\n\n        <logger level=\"DEBUG\" name=\"org.hibernate.SQL\">\n            <appender-ref ref=\"FILE-INFO\"/>\n        </logger>\n\n        <logger level=\"TRACE\" name=\"org.hibernate.type.descriptor.sql.BasicBinder\">\n            <appender-ref ref=\"FILE-INFO\"/>\n        </logger>\n    </springProfile>\n\n    <!--prod-->\n    <springProfile name=\"prod\">\n        <include resource=\"file-error-appender.xml\"/>\n\n        <root level=\"ERROR\">\n            <appender-ref ref=\"FILE-ERROR\"/>\n        </root>\n    </springProfile>\n\n</configuration>\n```\n\n`springProfile` 태그를 이용해 local, dev, prod로 분리할 수 있습니다.\n\ninclude 태그를 이용해 어떤 appender를 사용할지 추가합니다. 자바에서 import와 동일하다고 생각하면 됩니다.\n\n이후 각 profile 별로 로그 레벨을 설정하고 필요한 appender를 appender-ref 태그에 추가하면 됩니다.\n\n\n\n개발 서버의 경우 쿼리와 쿼리에 바인딩되는 파라미터를 확인하기 위해 DEBUG, TRACE 레벨의 appender를 추가해 줬습니다.\n\n\n\n로컬은 local로 profile을 구분해도 되고, 아무런 profile을 선언하지 않으면 default로 해당 설정을 읽기 때문에 구분하지 않아도 됩니다.\n\n만약 local로 profile을 설정한 경우에는 인텔리제이에서 아래와 같이 설정해야 콘솔 로그를 확인할 수 있습니다.\n\n![](.index_images/img8.png)\n\ndev, prod 환경에서의 로깅을 인텔리제이로 확인하고 싶은 경우에는 동일하게 profiles에 입력하고 RUN 하면 됩니다.\n\n### Reference\n\n- https://www.baeldung.com/logback\n- https://www.youtube.com/watch?v=1MD5xbwznlI\n- https://www.youtube.com/watch?v=JqZzy7RyudI \n\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '하마드'가 작성했습니다. 개요 우아한테크코스 5기 피움 프로젝트를 진행하면서, 필터링 기능 구현을 위해 Querydsl을 적용한 과정을 정리한다 이전글 과정 본 프로젝트는 다음과 같은 환경에서 구성되었다 Springboot 3.1.1 JAVA 17 Querydsl 설정 build.gradle에 관련 코드 추가   파일에 위 …","fields":{"slug":"/querydsl_apply_procedure/"},"frontmatter":{"date":"August 16, 2023","title":"피움 서비스의 Querydsl 적용 과정","tags":["Querydsl"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[하마드](https://github.com/rawfishthelgh)'가 작성했습니다.\n\n## 개요\n---\n우아한테크코스 5기 피움 프로젝트를 진행하면서, 필터링 기능 구현을 위해 Querydsl을 적용한 과정을 정리한다\n\n[이전글](https://pium-official.github.io/why_we_applied_querydsl/)\n\n## 과정\n---\n> 본 프로젝트는 다음과 같은 환경에서 구성되었다\n> \n> Springboot 3.1.1\n> \n> JAVA 17\n\n## Querydsl 설정\n---\n### build.gradle에 관련 코드 추가\n![](.index_images/1.png)\n\n\n```\ndependencies {\n    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'\n    implementation 'org.springframework.boot:spring-boot-starter-validation'\n    implementation 'org.springframework.boot:spring-boot-starter-web'\n\n    implementation 'com.github.maricn:logback-slack-appender:1.6.1'\n\n    //querydsl 추가\n    implementation 'com.querydsl:querydsl-jpa:5.0.0:jakarta'\n    annotationProcessor \"com.querydsl:querydsl-apt:5.0.0:jakarta\"\n    annotationProcessor \"jakarta.annotation:jakarta.annotation-api\"\n    annotationProcessor \"jakarta.persistence:jakarta.persistence-api\"\n}\n\n//querydsl 추가\ndef querydslSrcDir = 'src/main/generated'\n\nclean {\n    delete file(querydslSrcDir)\n}\n\ntasks.withType(JavaCompile) {\n    options.generatedSourceOutputDirectory = file(querydslSrcDir)\n}\n```\n`build.gradle` 파일에 위 설정을 추가해 준다.\n`dependencies` 하부의 옵션 설정은 아래와 같은 의미를 가진다.\n\n`def querydslSrcDir = 'src/main/generated'`\n\nQuerydsl이 생성한 소스 코드를 저장할 디렉토리를 정의한다. 여기서는 'src/main/generated'로 디렉토리 경로를 지정하고, 이를 querydslSrcDir 변수에 할당한다.\n\n`clean { delete file(querydslSrcDir) }`\n\nGradle의 clean 태스크를 정의하고, 해당 태스크가 실행될 때 querydslSrcDir 디렉토리를 삭제하도록 한다. 즉, 프로젝트를 clean 하면 Querydsl이 생성한 소스 코드도 삭제된다.\n\n`tasks.withType(JavaCompile) { options.generatedSourceOutputDirectory = file(querydslSrcDir) }`\n\nJavaCompile 태스크 유형을 가진 모든 태스크에 대해, 컴파일 옵션 중에서 생성된 소스 코드의 출력 디렉토리를 querydslSrcDir로 설정한다. Querydsl이 생성한 소스 코드가 지정한 디렉토리에 컴파일되도록 보장한다.\n### build 수행 및 Q Class 생성\n![](.index_images/2.png)\n![](.index_images/3.png)\n\n이제 바뀐 설정 파일로 `build`를 수행하면 우리가 지정한`src/main/generated` 경로에 위와 같은 Q Class가 생성된다.\n\nQueryDsl은 빌드 시점에 애플리케이션의 도메인 중 `@Entity`와 `@Embeddable` 어노테이션이 포함된 클래스를 찾아 Q Class를 생성한다. 이 Q class를 가지고 애플리케이션 내부에서 QueryDsl 관련 로직을 적용해야 한다.\n\n## 적용\n---\n### JPAQueryFactory Bean 등록\n![](.index_images/4.png)\n```java\n@Configuration\npublic class QueryDslConfig {\n\n    @PersistenceContext\n    private EntityManager entityManager;\n\n    @Bean\n    public JPAQueryFactory jpaQueryFactory() {\n        return new JPAQueryFactory(entityManager);\n    }\n}\n```\nJPAQueryFactory는 Querydsl을 사용하여 JPA 쿼리를 생성하고 실행하기 위한 클래스다. `EntityManager`를 주입해 JPAQueryFactory가 db 상호 작용을 수행할 수 있도록 해주는 Config 파일을 작성한다.\n\n### Querydsl 질의용 Repository 생성 및 적용\n```java\npublic interface HistoryCustomRepository {\n\n    Page<History> findAllByPetPlantIdAndHistoryTypes(Long petPlantId, List<HistoryType> historyTypes, Pageable pageable);\n}\n```\n먼저 해당 기능을 추상화한 Repository 인터페이스를 정의한다.\n```java\nimport static com.official.pium.domain.QHistory.history;\n\n@Repository\n@RequiredArgsConstructor\npublic class HistoryCustomRepositoryImpl implements HistoryCustomRepository {\n\n    private final JPAQueryFactory jpaQueryFactory;\n\n    @Override\n    public Page<History> findAllByPetPlantIdAndHistoryTypes(Long petPlantId, List<HistoryType> historyTypes, Pageable pageable) {\n\n        List<History> histories = jpaQueryFactory.selectFrom(history)\n                .where(history.petPlant.id.eq(petPlantId), inHistoryType(historyTypes))\n                .orderBy(new OrderSpecifier<>(Order.DESC, history.createdAt), new OrderSpecifier<>(Order.DESC, history.date))\n                .offset(pageable.getOffset())\n                .limit(pageable.getPageSize())\n                .fetch();\n\n        return new PageImpl<History>(histories, pageable, getCount(petPlantId, historyTypes));\n    }\n\n    private Long getCount(Long petPlantId, List<HistoryType> historyTypes) {\n        return jpaQueryFactory.select(history.count())\n                .from(history)\n                .where(history.petPlant.id.eq(petPlantId), inHistoryType(historyTypes))\n                .fetchOne();\n    }\n\n    private BooleanExpression inHistoryType(List<HistoryType> historyTypes) {\n        if (historyTypes == null || historyTypes.isEmpty()) {\n            return null;\n        }\n\n        return history.historyCategory.historyType.in(historyTypes);\n    }\n}\n```\n가장 중요한 부분이다. HistoryCustomRepository를 implements한 HistoryCustomRepositoryImpl를 작성한다.\n\n여기서 **인터페이스명+impl 네이밍 규약을 반드시 지켜야** 향후 HistoryRepository에 extends 할 시 해당 구현체가 주입될 수 있다.\n\n또 여기서 사용하려는 Q class(QHistory)를 위와 같이 static import하여 사용한다.\n\n`import static com.official.pium.domain.QHistory.history;`\n\n먼저, 우리가 수행하려는 동작과 도출하고자 하는 쿼리 형태는 아래와 같다.\n\n> - 기능 : 리스트로 받은 히스토리 타입들에 해당하는 히스토리를 시간순으로 조회하고, 페이징 정보(size, page)에 해당하는 만큼 데이터를 가져온다.\n\n```\nselect *\n    from\n        history h1_0 \n    join\n        history_category h2_0 \n            on h2_0.id=h1_0.history_category_id \n    where\n        h1_0.pet_plant_id=(반려식물_id) \n        and h2_0.history_type in (타입1,타입2...) \n    order by\n        h1_0.created_at desc,\n        h1_0.event_date desc \n        offset (페이지번호 * 페이지사이즈) rows fetch first (페이지사이즈) rows only\n```\n\n![](.index_images/5.png)\n\n이제 위 코드를 천천히 파악해보자.\n\n>**`jpaQueryFactory.selectFrom(history)`**\n\nhistory를 대상으로 select 질의를 수행함을 의미한다\n\n>**`.where(history.petPlant.id.eq(petPlantId), inHistoryType(historyTypes))`**\n\nwhere 조건문을 사용하는 코드이다. `eq` 메소드를 통해 파라미터로 주입받은 `petPlantId`와의 비교를 수행한다.\n\n![](.index_images/6.png)\n\n위는 `in` 절을 수행하는 `inHistoryType` 메소드다. 여기서 반환 타입을 `BooleanExpression`으로 사용한 것에 주목하자. **`BooleanExpression` 의 반환값이 null일 경우, 자동으로 조건절에서 제외된다.** 따라서 우리는 `historyTypes`가 들어오지 않았을 경우, in 절을 수행하지 않도록 지정했다.\n\n> **`.orderBy(new OrderSpecifier<>(Order.DESC, history.createdAt), new OrderSpecifier<>(Order.DESC, history.date))`**\n\n`order by`의 Sorting 조건을 지정하기 위한 부분이다.\n**Sorting 조건은 `OrderSpecifier` 클래스로 정의해 지정해야 한다.**\n\n첫 인자에 Sorting 방향 Enum(Order.DESC)을, 두 번째 인자에 Sorting 대상 필드(history.createdAt)를 지정한다. 우리는 Sorting 조건이 두 가지이므로, 두 개의 `OrderSpecifier` 를 정의해 정렬 순서대로 넣어주었다.\n\n> **`.offset(pageable.getOffset().limit(pageable.getPageSize())`**\n\n페이징 처리를 위한 offset, limit을 지정하는 부분이다. 이 부분은 그냥 Long 타입을 넣어주면 된다. 넘길 부분을 지정하는 offset 값과, 가져올 양을 지정하는 limit 값을 넣어준다.(해당 애플리케이션에서 offset 값은 Pageable 객체에 미리 계산되어 있다)\n\n> **`.fetch()`**\n\n`.fetch()` 를 활용하면 결과를 리스트로 반환할 수 있다. \n\n여기서 주의할 점은 **`.fetch()`를 사용한다고 `fetch join`을 수행하는 것이 아니다**. 결과 반환 방법은 아래 블로그에 자세히 설명되어 있어 첨부하겠다.\n[참고](https://devocean.sk.com/blog/techBoardDetail.do?ID=163915)\n\n```java\nprivate Long getCount(Long petPlantId, List<HistoryType> historyTypes) {\nreturn jpaQueryFactory.select(history.count())\n.from(history)\n.where(history.petPlant.id.eq(petPlantId), inHistoryType(historyTypes))\n.fetchOne();\n}\n```\n\n조건에 해당하는 레코드의 총 개수를 전달하는 Count 쿼리를 날리는 메소드다. 간단하게 `.select()` 메소드의 Qclass에 `.count()` 만 추가하면 count 쿼리를 생성해준다. \n`.fetchOne()` 을 통해 단건으로 `Long`을 반환할 수 있다.\n  \n### 생성한 CustomRepository를 기존 사용하던 Repository에 extends\n\n![](.index_images/7.png)\n\n  이제 만든 CustomRepository를 기존 사용하던 Repository에 extends 하기만 하면 기존 Service에 새로운 인터페이스 필드를 선언하지 않고도 사용할 수 있다.\n\n**HistoryService.java**\n ```java\n@Service\n@Transactional(readOnly = true)\n@RequiredArgsConstructor\npublic class HistoryService {\n\n    private final HistoryRepository historyRepository;\n    private final PetPlantRepository petPlantRepository;\n\n    public HistoryResponse read(Long petPlantId, Pageable pageable, Member member, List<String> filters) {\n        PetPlant petPlant = petPlantRepository.findById(petPlantId)\n                .orElseThrow(() -> new NoSuchElementException(\"일치하는 반려 식물이 존재하지 않습니다. id :\" + petPlantId));\n\n        if (petPlant.isNotOwnerOf(member)) {\n            throw new IllegalArgumentException(\"요청 사용자와 반려 식물의 사용자가 일치하지 않습니다. id :\" + member.getId());\n        }\n\n        List<HistoryType> historyTypes = new ArrayList<>();\n\n        if (filters != null) {\n            historyTypes = filters.stream()\n                    .map(HistoryType::from)\n                    .toList();\n        }\n\n        Page<History> historyPageByPetPlantId = historyRepository.findAllByPetPlantIdAndHistoryTypes(petPlantId, historyTypes, pageable);\n\n        return HistoryMapper.toHistoryResponse(historyPageByPetPlantId);\n    }\n}\n```\n기존에 주입받은 `historyRepository`를 활용해, 새로 생성한 `findAllByPetPlantIdAndHistoryTypes` 메소드를 사용하는 것을 확인할 수 있다.\n\n## 결과\n---\n  ```\n    select\n        h1_0.id,\n        h1_0.created_at,\n        h1_0.event_date,\n        h1_0.history_category_id,\n        h1_0.curr,\n        h1_0.prev,\n        h1_0.pet_plant_id,\n        h1_0.updated_at \n    from\n        history h1_0 \n    join\n        history_category h2_0 \n            on h2_0.id=h1_0.history_category_id \n    where\n        h1_0.pet_plant_id=? \n        and h2_0.history_type in (?,?) \n    order by\n        h1_0.created_at desc,\n        h1_0.event_date desc offset ? rows fetch first ? rows only\n2023-08-16 17:10:47.907 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [1] as [BIGINT] - [1]\n2023-08-16 17:10:47.907 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [2] as [VARCHAR] - [FLOWERPOT]\n2023-08-16 17:10:47.907 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [3] as [VARCHAR] - [LOCATION]\n2023-08-16 17:10:47.907 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [4] as [INTEGER] - [0]\n2023-08-16 17:10:47.907 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [5] as [INTEGER] - [5]\nHibernate: \n    select\n        count(h1_0.id) \n    from\n        history h1_0 \n    join\n        history_category h2_0 \n            on h2_0.id=h1_0.history_category_id \n    where\n        h1_0.pet_plant_id=? \n        and h2_0.history_type in (?,?)\n2023-08-16 17:10:47.919 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [1] as [BIGINT] - [1]\n2023-08-16 17:10:47.919 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [2] as [VARCHAR] - [FLOWERPOT]\n2023-08-16 17:10:47.919 [main] TRACE org.hibernate.orm.jdbc.bind - binding parameter [3] as [VARCHAR] - [LOCATION]\n```\n테스트 코드를 통해 실제 어떤 쿼리가 실행되는지 확인해 보니,\n우리가 위에서 구현하고자 했던 sql 쿼리와 정확히 일치하는 것을 볼 수 있다.\n\n![](.index_images/8.png)\n\n\n클라이언트 화면에서도 필터링이 잘 수행 된다. 기분이 좋다.\n### 어떻게 join이 걸렸을까?\n위 예시를 보면 join을 통해 연관관계가 있는 테이블 정보를 한 번에 가져와 n+1 문제가 발생하지 않는다. 그런데, 우리는 select where 설정만 했을 뿐, join 처리를 따로 하지 않았다. 그런데 어떻게 연관관계를 파악하고 Querydsl이 join을 생성해 준 걸까?\n\n**Where 조건에 추가된 연관 관계 정보를 확인하고 join 쿼리 생성**\n\n아까 where 조건에서 사용했던 `inHistoryTypes` 메소드를 다시 파악해볼 필요가 있다.\n\n![](.index_images/9.png)\n\n`history.historyCategory.historyType.in(historyTypes)` 와 같은 형태로 객체의 연관관계를 탐색하여 `in`절을 실행시키는 모습을 볼 수 있다. 따라서 해당 연관관계 정보가 필요함을 querydsl이 파악하고 join 쿼리를 실행시켜주는 것이다.\n\n**Where을 생략한 경우**\n![](.index_images/10.png)\n\n만약 where을 생략했을 때는 연관 관계 정보가 필요하지 않음을 파악하므로 join 쿼리가 날아가지 않는 모습을 볼 수 있다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '참새'가 작성했습니다. 우리는 왜 Cypress를 사용하게 되었나 tanstack query의 에서 기존에는  설정을 사용하지 않았었어요. \n근데 이 함수를 리팩터링하면서  설정을 추가했습니다. 하지만 해당 함수를 사용하는 모든 곳에서 를 적용하지 않았고, 결국 개발 서버에 올라간 어플리케이션이 터지는 사고가 생겼습니다. 이…","fields":{"slug":"/github-actions-cypress/"},"frontmatter":{"date":"August 14, 2023","title":"피움 Cypress 자동화 구축기","tags":["CI","Cypress","테스트 자동화"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[참새](https://github.com/WaiNaat)'가 작성했습니다.\n \n\n## 우리는 왜 Cypress를 사용하게 되었나\n\ntanstack query의 `useQuery`에서 기존에는 `suspense` 설정을 사용하지 않았었어요. \n근데 이 함수를 리팩터링하면서 `suspense: true` 설정을 추가했습니다. 하지만 해당 함수를 사용하는 모든 곳에서 `<Suspense>`를 적용하지 않았고, 결국 개발 서버에 올라간 어플리케이션이 터지는 사고가 생겼습니다.\n\n이러한 문제를 예방하기 위해서 리팩터링 이후에도 Cypress로 기존에 돌아가던 다른 기능이 돌아감을 보장하고자 했어요.\n\n이 글은 Cypress의 기본적인 사용법은 알고 있다는 가정 하에 쓰여졌습니다.\n\n## 설치와 실행\n\nCypress 설치는 간단하게 넘어갈게요.\n\n```shell\nnpm install --save-dev cypress\n```\n\npackage.json에 Cypress 실행 스크립트도 넣어줍니다.\n\n```json\nscripts: {\n    \"cypress\": \"cypress open\"\n}\n```\n\n실행해볼까요?\n\n```shell\nnpm run cypress\n```\n\n맨 처음 실행했을 때는 설정이 되지 않은 상태입니다.\n저희는 E2E 테스트를 할 거니까 `E2E Testing`을 눌러요.\n\n![e2e 테스트 선택창](.index_images/01-e2e.png)\n\n그러면 자동으로 설정이 됩니다.\n\n![자동 config](.index_images/02-config.png)\n\n![브라우저 선택](.index_images/03-browser.png)\n\n![e2e 테스트 선택창](.index_images/04-first-test.png)\n\n만약 `cy.visit`을 편하게 하고 싶다면 `cypress.config.ts`에 아래 설정을 추가해주세요!\n\n```ts\nexport default defineConfig({\n  e2e: {\n    baseUrl: 'http://localhost:8282',\n  },\n});\n```\n\n## 타입스크립트 오류 해결하기\n\n설레는 마음으로 첫 테스트를 쓰려고 했는데\n\n![코드에 빨간 줄](.index_images/05-error.png)\n\n이런! 벌써 느낌이 좋지 않네요..\n\n![](.index_images/06-error-code.png)\n\n저한테 왜 이러는 걸까요? 살펴봅시다.\n\n![](.index_images/07-error-details-1.png)\n\n![](.index_images/08-error-details-2.png)\n\n타입스크립트 문제네요. [같은 문제를 겪으신 분들의 해결책](https://github.com/vuejs/vue-cli/issues/4239#issuecomment-1214261721)으로 금방 처리할 수 있었습니다. `tsconfig.json`에 Cypress를 이해할 수 있다는 용기를 불어넣어 주세요.\n\n```json\n\"compilerOptions\": {\n\t\t\"types\": [\"cypress\"]\n},\n\"include\": [\"cypress/**/*\"]\n```\n\n![](.index_images/09-good.png)\n\n좋아요 좋아요\n\n## 테스트 작성\n\n이제 테스트를 열심히 만들어 줍니다.\n\n```ts\ndescribe('피움 메인 페이지', () => {\n  beforeEach(() => {\n    cy.visit('http://localhost:8282');\n  });\n\n  it('피움 소개 문구가 보인다.', () => {\n    cy.contains('식물을 쉽게')\n      .get('img[alt=\"logo\"]')\n      .contains('피움에 등록된 식물을 검색해 보세요');\n  });\n\n  it('검색창을 이용해 식물을 검색할 수 있다.', () => {\n    cy.get('input')\n      .type('아카시')\n      .wait(300)        // 검색창에 디바운스가 되어 있어서 기다림\n      .contains('아카시')\n      .contains('아카시아');\n  });\n});\n```\n\n## Github Actions 적용\n\n이제 Github Actions를 이용해 PR이 올라올 때마다 작성한 테스트를 돌려서 성공/실패 여부를 확인해봅시다.\n\n사실 기본적인 방법은 [Cypress 공식 홈페이지](https://docs.cypress.io/guides/continuous-integration/github-actions)에 잘 나와 있어요!\n`cypress-io/github-action@v5`를 사용하면 자동적으로 Cypress를 열어서 안에 있는 모든 spec들을 한 번씩 돌려봅니다. 결과 요약도 나중에 Actions 로그를 확인해보면 알 수 있어요.\n\n```yml\nname: Frontend E2E Test\n\non:\n  # develop과 main branch에 push되었을 경우 테스트 진행\n  # PR 머지 후에 혹시 깨지는지 확인 가능\n  push:\n    branches:\n      - develop\n      - main\n    # 피움 레포지토리 구조 상 프론트엔드 코드는 `{root}/frontend`에 존재\n    paths:\n      - frontend/**\n  # PR이 열렸을 경우 확인\n  # branch 설정을 통해 테스트가 통과하지 않으면 머지 불가 설정도 가능\n  pull_request:\n    branches:\n      - develop\n      - main\n    paths:\n      - frontend/**\n\ndefaults:\n  run:\n    working-directory: frontend\n\npermissions:\n  checks: write\n\njobs:\n  cypress:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Set up repository\n        uses: actions/checkout@v3\n\n      - name : Set up Node 18.16.0 \n        uses: actions/setup-node@v3\n        with:\n          node-version: 18.16.0\n\n      # package-lock.json이 바뀌지 않았을 경우 캐시한 node_module 사용\n      # npm install 시간 절약\n      - name: Cache node_modules\n        id: cache\n        uses: actions/cache@v3\n        with:\n          path: '**/node_modules'\n          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n          restore-keys: | \n            ${{ runner.os }}-node-\n          \n      - name: Install dependencies\n        run: npm ci\n        if: steps.cache.outputs.cache-hit != 'true'\n\n      # 테스트 자동 진행\n      - name: Run Cypress\n        uses: cypress-io/github-action@v5\n        with:\n          start: npm run start\n```\n\n이 파일을 `{repo_root}/.github/workflows/` 안에 넣어주시면 됩니다.\n\n## 경로 설정 문제!\n\n사실 이렇게 세팅하니까 Github Action을 실행할 때 아래와 같은 오류가 생기더라구요. 피움 레포지토리 구조 상 프론트엔드 코드는 `{repo_root}/frontend`에 있는데 자꾸 루트 폴더에서 `package-lock.json`을 찾아서 나오는 문제였습니다.\n\n![](.index_images/11-error.png)\n\n아래 코드처럼 cypress를 돌릴 때 시작점을 정해 주니까 해결됐어요.\n\n```yml\n      - name: Run Cypress\n        uses: cypress-io/github-action@v5\n        with:\n\t  working-directory: frontend\n          start: npm run start\n```\n\n## 완료\n\n![](.index_images/12.png)\n\n지금 이 순간에도 피움 팀의 레포지토리에서는 Cypress가 돌아가고 있다는 괴담이 들려옵니다..\n\n\n## Reference\n\n- [Cypress typings not recognized. Cannot find name 'cy'](https://github.com/vuejs/vue-cli/issues/4239#issuecomment-1214261721)\n- [Cypress 공식 홈페이지: GitHub Actions](https://docs.cypress.io/guides/continuous-integration/github-actions)"},{"excerpt":"이 글은 우테코 피움팀 크루 '하마드'가 작성했습니다. 개요 우아한테크코스 5기 피움 프로젝트를 진행하면서, 필터링 기능을 구현하기 위해 Querydsl 라이브러리를 도입한 과정을 정리한다. 문제상황 필터링 기능 도입 내가 담당한 파트는 아래와 같다. \"사용자가 보유한 반려 식물의 관리 이력을 최신 순으로 조회하기\"   은  라는 테이블 및 엔티티 객체에…","fields":{"slug":"/why_we_applied_querydsl/"},"frontmatter":{"date":"August 14, 2023","title":"피움 서비스의 Querydsl 도입 이유","tags":["Querydsl"]},"rawMarkdownBody":"\n![](.index_images/querydsl.png)\n\n> 이 글은 우테코 피움팀 크루 '[하마드](https://github.com/rawfishthelgh)'가 작성했습니다.\n\n## 개요\n---\n우아한테크코스 5기 피움 프로젝트를 진행하면서, 필터링 기능을 구현하기 위해 Querydsl 라이브러리를 도입한 과정을 정리한다.\n## 문제상황\n---\n### 필터링 기능 도입\n내가 담당한 파트는 아래와 같다.\n>\"사용자가 보유한 반려 식물의 관리 이력을 최신 순으로 조회하기\" \n\n`반려 식물의 관리 이력` 은 `History` 라는 테이블 및 엔티티 객체에 저장되고 관리된다.\n\n`History` 객체 및 테이블 형태는 아래와 같이 생겨먹었다.\n\n**History 객체**\n```java\n@Entity\n@Getter\n@Table(name = \"history\")\n@NoArgsConstructor(access = AccessLevel.PROTECTED)\npublic class History extends BaseEntity {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    @NotNull\n    @ManyToOne(fetch = FetchType.LAZY)\n    @JoinColumn(name = \"pet_plant_id\", nullable = false)\n    private PetPlant petPlant;\n\n    @NotNull\n    @Column(name = \"event_date\", nullable = false)\n    private LocalDate date;\n\n    @NotNull\n    @ManyToOne(fetch = FetchType.LAZY)\n    @JoinColumn(name = \"history_category_id\", nullable = false)\n    private HistoryCategory historyCategory;\n\n    @Valid\n    @NotNull\n    @Embedded\n    private HistoryContent historyContent;\n\t...\n```\n**history 테이블**\n![](https://velog.velcdn.com/images/rg970604/post/c71da5e7-4a11-4ced-84d9-4d81764abd29/image.png)\n\n기존에는 기록의 카테고리에 상관없이 전체를 불러와 조회를 하는 형태였지만, 피움 서비스는 4차 스프린트 과정에서 \n\n>원하는 카테고리(historyCategory) 조합에 따라서 해당 카테고리들에 해당하는 관리 기록(History)들만 조회해 온다\n\n라는 기능을 추가하게 되었다.\n\n`/history?petPlantId=1&page=0&size=20&filter=”lastwaterDate,flowerpot…”`\n위 url의 쿼리 파라미터 중 filter에 해당하는 값들을 기준으로 필터링을 수행하는 것이다.\n\n### 어떤 필터링 조건이냐에 따라 쿼리 형태가 어마무시하게 변화한다.\n\n`History` 의 카테고리 목록은\n\n`location, flowerpot, waterCycle, light, wind, lastWaterDate`\n\n총 여섯 가지를 보유하고 있다.\n\n이 카테고리를 활용한 필터링이 전혀 들어오지 않을 수도(0개), 모두 들어올수도(6개), 몇 개만 들어올 수도 있다(1~5개).\n\n또한 개수가 같아도, 어떤 카테고리가 조합되느냐에 따라 쿼리 형태가 변화한다.\n\n예를 들면 \n```sql\n// 필터링 조건이 location, flowerpot인 경우\nwhere history.pet_plant_id = 반려식물ID \n        and history_category.history_type in (location,flowerpot)\n        \n// 필터링 조건이 flowerpot, waterCycle인 경우\nwhere history.pet_plant_id = 반려식물ID\n        and history_category.history_type in (flowerpot,waterCycle)\n        \n// 어쩌구저쩌구 수많은 경우\n...\n```\n따라서, 계산해 봤을 때 \n>6C0 + 6C1 + 6C2 + 6C3 + 6C4 + 6C5  + 6C6 \n= 1 + 6 + 15 + 20 + 15 + 6 + 1 \n= 64\n\n총 64개의 경우의 수가 등장하게 된다 ;;\n\n따라서 현재 이 문제를 쌩으로 해결하려면, 조합에 따라 총64개의 분기문을 작성해야 한다...\n\n그런데 64개의 분기문을 작성하면 만들고 끝이 아니라, 필터링 조건이 하나만 늘어나도 엄청난 양의 분기문의 추가와 수정이 일어난다. 이럴 바엔 그냥 개발자를 그만 두는게 나을 수도 있다.\n\n따라서! 피움은 동적 쿼리를 쉽게 해결할 수 있는 도구를 선택 및 도입하기로 했다.\n\n## 대안분석 및 선택과정\n---\n\njava / spring에서 동적 쿼리를 해결하는 도구는 여럿 존재한다. 우리가 어떤 대안을 떠올렸는지와, 이를 도입하지 않은/도입한 이유를 간단하게 정리하면 아래와 같다.\n\n### **Mybatis**\n\nJdbcTemplate의 동적 쿼리 한계를 효율적으로 해결하기 위해 등장한 SQL Mapper 프레임워크이다. 하지만 우리는 SQL Mapper가 아닌 JPA라는 ORM을 사용하기에 navite한 sql 쿼리를 직접 작성하지 않으므로 도입하지 않았다.\n\n### **JPA - Criteria Query**\n\nJPA에서는 JPQL이라는 쿼리 기술을 사용하여 엔티티 객체를 대상으로 질의할 수 있는 기능을 제공한다. 그러나 JPQL도 sql mapper 처럼 String 형태로 작성하므로 컴파일 단계에서 오류를 잡아내기 힘든데, Criteria Query 클래스를 사용하면 자바 코드로 JPQL을 작성할 수 있어 타입 안전성을 제공한다. 또한 조건문을 활용하여 동적 쿼리를 처리할 수 있는 기능을 제공한다. 그럼 이 방법론을 써야겠다는 생각이 슬슬 들게 된다.\n\n그러나...Criteria Query에 대한 레퍼런스를 찾아보니, 대부분의 대답은 이러했다.\n> 쓰지 마세요 가독성 안좋습니다...\n\n그래서 이게 어떤 형태를 갖는지 대충 찾아봤다. gpt 선생님에게 \n`where history.pet_plant_id = 반려식물ID \n        and history_category.history_type in (타입 조건들...)`\n코드 작성을 요청드리니 다음과 같은 코드 형태가 태어났다. 아래 코드는 그냥 Criteria Query가 이런 형태를 갖는구나 정도로 생각하자.\n\n```java\npublic class DynamicCriteriaQueryExample {\n\n    public static void main(String[] args) {\n        EntityManager entityManager = ...; // EntityManager 생성 코드\n\n        CriteriaBuilder criteriaBuilder = entityManager.getCriteriaBuilder();\n\n        // Criteria Query 생성\n        CriteriaQuery<History> criteriaQuery = criteriaBuilder.createQuery(History.class);\n        Root<History> historyRoot = criteriaQuery.from(History.class);\n\n        // 동적으로 생성되는 조건을 위한 리스트 생성\n        List<Predicate> predicates = new ArrayList<>();\n\n        // 동적 조건 추가\n        int plantId = 반려식물ID; // 동적으로 설정되는 값\n        predicates.add(criteriaBuilder.equal(historyRoot.get(\"petPlantId\"), plantId));\n\n        List<String> types =히스토리 타입들; // 동적으로 설정되는 값\n        predicates.add(historyRoot.get(\"historyType\").in(types));\n\n        // 모든 동적 조건을 AND로 결합\n        criteriaQuery.where(predicates.toArray(new Predicate[0]));\n\n        // 결과 쿼리 실행\n        List<History> resultList = entityManager.createQuery(criteriaQuery).getResultList();\n\n        // 결과 처리 등의 로직\n    }\n}\n\n```\n딱 봐도 뭔가 코드가 더럽고, 어떤 sql이 탄생하는지 유추가 정말 힘들다. 위 코드를 보고 주석의 상세한 설명 없이 동적 where in 절이 탄생하는게 유추되는가? 코드 형태가 sql이 아닌 객체 중심적이다보니 발생한 문제이다.\n\n### **Querydsl**\n자바 코드로 jpql을 생성할 수 있는 Criteria Query의 이점을 유지하면서, sql과 비슷한 형태로 코드를 작성할 수 있는 장점까지 챙겨간 오픈소스 라이브러리다.\n\n따라서 피움은 \n>1. 가독성 : sql과 비슷한 형태의 자바 코드 작성\n>2. 타입 안전성 : 자바 코드로 작성해 컴파일 단계에서 오류 파악 가능\n>3. 생산성 : join, 서브쿼리, 집계함수 등 해당 서비스에서 필요한 복잡한 쿼리 형태를 간단하게 작성 가능한 api 제공\n>4. 유지보수성 : 현재 사용하고 있는 라이브러리, 프레임워크를 변경하지 않고 개발 가능\n\n동적 쿼리를 구현하는 과정에서, 다른 대안에 비해 위 네 가지의 가치를 창출할 수 있다는 결론을 내리고 Querydsl을 도입했다.\n\n다음 포스팅에서는 Querydsl 라이브러리를 우리 서비스에 어떤 형태로 적용했는지 구현 과정을 설명하도록 하겠다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 사건의 발단 물주기, 반려 식물 정보 등록 및 수정과 같은 행동에 대한 History를 기록해야한다. 이 과정에서 History를 기록하는 코드가 ReminderService, PetPlantService에 침투하는 상황이 발생했다. History라는 도메인이 Reminder, PetPlant 그리고 …","fields":{"slug":"/spring-event-apply/"},"frontmatter":{"date":"August 13, 2023","title":"Spring Event 적용하기","tags":["이벤트","Spring"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 사건의 발단\n\n물주기, 반려 식물 정보 등록 및 수정과 같은 행동에 대한 History를 기록해야한다.\n\n이 과정에서 History를 기록하는 코드가 ReminderService, PetPlantService에 침투하는 상황이 발생했다.\n\nHistory라는 도메인이 Reminder, PetPlant 그리고 앞으로 생겨날 히스토리가 필요한 로직들에 대해 강한 의존성을 가지게 될 것을 우려했다.\n\n실제로 다음과 같은 코드가 작성되면서 그 심각성은 더욱 가깝게 다가왔다.\n\n```java\n@Transactional\npublic void water(ReminderCreateRequest reminderCreateRequest, Long petPlantId, Member member) {\n    PetPlant petPlant = petPlantRepository.findById(petPlantId)\n            .orElseThrow(() -> new NoSuchElementException(\"일치하는 반려 식물이 존재하지 않습니다. id: \" + petPlantId));\n    checkOwner(petPlant, member);\n    LocalDate prevDate = petPlant.getLastWaterDate();\n    petPlant.water(reminderCreateRequest.getWaterDate());\n\n    HistoryCategory historyCategory = historyCategoryRepository.findByHistoryType(HistoryType.LAST_WATER_DATE)\n            .orElseThrow(\n                    () -> new NoSuchElementException(\"존재하지 않는 히스토리 타입입니다. type: \" + HistoryType.LAST_WATER_DATE));\n\n    History history = History.builder()\n            .petPlant(petPlant)\n            .date(reminderCreateRequest.getWaterDate())\n            .historyCategory(historyCategory)\n            .historyContent(HistoryContent.builder()\n                    .previous(prevDate.toString())\n                    .current(petPlant.getLastWaterDate().toString())\n                    .build())\n            .build();\n\n    historyRepository.save(history);\n}\n```\n\n이렇게 개발을 진행하다가는 모든 코드에 HistoryRepository, HistoryCategoryRepository들이 침투하게된다.\n\n아무리 개발이 급해도 이건 아니다 싶어서 `History를 기록한다` 라는 행위에 대한 관심사를 분리하는 방법을 찾아봤다.\n\n## 관심사 분리\n\n관심사를 분리하는 방법으로 다음 두가지 방법을 찾았다.\n\n`1. AOP를 사용한다.`\n\nAOP의 `@Around`어노테이션을 이용하여 History 기록이 필요한 부분에 대해 기록하는 동작을 수행할 수 있다.\n\n해당 방식은 다음과 같은 특징을 가진다.\n\n- 포인트컷 표현식으로 대상 메소드 혹은 클래스를 지정할 수 있다.\n- 메소드의 파라미터값, 반환값, 이름 등의 정보를 구할 수 있다.\n- 내부에서 선언된 변수에 대해서는 접근할 수 없다.\n- 선언된 메소드와 동일한 트랜잭션 주기를 가진다.\n\n`2. Spring Event를 사용한다.`\n\nSpring에서 제공하는 Event 기능을 사용할 수도 있다.\n\nHistory와 관련된 Repository나 Service에 의존하지 않고 EventPublisher를 의존하게 하여 패키지간 의존성을 해결할 수 있다.\n\n- 서비스간의 강한 의존성을 줄일 수 있다.\n- 이벤트로 분리된 부분을 비동기 방식으로 처리할 수 있다.\n\nAOP와 Event 두가지 방식을 알아봤다.\n\nAOP를 사용하면 프로덕션 코드 내에서 History에 대해 아예 몰라도 된다는 이점이 있었지만 History 생성 간 내부 값이 필요한 경우 이에 대처하기 어렵다는 문제가 있었다.\n\n이에 내부 값을 History에 맞는 양식으로 변환한 뒤 EventListener에게 위임하는 형태를 사용하기로 했다.\n\n## Event 흐름\n\nEvent의 흐름 자체는 간단하다.\n\n![](.index_images/5ba5ff42.png)\n\n기존의 서비스 로직이 History라는 도메인에 대한 의존성을 가지게 되는 구조였다면\n\n![](.index_images/aebfae33.png)\n\n다음과 같이 History 도메인에 의존하지 않는 구조로 개선될 수 있다.\n\n## Event 구현하기\n\nSpring Event를 구현하기 위해서는 크게 3가지 요소가 필요하다.\n\n### Event Class\n\n> 해당 실습은 SpringBoot 3.1(Spring Framework 6) 환경에서 진행됩니다. \n> \n> (Spring Framework 4.2 이전 버전은 설정이 상이하니 이점 유의해주시기 바랍니다.)\n\n우선 이벤트 클래스다.\n\n이벤트 클래스는 이벤트를 처리하기 위해 필요한 데이터를 가지고있다.\n\n```java\n@Getter\n@Validated\n@RequiredArgsConstructor\npublic class HistoryEvent {\n\n    @NotNull\n    private final Long petPlantId;\n\n    @NotNull\n    private final String previous;\n\n    @NotNull\n    private final String current;\n\n    @NotNull\n    private final HistoryType historyType;\n    \n    @NotNull\n    private final LocalDate date;\n}\n```\n\n### Event Publisher\n\n다음은 이벤트를 발행하는 publisher다.\n\nSpring에서는 ApplicationEventPublisher의 publish() 메소드를 사용하여 Context에 이벤트를 발행할 수 있다.\n\n```java\n@Service\n@Transactional(readOnly = true)\n@RequiredArgsConstructor\npublic class ReminderService {\n    ...\n    private final ApplicationEventPublisher publisher;\n    \n    @Transactional\n    public void water(ReminderCreateRequest reminderCreateRequest, Long petPlantId, Member member) {\n        // 반려 식물 물주기 수행\n        publisher.publishEvent(new HistoryEvent(...));\n    }\n    ...\n}\n```\n\n### EventListener\n\n마지막으로 이벤트 리스너다.\n\n발행된 이벤트를 컨텍스트에서 잡아서 수행하는 역할을 한다.\n\n```java\n@Component\n@RequiredArgsConstructor\npublic class HistoryEventListener {\n\n    private final PetPlantRepository petPlantRepository;\n    private final HistoryRepository historyRepository;\n    private final HistoryCategoryRepository historyCategoryRepository;\n\n    // 히스토리를 저장한다.\n    @EventListener\n    public void savePetPlantHistory(HistoryEvent historyEvent) {\n\n        PetPlant petPlant = petPlantRepository.findById(historyEvent.getPetPlantId())\n                .orElseThrow(() -> new NoSuchElementException(\"일치하는 반려 식물이 존재하지 않습니다. id: \" + historyEvent.getPetPlantId()));\n\n        HistoryCategory historyCategory = historyCategoryRepository.findByHistoryType(historyEvent.getHistoryType())\n                .orElseThrow(() -> new NoSuchElementException(\"존재하지 않는 히스토리 타입입니다. type: \" + historyEvent.getHistoryType()));\n\n        History history = History.builder()\n                .petPlant(petPlant)\n                .date(historyEvent.getDate())\n                .historyCategory(historyCategory)\n                .historyContent(HistoryContent.builder()\n                        .previous(historyEvent.getPrevious())\n                        .current(historyEvent.getCurrent())\n                        .build())\n                .build();\n\n        historyRepository.save(history);\n    }\n}\n```\n\n## 결론\n\nSpring Event를 도입하여 도메인간의 결합도를 줄여봤다.\n\n이벤트를 다루면 비동기 처리에 대한 이야기가 많이 보이는 것 같은데 아직 해당 작업이 비동기처리가 필요할 만큼 오래 걸리는 작업이라는 판단을 내리지 못했다.\n\n비동기 처리는 나중에 추가로 다뤄보도록 하자.\n\n## Reference\n\n- https://wildeveloperetrain.tistory.com/217\n- https://mangkyu.tistory.com/292"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 사건의 발단  3차 스프린트를 진행하기 하루 전날 프론트가 배포되었다.  그런데 배포된 사이트에 들어가보니 반려 식물의 타임라인을 보는 페이지가 404를 띄우고있었다. 당황하지 않고 침착하게 f12를 눌러 개발자도구를 열고 네트워크탭을 확인해보니 API가 이상하게 호출되고있었다. 평소 프론트분들을 굉장…","fields":{"slug":"/restdocs-start/"},"frontmatter":{"date":"August 11, 2023","title":"RestDocs로 API 문서화하기","tags":["문서화","RestDocs","테스트"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 사건의 발단\n\n![](.index_images/c4fe129c.png)\n\n3차 스프린트를 진행하기 하루 전날 프론트가 배포되었다.\n\n![](.index_images/dbb1d706.png)\n\n그런데 배포된 사이트에 들어가보니 반려 식물의 타임라인을 보는 페이지가 404를 띄우고있었다.\n\n당황하지 않고 침착하게 f12를 눌러 개발자도구를 열고 네트워크탭을 확인해보니 API가 이상하게 호출되고있었다.\n\n평소 프론트분들을 굉장히 신뢰하던 주노는 설마 하는 마음에 Notion을 열어봤다. 아니나다를까 Notion에 API 명세가 이상하게 작성되어있었다.\n\n범인은 바로 이것저것 끄적이다가 되돌리기를 하지 않은 주노...\n\n![](.index_images/03624593.png)\n\n위와 같은 사건을 겪으면서 사람 손으로 작성되는 문서의 신뢰도 문제를 인지했고, API 문서화를 자동화할 수 있는 기술의 필요성을 느꼈다.\n\n## 서론\n\nAPI 문서화 방식에 대해 찾아본 결과 크게 두가지 방법을 알 수 있었다.\n\n### 알아보기\n\n`1. Spring RestDocs`\n\nSpring Rest Docs는 Spring에서 제공하는 문서화 도구다.\n\n- 테스트 작성이 강제된다는 특징이 있다.\n- 테스트를 통해 도출된 결과를 문서로 작성하기 프로덕션 코드에 영향이 없다는 특징이 있다.\n- 아래 설명할 Swagger와 비교했을 때 설정이 약간 번거롭다는 단점이 있다. \n\n`2. Swagger` \n\nSwagger는 API 문서화를 돕는 오픈소스 툴이다.\n\n- 테스트 작성이 필수가 아니라는 특징이 있다.\n- swagger 이용 시 실제 API가 호출된다는 특징이 있다.\n- 테스트 작성을 안하는 대신 프로덕션 코드에 문서화와 관련된 코드가 작성된다. (DTO, Controller 등)\n\n### Spring RestDocs\n\n피움 팀은 RestDocs를 사용하기로 결정했고 이유는 다음과 같다.\n\n- 현재 피움팀은 각 계층별 테스트를 꼼꼼하게 작성하고있고 인수테스트까지 작성하고 관리하고있으므로 테스트가 강제된다는 점은 큰 단점으로 다가오지 않았다.\n- 코드리뷰가 필수적으로 이뤄지는 현 스프린트 단계에서 프로덕션 코드의 가독성을 해치는 방향이 오히려 큰 단점이 될 수 있다.\n- 성공하는 테스트에 대해서만 문서화가 이뤄지기 때문에 문제상황에서 필요했던 문서의 신뢰성을 충족시킨다.\n- (프론트 쵸파가 RestDocs를 좋아한다 🦌)\n\n> [피움🌱 문서화 도입 관련 discussion](https://github.com/woowacourse-teams/2023-pium/discussions/181)\n\n## 시작하기\n\nRestDocs를 설정하는 과정을 정리하려고한다.\n\n### RestAssured vs MockMvc?\n\nRestDocs를 적용하는 방법은 RestAssured를 기반으로 문서를 생성하는 방법과 MockMvc를 기반으로 문서를 생성하는 방법으로 크게 두가지가 있다.\n\n피움팀은 현재 인수테스트도 작성되어있기 때문에 RestAssured를 사용해도 충분히 문제가 없다.\n\n하지만 여러 계층이 협업하는 인수테스트 환경에 문서화와 관련된 코드가 작성된다면 안그래도 복잡한 인수테스트 코드의 가독성을 해칠 것이라고 우려되었다.\n\n슬라이스 테스트를 수행하여 비교적 코드의 무게가 가벼운 Controller 테스트가 MockMvc로 작성되어있었기 때문에 MockMvc를 기반으로 RestAssured를 적용하기로 결정했다.\n\n### Gradle 설정\n\n> TIP : `asciidoctor`는 adoc 파일을 html 등으로 변환해주는 도구입니다\n\n```gradle\nplugins {\n    ...\n    id \"org.asciidoctor.jvm.convert\" version \"3.3.2\" // asciidoctor 플러그인 추가\n}\n\nconfigurations {\n    asciidoctorExt // asciidoctorExt에 대한 선언\n    ...\n}\n\n\ndependencies {\n    ...\n    asciidoctorExt 'org.springframework.restdocs:spring-restdocs-asciidoctor' // asciidoctorExt에 spring-restdocs-asciidoctor 의존성 추가\n    testImplementation 'org.springframework.restdocs:spring-restdocs-mockmvc' // mockMvc 사용\n}\n\next {\n    snippetsDir = file('build/generated-snippets') // 스니펫이 생성되는 디렉터리 경로를 설정\n}\n\ntest {\n    outputs.dir snippetsDir // 스니펫이 생성되는 디렉터리를 설정\n}\n\nasciidoctor { // Gradle이 asciidoctor Task를 수행하는 설정 (함수 선언)\n    configurations 'asciidoctorExt' // asciidoctor 확장 설정\n    baseDirFollowsSourceFile() // .adoc 파일을 include 하면서 사용하기 위한 설정\n    inputs.dir snippetsDir // 스니펫을 불러올 위치 설정\n    dependsOn test // Gradle의 test Task 이후 asciidoctor를 수행\n}\n\nasciidoctor.doFirst { // asciidoctor Task가 수행될 때 가장 먼저 수행\n    delete file('src/main/resources/static/docs')\n}\n\ntask copyDocument(type: Copy) { // 생성된 html 파일을 옮긴다\n    dependsOn asciidoctor // Gradle의 asciidoctor Task 이후 수행\n    from file(\"${asciidoctor.outputDir}\")\n    into file(\"src/main/resources/static/docs\")\n}\n\nbuild {\n    dependsOn copyDocument // build 이후 html 파일 복사\n}\n\nbootJar {\n    dependsOn asciidoctor // asciidoctor 이후 bootJar 수행\n    from (\"${asciidoctor.outputDir}\") {\n        into 'static/docs'\n    }\n    ...\n}\n```\n\n### 테스트 코드 작성하기\n\n우선 다음과 같은 방식으로 코드를 작성할 수 있다.\n\n```java\n@AutoConfigureRestDocs\n@WebMvcTest(controllers = PetPlantController.class)\nclass PetPlantControllerTest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @MockBean\n    private PetPlantService petPlantService;\n    \n    @Test\n    void 반려_식물_단건_조회_정상_요청시_200을_반환() throws Exception {\n        PetPlantResponse response = RESPONSE.피우미_응답;\n        given(petPlantService.read(anyLong(), any(Member.class)))\n                .willReturn(response);\n    \n        mockMvc.perform(get(\"/pet-plants/{id}\", 1L)\n                        .header(\"Authorization\", \"pium@gmail.com\")\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .characterEncoding(StandardCharsets.UTF_8))\n                .andDo(document(\n                            \"petPlant/findById/\",\n                            Preprocessors.preprocessRequest(Preprocessors.prettyPrint()),\n                            Preprocessors.preprocessResponse(Preprocessors.prettyPrint()),\n                            requestHeaders(\n                                    headerWithName(\"Authorization\").description(\"사용자 인증 정보\")\n                            ),\n                            pathParameters(\n                                    parameterWithName(\"id\").description(\"반려 식물 ID\")\n                            )\n                        )\n                )\n                .andExpect(status().isOk())\n                .andDo(print());\n    }\n}\n```\n\n`andDo(document(...))` 부분이 문서화와 관련된 내용들이다.\n\n`document()` 내부의 파라미터를 하나씩 살펴보자\n\n- `\"petPlant/findById/\"`\n\n스니펫이 생성되는 대상 디렉터리 명이다. \n\n해당 테스트가 완료되면 문서화 대상으로 지정한 값들에 대해 `.adoc` 문서가 생성된다.\n\n생성 흐름과정은 뒤에서 설명한다.\n\n우선 각 파라미터가 어떤 결과물을 도출하는지 확인해보자.\n\n![](.index_images/22e5bb11.png)\n\n- `Preprocessors.preprocessRequest(prettyPrint())`\n\n요청값을 읽기 쉬운 서식으로 지정한다.\n\n![](.index_images/f25529d5.png)\n\n- `Preprocessors.preprocessResponse(prettyPrint())` \n\n응답값을 읽기 쉬운 서식으로 지정한다.\n\n![](.index_images/207cbfe0.png)\n\n> [Preprocessors 관련 공식문서](https://docs.spring.io/spring-restdocs/docs/current/reference/htmlsingle/#customizing-requests-and-responses-preprocessors)\n\n- `requestHeaders(headerWithName(\"Authorization\").description(\"사용자 인증 정보\"))` : RequestHeader에 대한 명세를 한다.\n\n![](.index_images/4282f935.png)\n\n- `queryParameters(parameterWithName(\"name\").description(\"사전 식물 검색 파라미터\"))`\n\n![](.index_images/c9dbf6a8.png)\n\n- `pathParameters(parameterWithName(\"id\").description(\"반려 식물 ID\"))`\n\n![](.index_images/f0d0524b.png)\n\n> `pathParameters`를 사용하기 위해\n> \n> mockMvc.perform(`get`(\"/pet-plants/{id}\", 1L) 에서\n> \n> `RestDocumentationRequestBuilders`의 `get()` 메서드를 사용해야한다는 부분을 신경써야한다.\n> \n> 자세한 내용은 [공식문서 참고](https://docs.spring.io/spring-restdocs/docs/current/api/org/springframework/restdocs/mockmvc/RestDocumentationRequestBuilders.html)\n> \n> ![](.index_images/cc09df18.png)\n> 고마워요 ! 리뷰어 그레이 ! \n\n문서화를 수행하고자 하는 나머지 테스트들에도 동일한 방법으로 작성해준다. \n\n### adoc 스니펫 생성하기\n\n> Snippet : 단편, 부분적, 작은 조각이라는 뜻으로 여기서는 `문서 조각` 정도로 이해하면 된다.\n> \n> 문서 조각을 모아서 하나의 문서를 만들게 된다.\n\n이렇게 작성한 테스트들을 수행해보자.\n\n![](.index_images/35bc712c.png)\n\n그러면 `build/generated-snippets/{파라미터에서 설정한경로}`에 adoc 파일이 생성된 것을 확인 할 수 있다.\n\n![](.index_images/5dd3f00f.png)\n\n```adoc\n// http-request.adoc 파일\n[source,http,options=\"nowrap\"]\n----\nGET /pet-plants/1 HTTP/1.1\nContent-Type: application/json;charset=UTF-8\nAuthorization: pium@gmail.com\nHost: localhost:8080\n\n----\n```\n\n### asciidoc 문서 생성하기\n\n위에서 생성한 adoc 스니펫들을 모아 하나의 API 문서를 만든다.\n\n`src > docs > asciidoc` 디렉터리를 생성하고 `.adoc` 파일을 생성한다.\n\n![](.index_images/850cce57.png)\n\nAPI가 여러개인 경우 각각을 파일로 관리한 뒤 하나의 adoc 파일로 include하여 합칠 수도 있다.\n\n#### index.adoc\n\n다음과 같이 adoc 파일을 작성할 수 있다.\n\n```adoc\n// index.adoc\n= Pium Application API Document\n:doctype: book\n:source-highlighter: highlightjs\n:sectlinks:\n:toc: left\n:toclevels: 3\n\ninclude::dictionaryPlant.adoc[]\ninclude::petPlant.adoc[]\ninclude::reminder.adoc[]\ninclude::history.adoc[]\n```\n\n```adoc\n// petPlants.adoc\n== 반려 식물(PetPlants)\n\n=== 반려 식물 단건 조회\n\n==== Request\n\ninclude::{snippets}/petPlant/findById/http-request.adoc[]\ninclude::{snippets}/petPlant/findById/request-headers.adoc[]\ninclude::{snippets}/petPlant/findById/path-parameters.adoc[]\n\n==== Response\n\ninclude::{snippets}/petPlant/findById/http-response.adoc[]\n...\n```\n\nadoc 파일의 자세한 문법은 [asciidoctor 공식문서 참고](https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/)\n\n### HTML 문서 생성하기\n\n이제 웹 페이지가 읽을 수 있도록 HTML 문서를 생성해보자.\n\nHTML 문서는 `src > docs > asciidoc`에 생성된다.\n\n다음 명령어를 수행해보자.\n\n```shell\n./gradlew asciidoctor\n```\n\n![](.index_images/3b9772e1.png)\n\n그러면 `build > docs > asciidoc` 경로에 문서가 생성된다.\n\n![](.index_images/5daac1d0.png)\n\n### 애플리케이션에서 확인해보기\n\nhttp://localhost:8080/docs/index.html 으로 접속해서 문서를 확인해보자.\n\n우선 gradle build를 수행한다.\n\n```shell\n./gradlew build\n```\n\n![](.index_images/adb0a6ab.png)\n\n이전에 Gradle에서 작성했던 스크립트 내용 중 copyDocument Task가 build가 일어난 뒤 수행되기 때문이다.\n\n```gradle\ntask copyDocument(type: Copy) { // 생성된 html 파일을 옮긴다\n    dependsOn asciidoctor // Gradle의 asciidoctor Task 이후 수행\n    from file(\"${asciidoctor.outputDir}\")\n    into file(\"src/main/resources/static/docs\")\n}\n\nbuild {\n    dependsOn copyDocument // build 이후 html 파일 복사\n}\n```\n\n그러면 `src > main > resources > static > docs` 경로에 HTML 문서가 생성된다.\n\n![](.index_images/6b2555b4.png)\n\n로컬에서 구동되는 애플리케이션은 해당 경로를 참조하기 때문에 파일을 복사해줬다.\n\n이제 어플리케이션을 구동하고 http://localhost:8080/docs/index.html 로 접속하면 다음과 같이 문서를 확인할 수 있다.\n\n![](.index_images/ad2fa30b.png)\n\n## Reference\n\n- https://spring.io/projects/spring-restdocs\n- https://hudi.blog/spring-rest-docs/\n- https://techblog.woowahan.com/2597/\n- https://dallog.github.io/apply-rest-docs/"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. OAuth란? 현재 개발하고 있는 서비스에서 구글 캘린더에 접근을 한다고 가정을 해봅시다. 현재 접근하는 사용자의 구글 캘린더 정보를 얻고자 한다면 구글 계정으로 로그인을 해야합니다. 진짜 간단한 방법이 있습니다. 사용자로부터 구글 id와 password를 받고 저희가 대신 로그인 해서 해당 사용자의 …","fields":{"slug":"/OAuth2.0/"},"frontmatter":{"date":"August 10, 2023","title":"내가 이해한 OAuth2.0","tags":["OAuth2.0"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다.\n\n\n## OAuth란?\n\n현재 개발하고 있는 서비스에서 구글 캘린더에 접근을 한다고 가정을 해봅시다. 현재 접근하는 사용자의 구글 캘린더 정보를 얻고자 한다면 구글 계정으로 로그인을 해야합니다. 진짜 간단한 방법이 있습니다. 사용자로부터 구글 id와 password를 받고 저희가 대신 로그인 해서 해당 사용자의 계정 정보를 받아오는 것입니다.\n\n하지만 이 방법은 매우 위험합니다. 계정 정보가 탈취당할 수도 있고, 애초에 **대리 로그인**이라니 신개념 보이스피싱으로 오해받을 수도 있습니다.\n\n제일 괜찮은 방법은 구글 자체에서 로그인 서비스를 대체할 수 있는 방식을 만들어서 제공을 하는 것입니다. 실제로  과거에 AuthSub라는 자체 개발한 방법을 통해 구현을 했지만 이 정형화 되지 않은 방법은 다양한 문제를 야기할 수 있었습니다. 흔히 카카오 로그인, 구글 로그인, 깃헙 로그인 등등 여러가지의 방식이 존재하는데 형식이 통일되어 있지 않다면 이 모든 것들을 각각 구현해서 사용을 해야하는 문제점이 있습니다. \n\n이러한 문제점을 해결하기 위해 나타난 개념이 OAuth입니다. 초기 버전인 1.0은 범위문제, 불확실한 역할, 보안 문제등으로 인해서 많이 사용 되지 않고 OAuth 2.0이 통상적으로 많이 사용 되고 있습니다. \n\n> 💡 **인증과 인가**: 인증은 요청자의 신원을 확인하는 것이고, 인가는 신원이 확인된 어느 사람이 권한이 있는지를 확인하는 것입니다. 개인적으로 생각했을때 인증은 사람의 직업 검증하는 것이고 인가는 그 사람의 직책을 통해 권한을 접근한다고 생각합니다. (직책에 따른 정보 접근 권한이 다르기 때문)\n\n\n## OAuth 2.0\n\nOAuth2.0을 이해하기 위해서는 크게 4가지 용어를 알고 있으면 됩니다. 바로 Resource Owner, Client, Resource Server, Authorization Server입니다. 각각의 역할과 설명은 다음과 같습니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/c65c8e80-b04a-4b51-b66a-dbdd6d43b543)\n\n- `Resource Owner`**:** 리소스(정보)의 주인. 즉,서비스를 이용하고자 하는 사용자를 의미합니다.\n- `Client`: 리소스(정보)를 가공 혹은 Resource Owner에게 제공하는 주체. OAuth 2.0을 개발하는 어플리케이션(서비스)을 의미한다.\n- `Resource Server`: 리소스(정보)를 갖고 있는 주체. 흔히 카카오, 구글, Github등 사용자 정보를 제공하는 주체를 의미한다.\n- `Authorization Server`: Resource Owner를 인증하고, Client에 인가를 할당하는 역할이다.\n\nOAuth를 가장 많이 도입한 기능 중 하나는 소셜 로그인입니다. 하나의 예시를 들어보겠습니다. 사용자가 Todo앱에 접근하려고 합니다. 사용자의 Todo인지를 확인하기 위해서는 로그인 기능이 필요합니다. 그런데 사용자는 회원가입을 하기 귀찮습니다. 이 과정에서 사용자의 기본 정보 (이메일, 닉네임, 프로필 사진 등)를 갖고 있는 소셜 네트워크가 있다고 생각해 봅시다. Client는 이 소셜 네트워크를 통해서 사용자에게 간단한 회원가입/로그인 기능을 제공할 수 있습니다. 이제 본인이 Todo앱에 회원가입을 하고, 나의 Todo를 카카오 스토리에 공유한다고 상상하고 OAuth의 동작과정에 대해 한번 알아보겠습니다.\n\n> 💡 OAuth에는 크게 4가지 grant(승인 방식)가 있는데, 이번에 다루는 승인 방식은 기본적으로 사용되고 있는 **Authorization Code Grant(권한 부여 승인 코드 방식)** 을 통해 기준으로 설명하겠습니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/e4f90a4b-6783-4d32-974a-2fdc2c5d3ae3)\n\n- 첫 번째는 로그인 요청입니다. 흔히 우리가 알고있는 “카카오로 로그인하기”를 클릭 했을 때 나오는 설정되는 방식입니다. 이때 Authorization Server에 4가지 파라미터를 동봉해서 보냅니다.\n    - client_id(required): Server에서 발급받은 인증 id입니다.\n    - redirect_uri(optional): Server에 등록할 때 함께 입력한 redirect_uri입니다.\n    - response_type(required): 반드시`code`로 설정해야 합니다.\n    - scope(optional): 제공하는 정보 범위를 설정합니다.\n- Authorization Server에서 Resource Owner에게 아이디와 비밀번호를 요구합니다. 사용자 인증이 된다면, 미리 입력받은 redierct_uri로 이동합니다.\n- 해당 uri로 리다이렉트 되면서 함께 동봉되어 온 Authorization Code를 받고 이 코드를 Access Token으로 교환합니다.\n- 교환한 Access Token을 통해서 Resource Server에서 원하는 데이터를 얻을 수 있습니다. (Todo일정을 카카오 스토리에 공유할 수 있습니다!!)\n\n위와 같은 과정을 통해서 OAuth를 통해서 제 3의 서비스에서 사용자의 정보에 인가를 받아서 가공하거나 직접적으로 제공할 수 있습니다.\n\n사용자로부터 로그인 정보를 받아서 로그인을 하고 사용자의 권한을 부여받은 Access Token을 통해서 Reource Server로부터 데이터를 얻는 것입니다.\n\n---\n\n### OAuth공부를 하면서 들었던 의문점\n\nAccess Token을 통해서 Resource에 있는 데이터를 얻는 것은 알겠는데, 그래서 이게 로그인이랑 어떤 관련이 있는가? 그리고, Client DB에 있는 데이터나 API랑도 관련이 있나?\n\nAuthorization Server를 통해서 넘겨받은 Access Token을 활용하면 소셜 로그인을 구현할 수 있었고, 해당 토큰을 통해서 모든 정보에 접근이 가능하겠구나 생각을 했었습니다. 하지만, 이 Access Token은 Resource Server에 있는 데이터를 얻는데 필요한 Access Token이고 **Client 내부에서 접근하는 DB와 API랑은 관련이 없다는 것입니다.**\n\nClient에서는 넘겨받은 Access Token을 통해서 개별적으로 구축한 member DB에서 해당 유저를 식별하고, 그에 맞는 로그인 기능을 구현해야 합니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/e6c1a44b-6e6e-4034-bca6-cfe9e2c5d8c0)\n\n로그인 기능을 개별적으로 구현해야 한다는 얘기를 듣고, Client의 역할을 어떻게 분리가 되는지에 대해 의문이 들었습니다. 여기서 Client는 사용자가 이용하는 앱 서비스인데, 사용자와 직접적인 인터렉션을 하는 Client(FE)와 데이터를 제공하는 Sever(BE)로 분리가 됩니다. \n\n그렇다면 OAuth를 이용한 서비스를 구현할 때 Client안에서는 어떤 일이 일어날까요?\n\n1. 사용자 로그인 페이지 제공 (FE)\n2. redirect_uri로 이동시키고, 넘겨받은 Autorization Code를 BE로 넘겨줌 (FE)\n3. FE로부터 넘겨받은 Autorization Code를 Authorization Server에 전달하고, Access Token과 Refresh Token을 받음(BE)\n4. 넘겨 받은 Access Token을 통해 Resource Owner 정보 조회(BE)\n5. DB Table에 해당 유저 확인 후 Client Session이나 Client Token 전달(BE)\n6. 전달 받은 Session이나 Token을 통해서 유저 로그인 유지 (FE)\n7. 사용자가 Client의 서비스를 이용하고자 할 때 Client Session을 통해 인증을 함 (FE)\n8. Client에서 제공하고자 하는 데이터가 Resource Server에서 얻어야 하는 데이터라면, 저장해 놓았던 Access Token을 통해 Resource Server로 부터 데이터를 받아옴. 그리고 Client에서 제공하는 API와 잘 조합해서 데이터 제공(BE)\n9. 사용자가 확인 가능하게 UI제공 (FE)\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/a668a660-8b62-4fbd-8caf-e3998c570a1c)\n\nClient에서의 역할을 나눠보면 다음과 같습니다.\n\n### FE\n\n- 로그인 페이지 제공\n- redirect_uri를 통해 Authorization Code 전달\n- Resource Server에 맞는 UI 제공\n\n### BE\n\n- 전달받은 Authorization Code를 통해 Access Token 교환\n- Access Token을 통해 Resource Server에 있는 데이터 요청 및 가공\n- Access Token만료 시 Refresh Token으로 Access Token 갱신\n\n---\n\n### 참조\n\n[OAuth의 인증 종류](https://blog.naver.com/mds_datasecurity/222182943542)\n\n[OAuth의 개념과 동작 원리](https://hudi.blog/oauth-2.0/)\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. style-lint, husky를 통한 컨벤션 자동화 하기 프로젝트는 혼자 할 수 도 있지만 팀원들과 함께 진행하는 경우도 있습니다. 피움의 경우에 팀원들과 함께 진행하고 있습니다. 개인적으로 생각했을 때 코드를 작성한다는 것은 그 자체로 하나의 문서를 만드는 것이라고 생각합니다. 그리고 그 문서의 양…","fields":{"slug":"/stylelint-and-husky/"},"frontmatter":{"date":"August 07, 2023","title":"stylelint와 husky를 통해 컨벤션 통일하기","tags":["stylelint","husky","컨벤션"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다.\n\n# style-lint, husky를 통한 컨벤션 자동화 하기\n\n프로젝트는 혼자 할 수 도 있지만 팀원들과 함께 진행하는 경우도 있습니다. 피움의 경우에 팀원들과 함께 진행하고 있습니다. 개인적으로 생각했을 때 코드를 작성한다는 것은 그 자체로 하나의 문서를 만드는 것이라고 생각합니다. 그리고 그 문서의 양식을 통일하는 것이 가독성이나 유지 보수를 수월하게 한다고 생각합니다. 그렇기에 각각의 팀바다 코드 컨벤션이 존재하고, 그 컨벤션을 강제하도록 하는 플러그인 (`eslint`, `prettier`)이 존재합니다. \n\n이번에 소개할 기능은 css컨벤션을 맞출 수 있게 도와주는 style-lint와 git 메세지 양식을 지정해주는 husky에 대해 소개해 보려 합니다. \n\n### Stylelint\n\n[링크](https://stylelint.io/)\n\n> A mighty CSS linter that helps you avoid errors and enforce conventions.\n> \n\nstylelint의 소개글 입니다. 간단하게 말해서 css 에러를 피하고 컨벤션을 강요하도록 하는 기능입니다. 설치는 다음과 같습니다. 저희는 styled-component를 사용하기 때문에 해당 요건에 맞는 라이브러리를 설치하겠습니다.\n\n```bash\nnpm install --save-dev stylelint stylelint-config-styled-components\n```\n\n라이브러리 설치가 끝나면 `.stylelintic`이라는 파일에서 원하는 css규칙을 결정할 수 있습니다. \n\n```json\n{\n  \"extends\": [\"stylelint-config-clean-order\", \"stylelint-config-styled-components\"],\n  \"plugins\": [\"stylelint-order\"],\n  \"customSyntax\": \"postcss-styled-syntax\"\n}\n```\n\n피움에서는 `stlyelint-order`라는 플러그인을 설치한 뒤에 해당 규칙에 따라 css 컨벤션을 정했습니다. 실행시키는 방법은 다음과 같습니다.\n\n```json\n\"scripts\": {\n\t\"stylelint\": \"stylelint --ignore-path .gitignore '**/*.(ts|tsx)'\",\n}\n```\n\n위와같이 설정하고 명령어를 입력하게 된다면 css파일을 돌면서 규칙에 맞지 않는 파일을 찾아줍니다. \n\n```bash\nnpm run stylelint\n```\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/ddb7ccf7-48ac-498f-a01b-199966021c93)\n\n\n하지만 우린 좀 더 편한걸 원합니다. 매번 실행시키고 경고창들을 변경하기 보다 실행시킬때마다 자동으로 수정시켜주는걸 원합니다. 만약 그것을 원한다면 명령어에 `—fix`만을 추가해주면 됩니다.\n\n```json\n\"scripts\": {\n\t\"stylelint\": \"stylelint --ignore-path .gitignore '**/*.(ts|tsx)' --fix\",\n}\n```\n\n\nhttps://github.com/pium-official/pium-official.github.io/assets/50974359/0f9aa05c-a861-4601-a959-58754752294d\n\n\n여기에 더 가서 파일을 저장할 때마다 실행시킬수도 있습니다.\n\nvsc의 `setting.json`에 들어가서 stylelint에 대한 설정을 해주면 됩니다. (vsCode extension을 먼저 설치해야 합니다)\n\n```json\n//...\n\t\"editor.codeActionsOnSave\": {\n\t\t\"source.fixAll.stylelint\": true,\n\t\t\"source.fixAll.eslint\": true,\n\t\t\"source.fixAll.prettier\": true\n\t},\n\t\"stylelint.validate\": [\n\t\t\"typescript\",\n\t\t\"typescriptreact\",\n\t\t\"css\",\n\t\t\"less\",\n\t\t\"postcss\",\n\t\t\"scss\",\n\t\t\"html\",\n\t\t\"javascript\",\n\t\t\"vue-html\",\n\t\t\"vue-postcss\",\n\t\t\"sass\",\n\t\t\"markdown\",\n\t\t\"vue\"\n\t],\n//...\n```\n\ncodeActionOnSave는 저장할때마다 stylelint에 있는 모든 것을 fix한다는 것이고 아래에 있는 stylelint.validate는 해당 옵션을 적용시킬 확장자를 의미합니다. 위와 같이 설정하고 저장한다면 저장을 할 때마다 자동으로 컨벤션 적용이 되는 것을 볼 수 있습니다.\n\n### Husky\n\n[링크](https://typicode.github.io/husky/)\n\n> Husky improves your commits and more 🐶 *woof!*\n> \n> \n> You can use it to **lint your commit messages**, **run tests**, **lint code**, etc... when you commit or push. Husky supports **[all Git hooks](https://git-scm.com/docs/githooks)**.\n> \n\n이번에는 `git`에 대한 컨벤션을 맞추기 위해 `husky`라는 라이브러리를 통해 `git hook`을 실행하보려 합니다. `git`에는 `commit`, `push` 등의 명령어가 있지만, 각각의 명령어가 실행되기 전에 전반적으로 실행시키고 싶은 명령어가 있거나, 컨벤션을 맞추거나, `commit` 메세지 형식을 강제할 수 있습니다. 이러한 일련의 과정이 필요한 이유는 팀 컨벤션에 맞춰진 **신뢰성 있는 코드**를 원격 저장소에 저장하기 위해서 입니다. 예시로 앞서 얘기한 `stylelint`가 적용이 되지 않은 코드가 원격 저장소에 올라가게 된다면, 컨벤션이 맞춰지지 않은것이고 공통된 문서로써의 역할을 잃을 수 있습니다. 이에 각각의 명령어기 실행되기 전에 단계를 `pre`라는 접두사를 붙여서 `pre-commit` 또는 `pre-push`단계라고 하는데, 이 각각의 단계에서 `test`를 실행하거나 build`를` 실행할 수도 있습니다.\n\n다음 명령어를 통해 설치할 수 있습니다.\n\n```bash\nnpm install husky --save-dev\nnpx husky install // 이 명령어를 통해서 git hook을 사용할 수 있도록 함.\n```\n\n`hooks`를 만들기 위해서는 다음과 같이 입력하면 됩니다. `husky add <file> [cmd]` → 이와 같은 형식으로 `husky` 폴더를 생성할 수 있습니다. (단, 해당 명령어를 입력하기 전에 `husky install`을 꼭 해야합니다!)\n\n```bash\nnpx husky add .husky/pre-commit \"npm test\"\n```\n\n위와같이 설정하고 나면 .husky폴더에 다음과 같은 파일이 생성됩니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/82a9d271-4748-4871-be3a-220ca36c2de0)\n\n이제 모든 커밋을 하기 전에 `npm test`명령어가 실행되게 됩니다. 이제, `test`를 통과하지 못한다면 `commit`은 일어나지 않습니다. 이를 통해 원격 저장소에 올라가는 코드들의 신뢰성은 급상승 하게 됩니다! \n\n하지만 하나의 단점이 있습니다. 피움 프로젝트를 모노레포 형식으로 진행하고 있는데, `husky`가 `githook`이기 때문에 `npm`에 의존성이 없는 `backend` 폴더에서도 커밋이 발생하기 전에 해당 `npm test`가 실행된다는 점입니다. 이는 상당히 좋지 못합니다… `pre-commit` 단계에서 명령어를 `cd frontend && npm test`와 같이 변경해서 실행해봐도 백엔드에서 커밋을 할때마다 해당 명령어가 실행된다는게 상당히 비효율적인 방식이라고 생각했습니다. 따라서 피움 프론트엔드 CI는 husky로 진행하지 않기로 했습니다. 단, `commit` 메세지 형태는 강제할 수 있었습니다. `git` 컨벤션인 부분이라 FE/BE 모두 공통으로 해당되는 내용이었기에 해당 부분만 추가하는 방식으로 `husky`를 사용하기로 했습니다.\n\n```bash\n#!/usr/bin/env sh\n. \"$(dirname -- \"$0\")/_/husky.sh\"\n\n# 커밋 컨벤션\n# 0. 검사 예외 조건 (자동 생성, 최초 커밋)\n# - Merge branch*, Merge pull request*, initial*\n# 1. 접두사의 글자는 소문자\n# 2. 맨 마지막 글자 '.' 마침표 금지\n# 3. 커밋 접두사 (규칙: '접두사' + '콜론' + ' ')\n# - feat: 새로운 기능 추가\n# - fix: 버그 수정\n# - docs: 문서의 수정\n# - style: (코드의 수정 없이) 스타일(style)만 변경(들여쓰기 같은 포맷이나 세미콜론을 빼먹은 경우)\n# - refactor: 코드를 리펙토링\n# - test: Test 관련한 코드의 추가, 수정\n# - chore: (코드의 수정 없이) 설정을 변경\n# - design: css 코드 수정\n# - build: build 및 관련 설정 변경\n\nCOMMIT_MSG_FILE=$1\nFIRST_LINE=`head -n1 ${COMMIT_MSG_FILE}`\nRES=\"needCheck\" # needCheck, auto, initial, lintError*, clear\n\nif [[ $FIRST_LINE =~ ^(Merge branch) ]] ||\n   [[ $FIRST_LINE =~ ^(Merge pull request) ]]; then\n  RES=\"auto\"\nfi\n\nif [[ $FIRST_LINE =~ ^(initial) ]]; then\n  RES=\"initial\"\nfi\n\nif [ $RES == \"needCheck\" ]; then\n  if [[ $FIRST_LINE =~ (\\.)$ ]]; then\n    RES=\"lintError1\"\n  fi\n\n  if [[ ! $FIRST_LINE =~ ^(feat: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(fix: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(docs: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(style: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(refactor: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(test: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(design: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(build: ) ]] &&\n     [[ ! $FIRST_LINE =~ ^(chore: ) ]]; then\n    RES=\"lintError2\"\n  fi\n\n  if [[ ! $RES =~ ^(lintError) ]]; then\n    RES=\"clear\"\n  fi\nfi\n\nif [[ $RES =~ ^(lintError) ]]; then\n  if [[ $RES == \"lintError1\" ]]; then\n    echo \"CommitLint#1: 문장 마지막의 ('.') 마침표를 제거해주세요.\"\n  fi\n  if [[ $RES == \"lintError2\" ]]; then\n    echo \"CommitLint#2: 접두사, 콜론, 띄어쓰기 형태를 확인하세요. (feat: , fix: , docs: , style: , refactor: , test: , chore: )\"\n  fi\n  exit 1\nelif [[ $RES == \"auto\" ]]; then\n  echo \"Automatically generated commit message from git\"\nelif [[ $RES == \"initial\" ]]; then\n  echo \"Initial commit\"\nelif [[ $RES == \"clear\" ]]; then\n  echo \"Pass commit lint!\"\nfi\n\nexit 0\n```\n\n프로젝트 `git` 컨벤션에 맞춰서 메세지 작성을 강제 했고, 이를 통해 `commit` 메세지 통일을 할 수 있었습니다!\n\n---\n\n### 맺는말\n프로젝트는 팀원들이 하나의 문서를 만드는 것이라고 생각을 합니다. 그리고 통일된 문서를 만들때 가장 필요한 것은 규칙(컨벤션)이고 그 규칙을 자동으로 강제할 수 있고, 코드의 신뢰성을 높여줄 수 있는 라이브러리에 대해서 한번 알아봤습니다.\n스타일 컨벤션을 맞추기 위해서는 `stylelint`를 `git hook`을 통해 `git`컨벤션을 맞추고 싶은 경우에는 `husky`를 통해서 컨벤션을 통일 할 수 있었습니다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. CRA없에 webpack으로 react 세팅하기 피움 서비스 환경을 만들기로 하면서, FE의 개발 스펙은 React + TypeScript로 설정하였습니다. React는 당연 현존 최강 라이브러리이고, 여기에 JavaScript의 동적 타입을 컴파일 단계에서 잡아주는 TypeScript까지 더해서 안정…","fields":{"slug":"/react-setting-without-cra/"},"frontmatter":{"date":"August 06, 2023","title":"webpack으로만 React 설정하기 (without CRA)","tags":["React","Webpack"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다.\n\n\n# CRA없에 webpack으로 react 세팅하기\n\n피움 서비스 환경을 만들기로 하면서, FE의 개발 스펙은 React + TypeScript로 설정하였습니다. React는 당연 현존 최강 라이브러리이고, 여기에 JavaScript의 동적 타입을 컴파일 단계에서 잡아주는 TypeScript까지 더해서 안정적인 서비스를 개발하려고 합니다. \n\n평소같았으면 `CRA`를 통해 손쉽게 리액트 설치를 하고 개발을 시작했을 텐데, CRA의 단점이 이미 정교하게 세팅된 babel과 webpack 설정을 따라 개발을 해야 한다는 것이고 그에 따른 트레이드 오프가 상당히 많이 발생할 수 있다는 점입니다. (하나의 예시로 CRA를 통해서는 절대경로 지정 하기가 어렵습니다.) 또한, 이왕에 내 서비스 개발하는거 남이 차려준 밥상 보다 내가 직접 한번 해보겠다는 생각으로 webpack으로만 react세팅을 시도해 보려고합니다 (가장 큰 원인은 요구사항이 CRA없이 webpack으로 react 설치하는 거였습니다 ㅎ)\n\n### package.json 설치\n\n우선 가장 처음 해야할 단계입니다. react, typescript, webpack 등 우리가 설치할 의존성 관리와 기타 프로젝트 정보를 저장해 줄 `package.json`을 설치 해줍니다.\n\n```bash\nnpm init // 또는\nyarn init -y\n```\n\n### 환경 구축에 필요한 패키지 설치\n\n```bash\nnpm install react react-dom // react 설치\nnpm install -D typescript @types/react @types/react-dom // typescript 설치\nnpm install tsc-init // tsconfig 설치\n\nnpm install -D webpack webpack-cli // 웹팩 라이브러리와 명령어를 통해 웹팩을 이용할 수 있는 라이브러리 설치\nnpm install -D html-webpack-plugin webpack-dev-server ts-loader  // 각각 번들 후 html파일을 만들어주는 플러그인, 개발할 때 사용할 웹 서버, typescript파일을 javascript로 변환하는 webpack loader 설치\n```\n\n위에 파일들을 설치하고 나면 이제부터 본격적인 세팅을 합니다.\n\n### webpack.config.js 파일 설정\n\n웹팩 파일 설정입니다. \n\n```jsx\n// webpack.config.js\nconst { resolve } = require('path'); // path 설정에 도움을 줍니다.\nconst HtmlWebpackPlugin = require('html-webpack-plugin');\nconst ReactRefreshWebpackPlugin = require('@pmmmwh/react-refresh-webpack-plugin');\nconst ReactRefreshTypeScript = require('react-refresh-typescript');\n\nconst isDevelopment = process.env.NODE_ENV === 'development';\n\nmodule.exports = {\n  entry: resolve(__dirname, 'src', 'index.tsx'), \n  output: {\n    path: resolve(__dirname, 'dist'), // 번들 파일이 저장될 디렉토리\n    filename: 'bundle.js', // 파일 이름\n    assetModuleFilename: 'assets/[name][ext]', // 에셋의 파일 이름\n    clean: true, // 빌드 이전 결과물 정리 옵션\n    publicPath: '/', // 빌드된 파일 앞에 붙일 경로\n  },\n  resolve: {\n    extensions: ['.ts', '.tsx', '...'], // 파일 확장자. TS및 JSX확자자ㅡㄹ 설정했습니다.\n    alias: { // 절대경로 설정\n      types: resolve(__dirname, 'src', 'types'),\n      pages: resolve(__dirname, 'src', 'pages'),\n      components: resolve(__dirname, 'src', 'components'),\n      hooks: resolve(__dirname, 'src', 'hooks'),\n      apis: resolve(__dirname, 'src', 'apis'),\n      utils: resolve(__dirname, 'src', 'utils'),\n      assets: resolve(__dirname, 'src', 'assets'),\n      constants: resolve(__dirname, 'src', 'constants'),\n      contexts: resolve(__dirname, 'src', 'contexts'),\n    },\n  },\n  module: { // 로더 및 규칙 설정\n    rules: [\n      {\n        test: /\\.tsx?$/i, // ts-loader를 사용했습니다. babel-loader를 사용하지 않은 이유는 IE까지 폴리필 지원을 하지 않아도 되기 때문입니ㅏㄷ.\n        exclude: /node_modules/,\n        loader: 'ts-loader',\n        options: {\n          getCustomTransformers: () => ({\n            before: isDevelopment ? [ReactRefreshTypeScript()] : [],\n          }),\n          transpileOnly: isDevelopment,\n        },\n      },\n      {\n        test: /\\.(png|jpg|jpeg|svg|woff|woff2|eot|ttf|otf)$/i,\n        type: 'asset/resource',\n      },\n    ],\n  },\n  plugins: [\n    new HtmlWebpackPlugin({ //빌드된 다음에 템플릿을 지정해줍니다.\n      template: resolve(__dirname, 'public', 'index.html'),\n    }),\n    ...(isDevelopment ? [new ReactRefreshWebpackPlugin()] : []),\n  ],\n  devServer: {\n    open: true,\n    port: 8282, // 기본은 3000번이지만 커스텀 기념으로 바꿨씁니다.\n    static: {\n      directory: resolve(__dirname, 'public'),\n    },\n    historyApiFallback: true,\n  },\n};\n```\n\n`entry`: 웹팩의 진입점을 의미. react에서는 src의 index.js에서 모든게 시작됩니다.\n\n`output`: 번들된 파일의 출력 경로를 정의합니다.\n\n`resolve`: 모듈 해석 옵션을 설정합니다.\n\n`module`: 로더의 규칙을 설정합니다.\n\n- 보통은 .css,.scss파일 확장을 위해 `style-loader`를 사용하기도 하지만, 피움에서는 CSS-in-JS를 선택했기 때문에 해당 확장자들이 존재하지 않아서 따로 설정하지 않았습니다.\n\n`plugins`:  웹팩 플러그인을 설정합니다. 플러그인은 빌드 과정을 확장하거나 수정하는데 사용됩니다.\n\n`devServer`: 개발 서버 옵션(localhost)을 설정합니다. \n\n> `ReactRefreshWebpackPlugin`은 리액트에서 저장을 파일 저장을 할 때마다 상태 초기화를 막아주는 역할을 하는 플러그인 입니다. CRA에서는 기본적으로 제공되어 있는 기능이지만, custom기능에는 존재하지 않기에 추가해 줬습니다.\n\n### tsconfig파일 설정\n\ntsconfig파일은 typescript의 버전과 모듈 등 컴파일 과정에서 어떤 버전으로 실행할 것인지 등을 설정하는 파일입니다.\n\n```json\n{\n  \"compilerOptions\": { // 컴파일러 옵션 설정\n    \"target\": \"ES2016\", // 컴파일 하는 JS 버전을 ES2016으로 설정\n    \"module\": \"CommonJS\", // 컴파일된 모듈 시스템 설정\n    \"jsx\": \"react-jsx\", // JSX문법을 어떻게 컴파일 할지 설정\n\n    \"strict\": true, // 엄격한 타입 체크\n    \"exactOptionalPropertyTypes\": true, // 선택적 프로퍼티 타입을 체크함\n    \"skipLibCheck\": true, // 라이브러리 파일의 타입 체크를 건너뜀\n    \"esModuleInterop\": true, // CJS 및 ES 모듈을 함께 사용할 때 옵션을 설정 ES6 모듈 사양을 준수하여 CJS사용 가능하게 함. (즉 import 가능)\n\n    \"baseUrl\": \"src\", // 루트가 되는 url\n    \"paths\": { // 절대 경로\n      \"types/*\": [\"types/*\"],\n      \"pages/*\": [\"pages/*\"],\n      \"components/*\": [\"components/*\"],\n      \"contexts/*\": [\"contexts/*\"],\n      \"hooks/*\": [\"hooks/*\"],\n      \"utils/*\": [\"utils/*\"],\n      \"assets/*\": [\"assets/*\"],\n      \"constants/*\": [\"constants/*\"]\n    },\n\n    \"types\": [\"cypress\"] // 타입 정의 파일 지정\n  },\n  \"include\": [\"src/**/*\", \"cypress/**/*\"] // 컴파일할 소스 코드 파일 정의\n}\n```\n\n피움에서는 IE를 제외한 모든 브라우저를 타겟으로 설정하고 있기 때문에 최대한 낮은 버전의 `JavaScript`를 타겟으로 설정했습니다. ES2016을 설정하고 처음 코드를 작성하다가 `import`가 제대로 이루어지지 않는 것을 보고 문제를 찾다가 `esModuleInterop`옵션을 true로 설정해 줘야지 CJS에서도 import가 가능하다는 것을 알게 되었습니다. \n\n또한, 상대 경로가 아닌 절대경로 사용을 선택했는데, 이는 복잡한 파일 구조를 보기 쉽게 알아보기 위해 설정했습니다.\n\n> `esModuleInterop`설정은 target으로 하는 옵션이 node16이거나 nodenext인 경우에는 true로 되어있지만, 그게 아닌 경우에는 false이므로, import 구문을 사용하고 싶다면 옵션을 true로 하는것이 좋습니다.\n\n### package.json 설정\n\n이제 설정한 웹팩과 tsconfig를 확인해 보기 위해서는 실행 스크립트를 알아야 합니다. 보통 자주 사용하는 명령어로는 `start`와 `build`가 있습니다. 그리고 나머지 명령어는 스토리북이나 cypress에 대한 명령어들입니다.\n```json\n{ //package.json\n//...\n\t\"name\": \"pium-frontend\",\n  \"version\": \"1.0.0\",\n  \"description\": \"피움 프론트엔드\",\n  \"main\": \"src/index.tsx\",\n  \"scripts\": {\n    \"test\": \"echo test is not prepared\",\n    \"start\": \"webpack-dev-server --mode=development\",\n    \"build\": \"NODE_ENV=production webpack\",\n    \"storybook\": \"storybook dev -p 6006\",\n    \"build-storybook\": \"storybook build -o ./dist/storybook\",\n    \"cypress\": \"cypress open\"\n  },\n//...\n}\n```\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 인수테스트를 작성하면서 다음과 같은 문제가 발생했다. RestAssured를 사용하고 있다. 각 테스트별로 (DB)데이터가 격리된 테스트를 수행하고 싶다. 이에 다음과 같은 시행착오를 거쳤다. 최종적으로 작성된 코드를 보고싶다면 최종코드를 확인해주세요. 시행착오 JPA를 사용하고 있는 현 시점에서…","fields":{"slug":"/acceptance-test-resolve/"},"frontmatter":{"date":"August 03, 2023","title":"인수테스트 데이터 초기화하기","tags":["인수테스트","RestDocs","테스트"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n인수테스트를 작성하면서 다음과 같은 문제가 발생했다.\n\n- RestAssured를 사용하고 있다.\n- 각 테스트별로 (DB)데이터가 격리된 테스트를 수행하고 싶다.\n\n이에 다음과 같은 시행착오를 거쳤다.\n\n> 최종적으로 작성된 코드를 보고싶다면 최종코드를 확인해주세요.\n\n## 시행착오\n\n\n```java\n@Transactional\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\nclass DictionaryPlantApiTest {\n  ...\n}\n```\n\nJPA를 사용하고 있는 현 시점에서`@Transactional`을 이용해서 하나의 테스트(트랜잭션)이 끝날 때 데이터가 롤백되는 현상을 기대했다.\n\n```java\n@Test\nvoid 사전_식물_상세정보_조회_시_사전_식물_정보_반환() {\n\n    // dictionaryPlantRepository.save() 에 대한 Fixture\n    DictionaryPlant REQUEST = dictionaryPlantSupport.builder().build();\n\n    DictionaryPlantResponse RESPONSE = RestAssured\n            .given().log().all()\n            .when()\n            .get(\"/dictionary-plants/{id}\", REQUEST.getId())\n            .then()\n            .log().all()\n            .statusCode(HttpStatus.OK.value())\n            .extract().as(DictionaryPlantResponse.class);\n    );\n    \n    ...\n}\n```\n\n![](.index_images/807f392f.png)\n\n하지만 테스트를 수행할 때 JPARepository로 save한 값을 RestAssured를 이용해 조회하면 데이터가 반영되지 않는 문제가 발생했다.\n\n### 문제 인식\n\n[Stack Overflow](https://stackoverflow.com/questions/41763417/api-test-transactional-rollback)의 다음 글을 읽어본 뒤 다음과 같은 지식을 얻었다.\n\n> @Transactional only works within the same thread, and as such the rollback provided by Spring will only work on the objects that were persisted on the thread where the transaction was started.\nYour rest-assured test will perform an integration test and will hit your Spring context on another thread. So although your rest-assured test would be able to see the objects persisted in your test setup, Spring will never be able to cleanup resources persisted in your rest-assured test automatically.\nThe only way to tackle this is to delete the entities yourself in your test (Using an Junit Rule, or cleaning up any state in your test setup).\n\n요약하면 다음과 같다.\n\n> 트랜잭션은 동일한 스레드 내에서 동작한다.\nSpring에서 제공하는 롤백은 트랜잭션이 시작된 스레드 내에서 생긴 객체에 대해서만 동작한다.\nRestAssured는 별도의 스레드를 통해 Spring 컨텍스트를 이용한다.\n이 때문에 RestAssured는 전역적으로 설정된 데이터 (Data.sql과 같은 설정으로 생긴 데이터)에는 접근할 수 있으나 Transaction과 같이 별도의 스레드에서만 제공되는 값에 대해서는 접근할 수 없다.\n... (해결방법 후략)\n\n위와 같이 별도의 RestAssured의 요청이 스레드로 동작하기 때문에 하나의 트랜잭션에서 별개로 저장한 값에 접근할 수 없다는 결론이 나왔다.\n\n> 자세한 내용은 [하마드의 트러블슈팅 글](https://pium-official.github.io/transactional-not-in-restassured/)을 참고해주세요~\n\n## 해결방법\n\n이 문제에 대해서 다양한 해결방법이 존재하고 각각의 장단점을 비교한 뒤 최종적으로 어떤 방식을 선택할지 고민해보자.\n\n### 1️⃣ DirtiesContext 사용하기\n\n매 테스트마다 Context를 새롭게 띄워서 데이터를 매번 초기화하는 방법을 생각해볼 수 있다.\n\n#### 장점\n\n컨텍스트를 다시 로드하면서 테스트간 격리 환경을 명확하게 구분할 수 있다.\n\n#### 단점\n\n- 매 테스트마다 컨텍스트를 다시 로드하기 때문에 속도가 상대적으로 느리다.\n- @Nested 내부에 정의되어있는 테스트에 대해 DirtiesContext가 적용되지 않는 문제가 있다.\n  [Spring @Nested 관련 공식 문서](https://docs.spring.io/spring-framework/reference/testing/testcontext-framework/support-classes.html#testcontext-junit-jupiter-nested-test-configuration)\n  @Nested 내부에 정의된 테스트에 DirtiesContext를 전파하기 위해 아래 설정을 추가해줘야한다.\n\n```java\n// 클래스레벨에 다음 어노테이션을 추가해주거나\n@NestedTestConfiguration(value = NestedTestConfiguration.EnclosingConfiguration.OVERRIDE)\n```\n\n혹은 properteis 설정을 변경해주는 방식도 존재한다.\n\n```properties\n# properties 파일에 다음 설정을 추가해주어 전역적으로 @Nested 기본 전략을 설정할 수 있다.\nspring.test.enclosing.configuration=OVERRIDE\n```\n> 참고 문서 : [Changing the default enclosing configuration inheritance mode](https://docs.spring.io/spring-framework/reference/testing/annotations/integration-junit-jupiter.html#integration-testing-annotations-nestedtestconfiguration)\n\n### 2️⃣ @SQL 사용하기\n\n@SQL 어노테이션을 사용하여 매 테스트 실행 전 SQL 스크립트를 실행하여 테이블을 다시 만들 수 있다.\n\n```java\n@Sql(\"classpath:init.sql\")\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\nclass DictionaryPlantApiTest {\n  ...\n}\n```\n\n#### 장점\n\n- 여러 스크립트를 수행할 수 있다.\n- 스키마 초기화 후 테이터도 함께 넣을 수 있다.\n\n#### 단점\n\n- 테스트에 사용하는 DB가 변경되었을 때 해당 벤더사의 Dialect에 맞게 SQL문을 고쳐줘야한다.\n- 쿼리를 텍스트로 관리하다보니 컴파일타임에 오류를 잡기 어렵다.\n- 테이블 연관관계가 존재하는 경우 쿼리문 작성 순서에 신경을 많이 써야한다.\n\n### 3️⃣ InitializingBean 사용하기\n\n`InitializingBean`을 이용하여 스프링 Bean이 초기화, 소멸 될 때 행위를 추가할 수 있다.\n\n```java\n// InitializingBean의 생김새\npublic interface InitializingBean {\n\tvoid afterPropertiesSet() throws Exception;\n}\n```\n\n이를 이용해 @Test 어노테이션으로 Bean이 등록될 때 마다 DB을 초기화하는 상황을 기대할 수 있다.\n\n**Entity Manager를 이용해 DB 초기화하기**\n\nJPA를 사용하는 입장으로 SQL문을 작성하기 보다는 EntityManager를 이용해보는것이 정적인 쿼리 관리에 들이는 소요가 적을 것이라고 생각된다.\n\n이에 EntityManager를 이용하여 DB를 초기화하는 방법을 알아보자.\n\n> 아래부터는 https://ttl-blog.tistory.com/1407 를 참고하여 진행된 글입니다.\n\n#### table 이름 조회하기\n\n```java\n@Component\n@Profile(\"test\")\npublic class DatabaseCleaner implements InitializingBean {\n\n\t// 테이블 이름들을 저장할 List\n    private final List<String> tables = new ArrayList<>();\n\n\t// 엔티티 매니저를 선언한다\n    @PersistenceContext\n    private EntityManager entityManager;\n\n    @SuppressWarnings(\"unchecked\")\n    @PostConstruct\n    public void findDatabaseTableNames() {\n        List<Object[]> tableInfos = entityManager.createNativeQuery(\"SHOW TABLES\").getResultList();\n        for (Object[] tableInfo : tableInfos) {\n            String tableName = (String) tableInfo[0];\n            tableNames.add(tableName);\n        }\n    }\n}\n```\n\nEntityManager를 이용하여 모든 테이블의 이름을 가져온다.\n\n#### 테이블 초기화하기 & 실행하기\n\n```java\n@Component\n@Profile(\"test\")\npublic class DatabaseCleaner implements InitializingBean {\n\n\t// 테이블 이름들을 저장할 List\n    private final List<String> tables = new ArrayList<>();\n\n\t// 엔티티 매니저를 선언한다\n    @PersistenceContext\n    private EntityManager entityManager;\n\n\t// 테이블 이름을 조회한다\n    @SuppressWarnings(\"unchecked\")\n    @PostConstruct\n    public void findDatabaseTableNames() {\n        ...\n    }\n    \n    // 테이블을 초기화한다\n    private void truncate() {\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 0)).executeUpdate();\n        for (String tableName : tableNames) {\n            entityManager.createNativeQuery(String.format(\"TRUNCATE TABLE %s\", tableName)).executeUpdate();\n        }\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 1)).executeUpdate();\n    }\n\n    @Override\n    public void afterPropertiesSet() throws Exception {\n        findDatabaseTableNames();\n    }\n\n    @Transactional\n    public void clear() {\n        entityManager.clear();\n        truncate();\n    }\n}\n```\ntruncate를 수행하여 모든 테이블의 데이터를 초기화하고 실행한다.\n\n결과적으로 봤을 때 다음과 같은 흐름으로 데이터베이스 초기화가 수행된다.\n\n- Bean이 생성된 이후 Table 이름을 찾는다.\n- 영속성 컨텍스트의 1차 캐시를 비우고 테이블을 초기화한다.\n\n#### 테스트코드에 적용하기\n\n인수테스트를 `AcceptanceTest`로 분리하고 상속받는 구조로 변경했다.\n\n```java\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class AcceptanceTest {\n\n    @LocalServerPort\n    int port;\n\n    @Autowired\n    private DatabaseCleaner databaseCleaner;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n        databaseCleaner.clear();\n    }\n}\n```\n\n작업한 내용을 적용하면 위와 같이 코드를 작성할 수 있다.\n\n#### 장점\n\n- 테스트 코드가 쿼리에 의존적이지 않다.\n- 설정을 분리하여 관리할 수 있다.\n\n#### 단점\n\n- JPA에 종속적인 설정이기 때문에 JPA를 사용하지 않거나 ORM을 제외하는 상황을 고려한다면 해당 방식의 도입을 생각해볼 필요가 있다.\n- AcceptenceTest 이외에 다른 통합테스트에서 사용하려면 중복된 코드를 작성해야한다.\n- Bean 생성 주기에 대한 설정이기 때문에 해당 클래스를 프로덕션에 남용할 수 있는 가능성이 존재한다.\n    - 테스트 코드지만 프로덕션에도 적용될 수 있는 설정이 그닥 마음에 들진 않는다.\n\n### 4️⃣ JUnit5 Extension 사용하기\n\n현재 코드는 AcceptenceTest를 사용하는 코드에 대해서만 DB 초기화를 진행하고있다.\n\nRestAssured가 별도의 스레드에서 동작하기 때문에 위와같이 구성했지만 별도의 통합 테스트에서도 DB 초기화 설정이 필요할 수 있다.\n\n@BeforeEach 내부에 clear를 선언하지 않고도 간단한 설정만으로 DB 초기화 로직을 적용하도록 개선해보자.\n\n이를 위해 JUnit5의 Extension을 사용하여 해당 설정을 리팩터링해볼 것이다.\n\n#### JUnit5 Extension이란?\n\nJunit5에서 제공하는 테스트의 생명주기에 대해 각각의 환경에 대해 확장할 수 있도록 제공하는 인터페이스다.\n\n다음과 같은 생성주기에 대한 확장을 제공한다.\n\n\n- BeforeAllCallback - @BeforeAll 실행 전에 실행된다.\n- BeforeEachCallback - @BeforeEach 실행 전에 실행된다.\n- BeforeTestExecutionCallback - 각 테스트가 실행되기 직전에 실행된다.\n- AfterTestExecutionCallback : 각 테스트가 종료된 후 실행된다.\n- AfterEachCallback : @AfterEach 실행 이후에 실행된다.\n- AfterAllCallback : @AfterAll 실행 이후에 실행된다.\n\n\n> Referenced By https://ttl-blog.tistory.com/1407\n\n#### BeforeEachCallback 이용하기\n\n현재 `@BeforeEach`에서 DatabaseCleaner가 수행되고 있으므로 `@BeforeEach` 이전에 실행될 수 있도록 `BeforeEachCallback`을 이용해보자.\n\nExtension을 이용하기 위해 기존에 DatabaseCleaner에 붙어있던 `implements InitializingBean`은 제거해줬다.\n\n```java\n@Component\npublic class DatabaseCleaner {\n\n    private final List<String> tableNames = new ArrayList<>();\n\n    @PersistenceContext\n    private EntityManager entityManager;\n\n\t// 바뀐부분 : 의존성 주입이후 초기화 수행 시 Table을 조회한다.\n    @PostConstruct\n    @SuppressWarnings(\"unchecked\")\n    private void findDatabaseTableNames() {\n        List<Object[]> tableInfos = entityManager.createNativeQuery(\"SHOW TABLES\").getResultList();\n        for (Object[] tableInfo : tableInfos) {\n            String tableName = (String) tableInfo[0];\n            tableNames.add(tableName);\n        }\n    }\n\n    private void truncate() {\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 0)).executeUpdate();\n        for (String tableName : tableNames) {\n            entityManager.createNativeQuery(String.format(\"TRUNCATE TABLE %s\", tableName)).executeUpdate();\n        }\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 1)).executeUpdate();\n    }\n\n    @Transactional\n    public void clear() {\n        entityManager.clear();\n        truncate();\n    }\n}\n```\n\n이후 `DatabaseClearExtension` 클래스를 다음과 같이 생성해준다.\n\n```java\npublic class DatabaseClearExtension implements BeforeEachCallback {\n\n    @Override\n    public void beforeEach(ExtensionContext context) throws Exception {\n        DatabaseCleaner databaseCleaner = getDataCleaner(context);\n        databaseCleaner.clear();\n    }\n\n    private DatabaseCleaner getDataCleaner(ExtensionContext extensionContext) {\n        return SpringExtension.getApplicationContext(extensionContext)\n                .getBean(DatabaseCleaner.class);\n    }\n}\n```\n\n#### 간결해진 테스트코드\n\n```java\n@ExtendWith(DatabaseClearExtension.class)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class AcceptanceTest {\n\n    @LocalServerPort\n    int port;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n    }\n}\n```\n\n이제 `@ExtendWith(DatabaseClearExtension.class)` 어노테이션만 붙여주면 테스트별로 DB를 초기화할 수 있다.\n\n각 테스트별로 truncate 문도 잘 수행되는 것을 확인할 수 있다.\n![](.index_images/086045e1.png)\n\n## 🚀 정리\n\n다양한 방법을 시도해보면서 결론적으로 BeforeEachCallback을 사용하는 방식까지 적용해봤다.\n깔끔하게 정리된 테스트를 보니 기분이 좋다. 👍\n\n### 최종 코드\n\n![](.index_images/21f5d279.png)\n\n패키지 구조는 위와 같이 구성되었고 각각의 코드는 다음과 같다.\n\n```java\n// DatabaseCleaner.java\n@Component\npublic class DatabaseCleaner {\n\n    private final List<String> tableNames = new ArrayList<>();\n\n    @PersistenceContext\n    private EntityManager entityManager;\n\n    @SuppressWarnings(\"unchecked\")\n    @PostConstruct\n    private void findDatabaseTableNames() {\n        List<Object[]> tableInfos = entityManager.createNativeQuery(\"SHOW TABLES\").getResultList();\n        for (Object[] tableInfo : tableInfos) {\n            String tableName = (String) tableInfo[0];\n            tableNames.add(tableName);\n        }\n    }\n\n    private void truncate() {\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 0)).executeUpdate();\n        for (String tableName : tableNames) {\n            entityManager.createNativeQuery(String.format(\"TRUNCATE TABLE %s\", tableName)).executeUpdate();\n        }\n        entityManager.createNativeQuery(String.format(\"SET FOREIGN_KEY_CHECKS %d\", 1)).executeUpdate();\n    }\n\n    @Transactional\n    public void clear() {\n        entityManager.clear();\n        truncate();\n    }\n}\n\n// DatabaseClearExtension.java\npublic class DatabaseClearExtension implements BeforeEachCallback {\n\n    @Override\n    public void beforeEach(ExtensionContext context) throws Exception {\n        DatabaseCleaner databaseCleaner = getDataCleaner(context);\n        databaseCleaner.clear();\n    }\n\n    private DatabaseCleaner getDataCleaner(ExtensionContext extensionContext) {\n        return SpringExtension.getApplicationContext(extensionContext)\n                .getBean(DatabaseCleaner.class);\n    }\n}\n\n// AcceptanceTest.java\n@ExtendWith(DatabaseClearExtension.class)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class AcceptanceTest {\n\n    @LocalServerPort\n    int port;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n    }\n}\n\n\n// DictionaryPlantApiTest.java\n\nclass DictionaryPlantApiTest extends AcceptanceTest {\n\t...\n}\n```\n\n## Reference\n\n- https://ttl-blog.tistory.com/1407\n- https://mangkyu.tistory.com/264\n- https://stackoverflow.com/questions/41763417/api-test-transactional-rollback\n- https://stackoverflow.com/questions/62142428/dirtiescontext-does-not-work-with-nested-tests\n- https://unluckyjung.github.io/testcode/2021/05/08/Independent-Test-Mehod/\n- https://www.baeldung.com/junit-5-extensions"},{"excerpt":"이 글은 우테코 피움팀 크루 '하마드'가 작성했습니다. 개요 우아한테크코스 5기 피움 서비스의 인수 테스트를 작성하던 도중, 실제 포트에 애플리케이션을 구동시켜 테스트를 하는 \n환경에서 테스트를 할 때, 테스트 메소드에서 분명히 해당 리소스를 저장했는데도 RestAssured의 GET 요청이 수행되지 않는 경우가 있었다. 이 이유를 파악하고 해결 과정을 …","fields":{"slug":"/transactional-not-in-restassured/"},"frontmatter":{"date":"July 31, 2023","title":"[트러블슈팅] @SpringbootTest의 RANDOM_PORT 환경에서 @Transactional 어노테이션을 사용했을 때, RestAssured GET 요청이 수행되지 않는 경우(트랜잭션 격리 이해하기)","tags":["Transaction","SpringbootTest"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[하마드](https://github.com/rawfishthelgh)'가 작성했습니다.\n\n## 개요\n우아한테크코스 5기 피움 서비스의 인수 테스트를 작성하던 도중, 실제 포트에 애플리케이션을 구동시켜 테스트를 하는 `@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)`\n환경에서 테스트를 할 때, 테스트 메소드에서 분명히 해당 리소스를 저장했는데도 RestAssured의 GET 요청이 수행되지 않는 경우가 있었다. 이 이유를 파악하고 해결 과정을 정리한다\n## 상황\n\n![img.png](.index_images/img.png)\n\n당시 다음과 같은 인수 테스트 환경에서 테스트를 수행하고 있었다. 각 테스트 메소드가 끝난 후 트랜잭션을 롤백시켜 격리를 보장하기 위해 클래스 레벨에 `@Transactional` 어노테이션을 선언했었다.\n\n![img_1.png](.index_images/img_1.png)\n\nAcceptanceTest 클래스를 상속받아 사전 식물의 조회를 테스트하는 인수 테스트를 작성했다.\n37번째 줄의 `DictionaryPlant REQUEST = dictionaryPlantSupport.builder().build();` 는\n![img_2.png](.index_images/img_2.png)\n다음과 같은 형태로 repository에 테스트용 엔티티 객체를 save하는 과정을 간편하게 하기 위해 작성된 DictionarySupport 클래스와 코드이다. 즉 37번 라인을 사용하면 임의의 객체 정보를 dB에 저장하는 insert 작업이 일어난다.\n\n즉, 현재 테스트 메소드는 repository에 사전 식물 객체를 저장하고, 이를 RestAssured를 활용하여 실제 서버에서 요청을 보내 저장된 사전 식물이 조회되는지를 확인하는 과정이다.\n\n![img_3.png](.index_images/img_3.png)\n\n그런데, 200 OK를 기대했던 테스트의 결과가 404 not found가 응답되고 사전 식물이 존재하지 않는다는 예외 메세지가 뜨는 것을 확인하게 되었다.\n\n뭘까? insert 쿼리가 날아가지 않은 걸까? 로그를 확인해 보았다.\n![img_4.png](.index_images/img_4.png)\n그러나, 로그에는 정확히 우리가 DictionarySupport에서 지정한 저장 메소드의 Insert 쿼리가 날아가는 것을 보고 있었다. 심지어 value 값도 모두 정확히 들어갔다.\n![img_5.png](.index_images/img_5.png)\nselect 쿼리가 잘못된게 아닌가? 확인해봤지만 예상한대로 나갔다.\n여기까지 봐서는 이해가 안 간다. insert를 하고 select를 했는데 조회가 안 되니 말이다.\n## RandomPort 환경에서의 스레드와 트랜잭션\n이를 이해하기 위해서는 RandomPort 환경에서의 요청이 어떤 스레드를 갖는지를 알아야 한다. 이 내용은 스프링 공식 문서를 들여다볼 필요가 있다.\n> If your test is @Transactional, it rolls back the transaction at the end of each test method by default. However, as using this arrangement with either RANDOM_PORT or DEFINED_PORT implicitly provides a real servlet environment, the HTTP client and server run in separate threads and, thus, in separate transactions. Any transaction initiated on the server does not roll back in this case.\n\n> 테스트에 @Transactional이 붙어 있으면 롤백을 보장할 수 있다. 단, RANDOM_PORT 나 DEFINED_PORT 환경에서 테스트를 수행하면, 실제 서블릿 환경에서 테스트를 진행한다. 이 때 http 클라이언트(테스트)와 서버는 서로 다른 스레드를 갖는다. 즉 별개의 트랜잭션이 수행되는 것이다. 따라서 서버 쪽에서의 트랜잭션은 롤백되지 않는다.\n\n쉽게 설명하자면, 랜덤 포트 환경에서 테스트를 수행하는 순간, localhost 포트 어딘가에 내장 서버를 띄운다. 그리고 그 서버에서 요청을 보내도록 하는 것이 RestAssured 이다. 따라서 RestAssured에서 보내는 요청은, 내가 해당 테스트 메소드에서 실행하는 스레드와 별개의 스레드를 갖는다. 따라서 트랜잭션도 각자 별개로 갖는다.\n\n자, 아까 우리가 AcceptanceTest의 클래스 레벨에서 `@Transactional`을 선언했음을 기억할 것이다. 클래스 레벨에 이 어노테이션을 선언하면, 자동적으로 해당 클래스와 그를 상속한 메소드 레벨에 모두 트랜잭션이 걸린다.\n![img_6.png](.index_images/img_6.png)\n따라서, 현재 테스트 메소드의 트랜잭션 범위는 다음 화살표와 같이 테스트가 끝날 때 까지 유지된다.\n\n우리는 여기서 트랜잭션의 개념에 대해 다시 짚을 필요가 있다.\n\"트랜잭션이 끝나지 않았다\"는 의미는 무엇인가?\n내가 수행한 insert, update, delete 문이 커밋, 혹은 롤백되지 않았다는 뜻이다.\n\n커밋되지 않았다는 뜻은 무엇인가?\n데이터베이스의 갱신이 이뤄졌어도, 다른 스레드에서 확인할 수 없다는 뜻이다.\n왜? 트랜잭션은 각자 격리되어 서로의 트랜잭션이 끝나기 전 까지는 연산 과정을 볼 수 없기 때문이다.(트랜잭션의 Isolation(격리성) 특징을 기억하자)\n\n따라서, 현재 트랜잭션이 끝나기도 전에, 즉 반영되기도 전에 RestAssured를 통해 띄운 별개의 서버 스레드에서 GET 요청이 들어가니 해당 데이터를 확인할 수 없는 것이다.\n정확히 말하면 트랜잭션이 커밋되지 않았을 때 까지의 데이터는, 다른 스레드 입장에서 그냥 없는 데이터다. 없는걸 조회하니 당연히 값이 안 나온다.\n\n![img_7.png](.index_images/img_7.png)\n그래서 `@Transactional` 어노테이션을 제거하고 테스트를 수행했더니, 다음과 같이 통과하는 것을 볼 수 있다.\n![img_8.png](.index_images/img_8.png)\n이해를 쉽게 하기 위해 바뀐 트랜잭션의 범위를 화살표로 표현하면, DictionaryPlant를 저장하는 과정에서 트랜잭션이 끝나고 커밋이 되어 db에 영속화가 완료된다.\n따라서 이후 랜덤 포트의 스레드에서도 해당 데이터를 조회할 수 있다.\n\n\n## 트랜잭션의 격리 이해하기\n\n![img_9.png](.index_images/img_9.png)\n![img_10.png](.index_images/img_10.png)\n해당 동작에 대해 조금 더 설명하기 위해 별개의 스레드에 h2 database 콘솔을 띄워 확인해 보겠다. 위의 두 콘솔 화면은 세션 id가 다른 별개의 스레드를 갖는다. 편의상 위를 스레드A, 아래를 B라 하겠다\n\n![img_11.png](.index_images/img_11.png)\n스레드 A에서 트랜잭션을 시작하기위해 Autocommit을 False로 변경한 후, 두 명의 member 데이터를 insert 했다.\n![img_12.png](.index_images/img_12.png)\n이 때 A에서 select 문을 실행하면 데이터가 들어가 있는 것을 확인할 수 있다.\n![img_13.png](.index_images/img_13.png)\n그러나, B에서 동일한 select문을 실행하면 데이터를 확인할 수 없다.\n왜냐? 스레드 A의 트랜잭션은 끝나지 않았기 때문에 A에서 보이는 데이터는 \"임시 반영\"된 데이터일 뿐, \"영속화\"되지 않았기 때문이다.\n![img_14.png](.index_images/img_14.png)\n만약 위와 같이 스레드 A에서 커밋을 하고 나면\n![img_15.png](.index_images/img_15.png)\n이제는 스레드 B에서도 동일한 데이터를 확인할 수 있다.\n## 결론\n결국 메소드 레벨에 `@Transactional` 어노테이션이 붙은 탓에 save 메소드를 성공적으로 수행해도, 트랜잭션이 끝나지 않아(정확히 말하면 커밋되지 않아) 랜덤 포트 서버의 스레드에서 확인할 수 없는 것이다.\n따라서 랜덤 포트 환경의 격리성을 보장하고자 할 때는 `@Transactional` 어노테이션을 사용하기 보다는, truncate를 하는 sql 스크립트를 사용하는 방법을 고려하는게 더 적절하다고 생각한다.\n\n\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노', '그레이', '조이', '하마드'가 작성했습니다. 서론 현재 피움팀 서버는  요청으로 이뤄져있다.\n는 WWW 상에서 정보를 주고 받는 프로토콜인데, 누군가 네트워크에서 신호를 가로채면 내용이 노출되는 문제가 발생할 수 있다.\n이를 해결해주는 프로토콜이 바로 이다. 는 정보를 암호화하는 SSL을 이용한 프로토콜이다. …","fields":{"slug":"/apply-https/"},"frontmatter":{"date":"July 28, 2023","title":"내 서버에 HTTPS 설정하기","tags":["HTTPS"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)', '[그레이](https://github.com/kim0914)', '[조이](https://github.com/yeonkkk)', '[하마드](https://github.com/rawfishthelgh)'가 작성했습니다.\n\n\n## 서론\n\n현재 피움팀 서버는 `HTTP` 요청으로 이뤄져있다.\n`HTTP`는 WWW 상에서 정보를 주고 받는 프로토콜인데, 누군가 네트워크에서 신호를 가로채면 내용이 노출되는 문제가 발생할 수 있다.\n이를 해결해주는 프로토콜이 바로 `HTTPS`이다.\n\n`HTTPS`는 정보를 암호화하는 SSL을 이용한 프로토콜이다. \n현재는 TLS 방식도 사용되는데 여기에서 **핵심은 보안 문제를 해결하기 위해 암호화**를 한다는 것이다.\n\n대칭키와 공개키, 인증서와 같은 방식을 사용해 HTTPS를 적용할 수 있다.\n이번 글에서는 `Certbots`을 이용해 `HTTPS`를 nginx에 쉽게 설치하고 적용하는 방법을 다뤄본다.\n\nHTTP, HTTPS에 대한 자세한 내용은 별도의 글에서 다룰 예정이다.\n\n`Let's Encrypt`에서는 인증서를 무료로 발급해주고 있고 `Certbots`과 함께 사용하기를 권장하기 때문에 `Certbots`을 사용하기로 결정했다.  \n\n이 내용은 아래 이미지에서 확인할 수 있다.  \n![img.png](img.png)\n\n\n\n## Certbot 설치\n\n서버에 HTTPS 설정을 해보자\n[Certbot 공식문서](https://certbot.eff.org/)\n\n> 공식문서의 가이드를 따라 작성된 문서입니다.\n> https://certbot.eff.org/instructions\n\n![](.index_images/a8ec6c59.png)\n\n```shell\n# certbot을 설치하기 위한 snap을 설치한다.\nsudo apt update\nsudo apt install snapd\n\n# 이미 설치되어있는 certbot을 제거한다.\nsudo apt-get remove certbot\n\n# certbot을 설치한다.\nsudo snap install --classic certbot\n\n# certbot이 잘 설치되어있는지 확인한다.\nsudo ln -s /snap/bin/certbot /usr/bin/certbot\n\n# certbot을 nginx에 연결하기\nsudo certbot --nginx\n```\n## HTTPS 설정\n\n![](.index_images/5498d43a.png)\n\n```shell\n# certbot이 SSL 인증서를 자동 갱신하는 cron을 등록한다\nsudo certbot renew --dry-run\n```\n\n## NGINX 설정 확인\n\nNGINX 설정에 다음과 같이 certbot이 HTTPS 설정을 추가한 것을 확인할 수 있다.\n\n![](.index_images/3d08df23.png)\n\n## Reference\n\n- https://certbot.eff.org/instructions\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '클린'가 작성했습니다. 사건의 발단 이번 Level3 2차 데모데이때 권장사항을 맞추기 위해서 프로덕션 배포까지 진행했습니다. 어찌 저찌 배포는 잘 끝이 났는데 문제는 첫 페이지가 로드 되는데 약 4초 가량의 시간이 걸린다는 것이었습니다. 네트워크 문제 문제인가? 하고 봤는데 아니었고, 결국  파일의 크기를 보니 무려 16m…","fields":{"slug":"/bundle-analyze/"},"frontmatter":{"date":"July 28, 2023","title":"bundle-analyze를 통한 bundle 크기 분석","tags":["bundle","최적화"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[클린](https://github.com/hozzijeong)'가 작성했습니다.\n\n## 사건의 발단\n\n이번 Level3 2차 데모데이때 권장사항을 맞추기 위해서 프로덕션 배포까지 진행했습니다. 어찌 저찌 배포는 잘 끝이 났는데 문제는 첫 페이지가 로드 되는데 약 4초 가량의 시간이 걸린다는 것이었습니다. 네트워크 문제 문제인가? 하고 봤는데 아니었고, 결국 `bundle.js` 파일의 크기를 보니 무려 16m나 되었던 것입니다.\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/72ec258f-77e0-41aa-8bcd-c4c3d3b0410e)\n\n왜 이렇게 크기가 크지? 라는 생각을 하며 크게 2가지의 경우를 생찾아봤습니다. 하지만 다음과 같은 이유로 원인이 아님을 알았습니다.\n\n1. minify가 되지 않았다 → webpack 4 이상을 사용하면서 자동으로 `production` 일때는 자동으로 설정\n2. css 파일 크기가 너무 큼 → `styled-component`를 사용하기 때문에 .css 가 없음.\n\n따라서 `bundle.js`의 성능을 확인해 볼 필요가 있었고 검색하던 도중 [webpack-bundle-analyzer](https://www.npmjs.com/package/webpack-bundle-analyzer)라는 라이브러리를 알게되었습니다.\n\n## Webpack Bundle Analyzer\n\nwebpack을 통해 build한 bundle의 성능을 측정하는 라이브러리로 모든 번들에 대한 트리를 시각화 해서 보여주는 이점을 갖고 있습니다. 사용법은 매우 간단합니다. 우선 설치를 해줍니다.\n\n```jsx\n# NPM\nnpm install --save-dev webpack-bundle-analyzer\n# Yarn\nyarn add -D webpack-bundle-analyzer\n```\n\n설치를 한 뒤 `webpack.config.js` 파일 내부에서 플러그인을 사용해 줍니다.\n\n```jsx\nconst BundleAnalyzerPlugin = require('webpack-bundle-analyzer').BundleAnalyzerPlugin;\n\nmodule.exports = {\n  plugins: [\n    new BundleAnalyzerPlugin()\n  ]\n}\n```\n\n플러그인 안에 옵션을 설정할 수 있는데, [여기](https://github.com/webpack-contrib/webpack-bundle-analyzer#options-for-plugin)에서 옵션에 대한 정보를 확인할 수 있습니다.\n\n위에 플러그인 옵션에서 `generateStatsFile` 을 `true` 로 설정한 다음에 `package.json`에 `scripts` 에다음 명령어를 통해 실행할 수 있습니다.\n\n```json\n\"scripts\":{\n  ...\n  \"preanalyze\": \"npm run build-dev\",\n  \"analyze\": \"webpack-bundle-analyzer bundle/output/path/stats.json\"\n}\n```\n\n![image](https://github.com/pium-official/pium-official.github.io/assets/50974359/6bc524c4-4866-4ebe-9daa-95035e0c0dd5)\n\n실행한 뒤에 bundle을 분석해 봤더니 react-icons에서 15.3m나 차지하고 있었습니다. 드디어 문제의 원흉을 찾아냈습니다. 그렇다면 react-icons를 대체할 수 있는 방법이 있을까요?\n\n### Icons\n\n[여기](https://icones.js.org/collection/all?s=material-symbols:cancel)에 가시면 다양한 아이콘들을 찾아볼 수 있습니다. 원하는 아이콘을 선택한 다음에 SVG, Component 등으로 코드를 반환해 줍니다. 이를 통해서 react-icons를 제거하고 icons를 사용해서 아이콘을 대체할 수 있었습니다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노', '그레이', '조이', '하마드'가 작성했습니다. 서론 본 설명 글은 다음과 같은 환경에서 진행됩니다. 배포서버 : Ubuntu 22.04.2 LTS DB서버 : Ubuntu 22.04.2 LTS 배포서버와 DB서버는 동일한 VPC를 사용하고 있습니다. DB : MySQL 8.0.33 Application : Spr…","fields":{"slug":"/db-drop-stop/"},"frontmatter":{"date":"July 28, 2023","title":"Table Drop 못하게 막아버리기","tags":["DB","MySQL"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)', '[그레이](https://github.com/kim0914)', '[조이](https://github.com/yeonkkk)', '[하마드](https://github.com/rawfishthelgh)'가 작성했습니다.\n\n\n## 서론\n\n본 설명 글은 다음과 같은 환경에서 진행됩니다.\n> - 배포서버 : Ubuntu 22.04.2 LTS\n> - DB서버 : Ubuntu 22.04.2 LTS\n> - 배포서버와 DB서버는 동일한 VPC를 사용하고 있습니다.\n> - DB : MySQL 8.0.33\n> - Application : SpringBoot 3.1.1\n\n개발 과정에서 실수로 Application 단에서 DB 혹은 테이블을 Drop 해버리는 상황이 있을 수 있다.\n\n테이블의 수정이 제한된 유저를 생성하여 해당 유저를 Application에서 사용하도록 구성해보자. \n\n> 도움을 주신 G선생님께 감사의 인사 올리며 시작하겠습니다.\n> \n> ![](.index_images/c34a6c3a.png)\n\n## 유저 생성하기\n\n> 이후 진행할 작업에서 권한 부여를 위해 root 권한이 필요합니다.\n\n우선 root 권한을 가진 계정으로 MySQL에 로그인 한다.\n\n```shell\n# MySQL에 로그인\nsudo mysql -u root -p\n```\n\nApplication에서 사용할 유저를 생성한다.\n\n```sql\nCREATE USER 'pium'@'localhost' IDENTIFIED BY 'password';\n```\n\n> 만약 외부에서 접근해야하는 유저의 경우 다음 명령어를 참고하세요\n> \n> ```sql\n> # 외부 모든 IP에 대한 pium 이라는 유저 생성 \n> CREATE USER 'pium'@'%' IDENTIFIED BY 'password';\n> \n> # 특정 인스턴스에서만 접속가능한 pium 이라는 유저 생성 \n> CREATE USER 'pium'@'접속할 인스턴스의 private ip' IDENTIFIED BY 'password';\n> ```\n\n## 권한 부여하기\n\nMySQL에서 설정할 수 있는 권한은 아래와 같다.\n\n| 권한                | 내용                     |\n|-------------------|------------------------|\n| CREATE, ALTER, DROP | 테이블 생성, 변경, 삭제  |\n| SELECT, INSERT, UPDATE, DELETE | 테이블의 레코드 조회, 입력, 수정, 삭제 |\n| RELOAD| 권한 부여된 내용을 리로드 |\n| SHUTDOWN| 서버 종료 작업 실행 |\n| ALL| 모든 권한 허용 |\n| USAGE| 권한 없이 계정만 생성 |\n\n우리는 `INSERT`, `UPDATE`, `SELECT`, `DELETE`만 사용하면 되기 떄문에, 새로 생성한 유저에게 해당 권한만 부여한다.\n\n```sql\nGRANT SELECT, INSERT, UPDATE, DELETE ON pium_db.* TO 'pium'@'localhost';\n```\n\n설정한 권한을 `flush` 명령어를 이용해 적용한다.\n\n```sql\nFLUSH PRIVILEGES;\n```\n\n## 확인하기\n\n![](.index_images/bd8e631c.png)\n\n실제로 생성된 유저로 접속하여 Drop Table 명령어를 수행했을 때 권한 부족으로 수행되지 않음을 확인할 수 있다.\n\n## Application properties 적용\n\nApplication의 설정파일에 생성한 유저의 정보를 작성한다.\n\n![](.index_images/54161808.png)\n\n이를 통해 Applicaiton에서는 Table의 생성, 수정, 삭제에 대한 조작을 할 수 없게 되었다.\n\n만약 테이블의 생성, 수정, 삭제에 대한 작업이 필요할 경우 DBA 혹은 DB 담당자에게 요청하는 프로세스를 거쳐야만 한다.\n\n## Reference\n\n- Chat GTP 4\n- 피움 백엔드 팀의 집단지성\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 현재 피움팀의 젠킨스에서 빌드 트리거로 을 사용하고있다.  해당 설정은 Github에서 유발되는 모든 WebHook에 대해 빌드 트리거가 동작하는 설정이다.  빌드 스크립트에서 위처럼 triggers 설정을 별도로 지정할 수 있지만 현재 프로젝트 구조상 더 세분화된 설정이 필요한 상태다.  현재 …","fields":{"slug":"/jenkins-hook-by-label/"},"frontmatter":{"date":"July 25, 2023","title":"PR 라벨로 젠킨스 빌드유발을 구분하기","tags":["젠킨스","CI"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n현재 피움팀의 젠킨스에서 빌드 트리거로 `GitHub hook trigger for GITScm polling`을 사용하고있다.\n\n![](.index_images/bc288fb6.png)\n\n해당 설정은 Github에서 유발되는 모든 WebHook에 대해 빌드 트리거가 동작하는 설정이다.\n\n![](.index_images/c0a90061.png)\n\n빌드 스크립트에서 위처럼 triggers 설정을 별도로 지정할 수 있지만 현재 프로젝트 구조상 더 세분화된 설정이 필요한 상태다.\n\n![](.index_images/b47bf110.png)\n\n현재 레포지토리 하나에 프론트, 백엔드 코드가 같이 관리되고있다.\nPR이 Merge 되었을 때 각 트랙별로 코드가 빌드 및 배포될 수 있도록 구성해야한다.\n\n## 어떻게 할까?\n\n현재 하나의 레포지토리에서 두개의 코드가 관리되고 있기 떄문에 단순히 develop 브랜치의 변경감지로는 어느 트랙의 작업내용인지 구분하기가 어렵다.\n\n![](.index_images/88fd7187.png)\n\n현재 피움은 라벨을 이용하여 트랙별 작업구분을 하고 있다.\n\n이를 이용하여 라벨별로 빌드, 배포를 진행할 수 있도록 구성해보자.\n\n## Generic Webhook Trigger 설치\n\n![](.index_images/82e40c68.png)\n\nGeneric Webhook Trigger는 젠킨스에서 제공하는 플러그인으로 HTTP 요청을 수신하여 JSON, XML 형태의 데이터를 추출하여 트리거를 지정할 수 있는 플러그인이다.\n\nGeneric Webhook Trigger를 사용하여 빌드 유발 상황을 구분해보자.\n\n![](.index_images/4ff8e063.png)\n\n플러그인을 설치했다면 파이프라인의 Build Triggers 설정에서 다음과 같은 탭을 확인 할 수 있다.\n\n![](.index_images/7b51b24a.png)\n\n해당 설정을 클릭해보면 다음과 같은 설명을 확인할 수 있다.\n\n`http://JENKINS_URL/generic-webhook-trigger/invoke` 로 오는 요청에 대해 트리거가 수행된다는 말이 적혀져있다.\n\n위 설명에 맞춰 GitHub에서 WebHook 설정을 변경해주자.\n\n> 위에서 설명한 탭에 대해서는 아직 저장하지않고 깃허브로 넘어와서 진행합니다.\n>\n> 젠킨스 설정은 아래에서 설명하는 `Github WebHook 설정`이후 이어서 진행합니다.\n\n## Github WebHook 설정\n\n![](.index_images/eebe67d7.png)\n\n`프로젝트 레포지토리` -> `Settings` -> `Webhooks 탭`에 들어와서 웹훅을 새로 만들어보자.\n\n![](.index_images/19691acb.png)\n\nPayload URL을 위에서 확인한 `Generic Webhook Trigger`의 설정에 맞게 작성한다.\n\n그리고 트리거가 동작하는 이벤트에 대해서 `Send me everything` 옵션을 체크한다.\n\n![](.index_images/5e2c402b.png)\n\nAdd webhook을 한 뒤 다시 등록한 웹 훅을 클릭하여 `Recent Deliveries` 탭을 확인해본다.\n\n![](.index_images/bb813e21.png)\n\n내용을 보면 요청을 보낼 때 Payload에 다양한 정보들이 담겨있는 것을 확인할 수 있다.\n\n### (이해하기) JSONPath 살펴보기\n\n> 해당 과정은 원리를 이해하기 위한 과정으로 수행하지 않아도 무관합니다.\n\n![](.index_images/4ff6a8c1.png)\n![](.index_images/f6916224.png)\n\nPR과 연관된 요청이 발생했을 때는 Payload에 pull_request에 대한 정보가 추가된다.\n\n![](.index_images/c301431e.png)\n\nhttps://jsonpath.com 에 접속하여 해당 Payload를 붙여넣어 확인해본다.\n\nJSONPath로 표현했을 때 `$.pull_request.merged`를 확인하면 해당 요청이 PR이 merge된 요청인지 확인할 수 있다.\n\n![](.index_images/0d6d400d.png)\n\n`$.pull_request.base.ref`는 해당 PR의 base 브랜치를 의미한다.\n\n예를들어 feature -> develop 형태라면 develop이 나온다\n\n![](.index_images/47afaad7.png)\n\n`$.pull_request.labels..name`는 해당 PR의 라벨들의 이름을 의미한다.\n\n## Generic Webhook Trigger 설정\n\n> ![](.index_images/5b94c679.png)\n파이프라인 설정 구성에 들어와서 작업을 이어나갑니다.\n\n![](.index_images/f9141614.png)\n\n기존에 설정해둔 `GitHub hook trigger for GITScm polling` 설정을 해제한다.\n\n![](.index_images/40676eae.png)\n\n그리고 Generic WebHook Trigger 를 활성화한다.\n\n### Post content parameters 설정\n\n`Post content parameters`의 추가 버튼을 눌러 파라미터를 추가한다.\n\n![](.index_images/f66fbb52.png)\n\n`$.pull_request.merged`를 등록하여 Merge 되었는지 여부를 가져온다.\n\n![](.index_images/2f09a4a8.png)\n\n`$.pull_request.base.ref`를 등록하여 base 브랜치를 가져온다.\n\n원하는 값은 develop 브랜치가 될 것이다.\n\n![](.index_images/d7a4b4fc.png)\n\n`$.pull_request.labels..name`를 등록하여 라벨들의 이름을 가져온다.\n\n원하는 값은 등록된 라벨들 중에 `백엔드`를 포함하는 값이 존재하는 상황이 될 것이다.\n\n### Optional filter 설정\n\n아래로 내리다보면 Optional filter 탭을 볼 수 있다.\n\n![](.index_images/3f3d0f71.png)\n\nExpression에 다음과 같은 정규표현식을 작성했다.\n\n`(?=.*true)(?=.*develop)(?=.*백엔드).*`\n\n또한 Text 부분에는 위에서 파라미터로 선언한 값들을 사용했다.\n\n위 표현식으로 다음과 같은 효과를 기대할 수 있다.\n\n`merge가 true 이고`, `브랜치가 develop 이고`, `라벨에 백엔드가 존재`하는 작업에 대해 트리거를 수행하는것을 기대할 수 있다.\n\n### Token 설정\n\n![](.index_images/b55101b4.png)\n\nToken 탭에 토큰 이름을 작성한다.\n\n![](.index_images/8e4baf8e.png)\n\n이 때 GitHub WebHook 설정에 들어가서 Payload URL에 token 파라미터를 포함하도록 수정한다.\n\n![](.index_images/675ca490.png)\n\n바로 아래 탭에서 TokenCredential에 토큰에 대한 Secret Key를 추가해줘야한다.\n\n외부에서 빌드를 유발하기 위해서는 token을 확인하는 방식이 존재하는데 이 때 Jenkins의 Credential에 있는 정보를 기반으로 수행한다.\n\n> token을 사용하지 않는 경우 계정정보를 입력할 수도 있지만 해당 과정에서는 token을 등록하는 방식을 사용합니다.\n\n![](.index_images/639919be.png)\n\n위와 같은 정보로 Secret text를 작성하고 등록하고 적용한다.\n\n## 응답 확인하기\n\n![](.index_images/8f20f94d.png)\n\nGitHub의 WebHook 탭의 Recent Deliveries 탭으로 가서 확인해보면 Response Body값에 다음과 같은 응답이 담겨있는 것을 확인할 수 있다.\n\n```json\n{\n  \"jobs\": {\n    \"pium-dev\": {\n      \"regexpFilterExpression\": \"(?=.*true)(?=.*develop)(?=.*백엔드).*\",\n      \"triggered\": true,\n      \"resolvedVariables\": {\n        \"BRANCH\": \"develop\",\n        \"LABELS\": \"[\\\"?️ \\\\b리팩터링\\\",\\\"? 백엔드\\\"]\",\n        \"LABELS_0\": \"?️ \\b리팩터링\",\n        \"LABELS_1\": \"? 백엔드\",\n        \"MERGED\": \"true\"\n      },\n      \"regexpFilterText\": \"true develop [\\\"?️ \\\\b리팩터링\\\",\\\"? 백엔드\\\"]\",\n      \"id\": 248,\n      \"url\": \"queue/item/248/\"\n    }\n  },\n  \"message\": \"Triggered jobs.\"\n}\n```\n\n`regexpFilterText`에 원하는 값이 잘 들어간 것을 확인할 수 있다.\n\n## 결론\n\n젠킨스의 `Generic Webhook Trigger` 플러그인을 이용하여 Merge 여부, target branch, label 값을 비교하고 빌드를 유발하는 설정을 했다.\n\n경우에 따라 세부적인 검증을 추가할 수 있을 것 같다.\n\n### 트러블 슈팅\n\nGeneric Webhook Trigger 설정 - Token 설정에서 많이 헤맸다.\n\n![](.index_images/1c588600.png)\n\n`{\"jobs\":null,\"message\":\"Did not find any jobs with GenericTrigger configured! If you are using a token, you need to pass it like ...trigger/invoke?token=TOKENHERE. If you are not using a token, you need to authenticate like http://user:passsword@example.org/generic-webhook... \"}`\n\n404 에러와 함께 위 메시지가 반환되었는데 Jenkins의 Token에 대해 이해를 하지 못해 생긴 문제였다.\n\n> 왜 다들 Token이 뭔지는 설명을 안해주는거지..?? ㅠㅠ\n>\n> `외부에서 빌드를 유발하기 위해서는 token을 확인하는데 이 때 Jenkins의 Credential에 있는 정보를 기반으로 수행한다.`\n>\n> 이 한마디를 이해하기 위해 하루가 걸렸다... 😇\n\n## Reference\n\n### 설정 과정에서 참고한 글\n\n- https://bepoz-study-diary.tistory.com/385\n- https://velog.io/@zayson/Jenkins-CICD-4.-WebHook%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EB%8F%99-%EB%B0%B0%ED%8F%AC\n\n\n### 트러블 슈팅 과정에서 참고한 글\n\n- https://georgik.rocks/jenkins-generic-webhook-plugin-failed-with-did-not-find-any-jobs-with-generictrigger-configured/\n- https://pikachu987.tistory.com/61"},{"excerpt":"이 글은 우테코 피움팀 크루 '그레이'가 작성했습니다. 이번 피움 서비스에서 CI/CD를 적용하기 위해 Jekins와 Github Webhook을 이용했습니다. 본 글에서는 Jenkins와 Github Webhook을 이용한 SpringBoot 서버 자동 빌드, 자동 배포 과정을 다루겠습니다. Jenkins 설치 과정은 피움 팀 젠킨스 설치하기 를 참고하…","fields":{"slug":"/ci-cd-setting/"},"frontmatter":{"date":"July 24, 2023","title":"Jenkins와 Github Webhook을 이용해 CI/CD 구축하기","tags":["CI/CD","젠킨스","Webhook","설정"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[그레이](https://github.com/Kim0914)'가 작성했습니다.\n\n이번 피움 서비스에서 CI/CD를 적용하기 위해 Jekins와 Github Webhook을 이용했습니다.\n\n\n\n본 글에서는 Jenkins와 Github Webhook을 이용한 SpringBoot 서버 자동 빌드, 자동 배포 과정을 다루겠습니다.\n\n\n\nJenkins 설치 과정은 [피움 팀 젠킨스 설치하기](https://pium-official.github.io/jenkins-setting/) 를 참고하시면 됩니다 !\n\n\n\n작업 환경\n\n- 인스턴스: AWS EC2 t4g.small\n- OS: Ubuntu 22.04.2 LTS\n- RAM: 2GB\n\n\n## Jenkins 접속\n젠킨스를 접속하는 방법은 간단합니다. 젠킨스를 설치한 인스턴스 public IP와 port를 주소창에 입력하면 쉽게 접근할 수 있습니다.\n\n\n\n정상적으로 접속하면 다음과 같은 화면을 만나게 됩니다.\n![](.index_images/img.png)\n\n---\n## Github Webhook 설정\n먼저 Webhook을 설정하기 위해 현재 프로젝트의 깃허브 레포지토리로 이동합니다.\n\n\n\n레포지토리의 Settings를 눌러서 접속할 수 있습니다. 이후 왼쪽 메뉴바에 Webhooks를 누르면 아래와 같은 화면을 볼 수 있습니다.\n![](.index_images/img_1.png)\n\n오른쪽 상단의 Add Webhook 버튼을 눌러 webhook을 추가하는 화면으로 이동합니다.\n\n\n\n그러면 아래와 같은 Payload URL, Content Type, Secret을 입력하는 폼이 나타납니다.\n![](.index_images/img_2.png)\n\n**Payload URL**은 젠킨스가 설치되어 있는 서버의 `도메인 주소:포트/github-webhook/` 을 입력합니다.\n\n젠킨스의 기본 포트는 8080으로 설정되어 있습니다.\n\n주소 마지막에 / 는 꼭 넣어주셔야 합니다.\n\n/를 넣지 않으면 redirect URL로 인식해 정상적으로 동작하지 않습니다.\n\n\n\n**Content-type**은 `application/json`을 선택합니다.\n\n\n\nSecret은 비워두면 됩니다.\n\n\n\n마지막으로 어떤 동작에서 webhook을 발생시킬지 선택하면 됩니다.\n\n저희 팀의 경우에는 **Let me select individual events**를 눌러 **push**와 **pull request**가 발생했을 때 webhook이 동작하도록 선택했습니다.\n\n\n\n모든 선택을 마친 후 **Add webhook 버튼을 누르면 토큰이 발행**됩니다.\n\n\n\n토큰을 생성할 때 토큰 권한도 함께 설정할 수 있는데, 이 때 repo와 admin:repo_hook을 선택해주시면 됩니다.\n\n\n\n만약 CI를 통해 PR에 comment를 남기는 등의 write가 필요없는 경우에는 wirte 권한을 빼줄 수 있을 것 같습니다.\n\n\n\n해당 토큰은 생성 후 재발행되지 않기 때문에 따로 보관을 하는 것이 좋습니다.\n\n---\n## Credentials 생성\n다시 젠킨스 화면으로 이동해서 Credentials를 등록하여 Github webhook을 연동 시켜야 합니다.\n\n\n\n젠킨스 메인 화면에서 Jenkins 관리 페이지로 접속하면 Security 탭에 Credentials를 눌러 접속합니다.\n![](.index_images/img_3.png)\n\nAdd Credentials를 누릅니다.\n![](.index_images/img_4.png)\n\n그러면 아래와 같은 화면을 볼 수 있습니다.\n\n\n\n깃허브 webhook에서 발급받은 토큰을 이용해 크리덴셜 값을 생성합니다.\n![](.index_images/img_5.png)\n\n**kind**는 `Username with password`를 선택합니다.\n\n\n\n**username**에는 깃허브 토큰을 발급받은 `깃허브 아이디`를 작성합니다.\n\n\n\n**password**에는 `해당 토큰 값`을 넣으면 됩니다.\n\n\n\n**ID**는 젠킨스에서 이 **크리덴셜을 식별하기 위한 이름이므로 크게 의미가 없는 값** 입니다.\n\n---\n### Gradle 설정\n현재 우리 팀은 스프링 부트 서버를 빌드하기 때문에 gradle이 반드시 필요합니다.\n\n\n\n젠킨스 메인 화면에서 대시보드 -> 젠킨스 관리 -> Tools를 누르면 아래와 같은 Gradle 설정 화면을 볼 수 있습니다.\n![](.index_images/img_6.png)\n\nGradle 8.2.1 버전을 추가했습니다.\n\n추가를 완료하고 바로 페이지를 나가면 안됩니다.\n\n가장 아래에 있는 파란색 save 버튼을 눌러야 해당 설정이 저장됩니다 !!\n\n\n\nGradle 설정까지 마치면 빌드할 준비를 모두 마쳤습니다.\n\n\n\n이제 CI를 자동으로 실행하기 위해서는 CI 스크립트를 작성해야 합니다.\n\n---\n## CI 스크립트 작성\n드디어 CI를 담당하는 젠킨스 아이템을 생성할 시간입니다 !\n\n\n\n대시보드에서 New Item을 누르면 아래와 같은 화면을 볼 수 있습니다.\n![](.index_images/img_7.png)\n\n상단에 아이템 이름을 설정하고 Pipeline을 구축할 것이기 때문에 Pipeline을 선택하고 OK를 누릅니다.\n\n\n\n이후 아래와 같이 아이템을 설정할 수 있는 화면이 나옵니다.\n![](.index_images/img_8.png)\n\n저희는 깃허브 프로젝트를 사용하고 있기 때문에 Github Project를 체크한 후 레포지토리의 url을 입력합니다.\n\n\n\n다음으로 Build Trigger를 선택해야하는데, Github webhook을 사용할 계획이므로 Github hook triger for GITScm polling을 선택합니다. 해당 버튼을 클릭하면 아래에 스크립트를 작성하는 칸이 나옵니다.\n![](.index_images/img_9.png)\n\n\n\nScript칸에 팀에서 필요로 하는 CI Script를 입력하면 됩니다.\n\n\n\n여기서 저장 버튼 위에 보이는 Pipeline Syntax를 누르면 깃허브를 이용해 접근할 수 있는 스크립트를 생성하는 화면이 나옵니다.\n![](.index_images/img_10.png)\n\nSample Step을 Git으로 설정한 후 아래에 값을 입력하면 됩니다.\n\n\n\nCredentials에는 좀 전에 생성했던 크리덴셜을 선택하면 됩니다. 해당 정보에 토큰 값이 있기 때문에 정확한 크리덴셜을 선택해야 합니다.\n\n크리덴셜은 생성할 때 입력한 ID와 동일하게 보여집니다.\n\n\n\n모두 입력했다면 Generate Pipeline Script를 눌러 완성된 스크립트를 확인합니다.\n\n\n\n해당 스크립트를 만드는 이유는 빌드 과정에서 Github 저장소에서 저장소를 복제해야하기 때문입니다.\n\n그러므로 저장소에 대한 접근 권한이 반드시 필요합니다.\n\n\n\n전체 빌드 스크립트는 아래와 같습니다.\n\n```shell\npipeline {\n    agent any\n    tools {\n        gradle 'gradle'\n    }\n    triggers {\n        githubPush()\n    }\n    stages {\n        stage('저장소 복제') {\n            steps {\n                // 여기서 방금 생성된 깃허브 스크립트 넣기\n        }\n        \n        stage('빌드') {\n            steps {\n                sh '''\n                cd backend/pium\n                ./gradlew clean build\n                '''\n            }\n            post {\n                failure {\n                    slackSend (\n                        channel: '#알림-젠킨스',\n                        color: '#FF0000',\n                        message: \"백엔드 Build 실패...😢\"\n                    )\n                }\n            }\n        }\n    }\n}\n```\n\n\nstages에서 깃허브 저장소에 있는 프로젝트를 가져온 후, 본인의 프로젝트 디렉토리로 이동해 build를 진행하도록 하면 됩니다.\n\n디렉토리를 옮기는 부분에서 몇 번 에러가 있었는데, Jenkins 대시보드의 Console Output을 이용해 디버깅하면서 수정해나가면 쉽게 설정할 수 있습니다.\n\n\n\n지금까지 올바르게 적용되었다면 CI 설정은 모두 정상적으로 마쳤습니다.\n\n\n\n다음으로 CD(자동 배포) 설정 방법을 알아보겠습니다.\n\n---\n## SSH Agent 설치\n현재 빌드 서버와 운영 서버가 서로 다른 인스턴스로 분리되어 있기 때문에, 젠킨스 빌드 서버에서 빌드한 파일을 운영서버로 전달해야\n\n합니다. 이때 다른 인스턴스로의 접속이 필요하므로 SSH 연결 설정을 해야 합니다.\n\n\n\n기존의 블로그나 여러 레퍼런스를 찾아보면 Publish Over SSH를 많이 사용하는데, 현재 접속 불가 이슈가 있습니다.\n\n그러므로 SSH Agent를 사용해야 합니다.\n\n\n\nJenkins 관리의 Plugin 탭을 들어가서 설치할 수 있습니다.\n\n\n\nJenkins도 하나의 서버에서 동작하고 있기 때문에 운영서버로 접속하기 위한 pem 키가 필요합니다.\n\n\n\n대시보드 -> 젠킨스 관리 -> Credentials 로 이동합니다.\n\n\n\n깃허브 크리덴셜을 생성하는 것과 동일한 방법으로 pem 키를 크리덴셜로 설정합니다.\n![](.index_images/img_11.png)\n\n**kind**는 `SSH Username with private key`를 선택합니다.\n\n\n\n**id**는 깃허브 크리덴셜과 마찬가지로 식별자이기 때문에 `원하는 값을` 입력하면 됩니다.\n\n\n\n**username**은 인스턴스의 HostName ex) ubuntu@123-345-678-23의 `ubuntu` 를 입력하면 됩니다.\n\n\n\n마지막으로 **인스턴스를 생성하면서 받았던 pem 키를 Add 버튼을 눌러 추가**합니다.\n\npem 키에 있는 모든 내용(Begin, End 포함)을 다 복사해서 넣어야합니다.\n\n---\n## CD 스크립트 작성\n이제 젠킨스가 설치된 서버에서 빌드된 결과물을 운영 서버로 전달하는 스크립트를 작성해야 합니다.\n\n이때 scp 라는 프로토콜을 사용하여 파일을 전달합니다.\n\n\n\n저희 팀이 사용한 스크립트는 아래와 같습니다.\n\n빌드 성공 유뮤, 배포 성공 유무를 슬랙과 연동해 사용하고 있습니다 !\n\n```shell\npipeline {\n    agent any\n    tools {\n        gradle 'gradle'\n    }\n    triggers {\n        githubPush()\n    }\n    stages {\n        stage('저장소 복제') {\n            steps {\n                // 깃허브 크리덴셜\n            }\n        }\n        \n        stage('빌드') {\n            steps {\n                sh '''\n                cd //프로젝트 위치\n                ./gradlew clean build\n                '''\n            }\n            post {\n                failure {\n                    slackSend (\n                        channel: '#알림-젠킨스',\n                        color: '#FF0000',\n                        message: \"백엔드 Build 실패...😢\"\n                    )\n                }\n            }\n        }\n        stage('배포') {\n            steps {\n                sshagent(credentials: ['pem 키로 생성한 크리덴셜 ID']) {\n                    sh '''\n                        cd //프로젝트 위치\n                        \n                        ssh -o StrictHostKeyChecking=no {HostName}@{Private IP} uptime\n                        cd build/libs\n                        \n                        scp {빌드된 Jar 파일명} {HostName}@{Private IP}:/home/ubuntu\n                        ssh -t {HostName}@{Private IP} {운영서버 배포 스크립트 파일}\n                    '''\n                }\n            }\n            post {\n                success {\n                    slackSend (\n                        channel: '#알림-젠킨스',\n                        color: '#00FF00',\n                        message: \"백엔드 배포 성공! 🚀\"\n                    )\n                }\n                failure {\n                    slackSend (\n                        channel: '#알림-젠킨스',\n                        color: '#FF0000',\n                        message: \"백엔드 배포 실패 🥲\"\n                    )\n                }\n            }\n        }\n    }\n}\n```\n\n\nCI/CD 설정을 모두 마친 후 깃허브 프로젝트에 push, pull request와 같은 trigger가 발생하면 아래와 같이 빌드 결과를 볼 수 있습니다.\n![](.index_images/img_12.png)\n\n### Reference\n- [베베의 CI/CD 글](https://developer-nyong.tistory.com/47#article-7-1--ssh-%ED%94%8C%EB%9F%AC%EA%B7%B8%EC%9D%B8-%EC%84%A4%EC%B9%98)\n"},{"excerpt":"이 글은 우아한테크코스 5기, 피움팀 크루 '참새'가 작성했습니다. 서론  피움의 프론트엔드에서는 서버 상태 관리를 위해 Tanstack Query를 사용하기로 하였다. 이 글에서는 많고 많은 상태 관리 라이브러리 중 Tanstack Query를 사용하기로 결정한 이유를 소개하고자 한다. 설치한 버전: 5.0.0-alpha.86 Tanstack query…","fields":{"slug":"/fe-tanstack-query/"},"frontmatter":{"date":"July 23, 2023","title":"프론트엔드: Tanstack Query 사용에 대하여","tags":["Tanstack Query","React Query","리액트 쿼리"]},"rawMarkdownBody":"\n> 이 글은 우아한테크코스 5기, 피움팀 크루 '[참새](https://github.com/WaiNaat)'가 작성했습니다.\n \n\n## 서론\n\n![Tanstack: High-quality open-source software for web developers. Headless, type-safe, and powerful utilities for state management, routing, data visualization, charts, tables, and more.](.index_images/tanstack.png)\n\n피움의 프론트엔드에서는 서버 상태 관리를 위해 [Tanstack Query](https://tanstack.com/query/v5/docs/react/overview)를 사용하기로 하였다.\n\n이 글에서는 많고 많은 상태 관리 라이브러리 중 Tanstack Query를 사용하기로 결정한 이유를 소개하고자 한다.\n\n> 설치한 버전: 5.0.0-alpha.86\n\n## Tanstack query로 얻을 수 있는 개발적인 이점\n\n### 코드 간소화와 컴포넌트의 관심사 분리\n\nTanstack query에서 제공하는 다양한 함수들을 활용하면 비동기 통신의 처리 코드를 선언형으로 만들 수 있다. 또한 Tanstack query가 서버와의 통신을 대신해주기 때문에 컴포넌트의 렌더링 함수는 UI/UX의 구현이라는 보다 근본적인 내용에 집중할 수 있다. 조금 더 풀어서 설명하면 네 가지로 나눌 수 있다.\n\n우선 `<Suspense>`와 `<ErrorBoundary>`의 편한 사용이 가능하다. 이 둘을 사용하기 위해서는 각각 promise 또는 error를 throw하는 과정이 필요하다. 특히 error가 아닌 것을 던진다는 생각은 기존의 틀을 깨는 것으로 (eslint에 [관련 설정](https://eslint.org/docs/latest/rules/no-throw-literal)이 있다는 점으로 미루어 볼 때 자주 사용하는 기법이 아니었을 것이다) 새로운 코드를 짜야 한다. [Suspense를 설명하는 리액트 공식 문서](https://react.dev/reference/react/Suspense#displaying-a-fallback-while-content-is-loading)의 첫 번째 예제에서 Fork를 눌러 `Albums.js`안의 `use()`함수를 보면 promise를 던지기 위해 굉장히 재미있는 기법을 사용하고 있다는 걸 확인할 수 있다. 이러한 일련의 과정을 tanstack query가 대신하기 때문에 편리한 사용이 가능하다.\n\n다음으로 낙관적 업데이트를 적용하기에 유리하다. `useMutation`의 `onSuccess`, `onError`, `onSettled`와 같은 설정을 이용하면 된다.\n\n세 번째로는 비동기 통신의 결과 타입을 자동으로 추론해준다는 점이다. 따라서 컴포넌트 내부에서 불필요한 타입 가드 및 추론, 강제 타입 지정과 같은 행동을 하지 않아도 되므로 타입스크립트를 더 깔끔하게 사용할 수 있다.\n\n마지막으로 서버 상태 처리를 위한 전역 상태 관리 라이브러리를 도입하지 않아도 된다. 물론 이건 비단 tanstack query만의 장점은 아니다. 하지만 캐싱을 이용해서 전역 상태를 이용하지 않고도 여러 컴포넌트에서 같은 쿼리를 요청했을 경우 추가적인 통신 없이도 정보를 전달받을 수 있다는 점은 매력적이다.\n\n### 비동기 통신\n\n1. 캐싱을 지원하기 때문에 서버와의 불필요한 통신을 줄일 수 있다.\n2. 다양한 상황에서 자동 refetch가 가능해서 원한다면 항상 최신 값을 유지할 수 있다.\n3. 여러 컴포넌트가 같은 쿼리를 사용할 때, 해당 쿼리로부터 반환되는 데이터가 바뀌었다면 그 컴포넌트들을 한 번에 업데이트할 수 있다.\n4. 쿼리가 오랫동안 사용되지 않으면 가비지 컬렉터를 자동으로 불러서 정리한다.\n\n## 다른 라이브러리와의 비교\n\n우리 팀에서는 Recoil 이외의 다른 전역 상태 라이브러리나 서버 상태 라이브러리를 사용해본 경험이 많은 크루가 없어서 간단하게만 비교하였다.\n\n### [SWR](https://github.com/vercel/swr)\n\nNext.js 팀에서 만든 서버 상태 관리 라이브러리. 데이터 캐싱이라는 측면에 집중했다. 따라서 다른 라이브러리와 비교했을 때 굉장히 가벼운 편에 속한다.\n\n### [Tanstack Query](https://tanstack.com/query/v5/docs/react)\n\n데이터 캐싱뿐만이 아니라 에러 처리나 무한 스크롤과 같은 다양한 기능을 지원한다. 비슷한 기능을 하는 리액트 쿼리의 [`useQuery`](https://tanstack.com/query/v5/docs/react/reference/useQuery)와 SWR의 [`useSWR`](https://swr.vercel.app/docs/data-fetching)의 반환값을 확인해보았을 때 받을 수 있는 정보의 양이 차이가 많이 난다. 다양한 상황을 고려한 UI를 보여준다면 Tanstack query를 활용하는 것이 좋겠다는 생각을 했다. 추가적으로 받아온 데이터의 일차 가공이 필요하다면 `useQuery`에서 `selector`를 사용할 수 있다는 점도 상대적인 장점이라고 생각한다.\n\n### Redux, Recoil 등 다른 전역 상태 관리 라이브러리\n\n현재 피움 서비스 내에서 전역적으로 관리할 값이 많지 않다고 판단하였다.\n\n또한 일반적인 전역 상태 라이브러리의 경우 서버 상태를 나타나기에는 부적절한 것 같다. 특히 Recoil은 selector의 캐시 문제로 인해 사용자의 장바구니 목록처럼 자주 변하는 값을 받아오기 굉장히 힘들었던 기억이 있다.\n\n## Tanstack Query로 사용자에게 어떤 서비스를 제공하고자 하는지\n\n우리가 추구해야 하는 것은 기술적 고점이 아니다. 그렇기 때문에 이 라이브러리를 도입해서 사용자에게 어떤 경험을 선사할 수 있을지 고민하였다.\n\n1. Suspense\n    - Suspense는 사용자에게 '지금 무슨 일을 하는 중이다'라는 암시를 준다. 따라서 사용자의 기다리는 시간을 편하게 할 수 있다.\n    - 피움 서비스의 '식물 사전' 기능은 공공데이터를 활용한다. 현재 사용하고 있는 공공데이터의 경우 이용에 제약이 없어서 DB에 저장할 수 있다. 만약 그렇지 않아서 항상 백엔드 서버에서 공공데이터 API와의 통신을 추가적으로 진행해야 할 경우 정보를 받아오는 데 시간이 걸린다. 이 때 Suspense를 잘 활용할 수 있을 것이다.\n2. Refetch\n    - 피움 서비스는 모바일 환경을 우선적으로 고려하고 있다.\n    - 지하철 와이파이처럼 인터넷이 불안정한 곳에서는 요청이 잘 가지 않을 수 있다. 이 때 사용자가 같은 행동을 두 번 하지 않더라도 tanstack query가 자동적으로 요청을 보내서 UX 측면의 개선이 가능하다.\n3. caching\n    - '식물 사전 정보'는 변하지 않는 자료이다. 따라서 사용자가 여러 번 요청을 하더라도 매번 같은 정보를 보게 된다. 이러한 요청에 대해서는 `staletime`을 길게 잡아 캐시를 오래 유지한다면 불필요한 서버와의 통신을 줄이고 사용자 경험도 개선할 수 있다.\n    - '내가 키우는 식물 목록'이나 '내가 키우는 식물의 상세 정보'도 사용자가 값을 추가하거나 바꾸지 않는 이상 변하지 않는 값이다. 이 친구들도 적절한 캐싱을 사용하면 서버와의 통신을 줄여 사용자의 대기시간을 줄일 수 있다.\n4. 무한 스크롤\n    - 피움에서는 자신의 식물에게 물을 준 날짜들을 기억해서 타임라인 형태로 볼 수 있도록 제공할 계획이다. 타임라인은 시간의 흐름을 나타내기 위한 도구이다. '시간'과 '흐름' 모두 연속성을 띠는 표현인 만큼 무한 스크롤을 이용해 유의미한 경험을 제공할 것이다.\n\n\n### Tanstack Query v5를 사용하게 된 이유\n\n마지막으로 피움 프론트엔드에서 Tanstack query의 다섯 번째 메이저 버전을 사용하기로 한 이유를 짧게 남긴다. Tanstack query v5는 [획기적인 변화](https://tanstack.com/query/v5/docs/react/guides/migrating-to-v5)가 있고, alpha 단계이다.\n\n1. 어차피 버전은 5로 흘러간다. 피움 프론트엔드의 세 크루 모두 Tanstack query를 처음 배우는 단계라면 굳이 역사의 뒤안길로 흘러갈 v4부터 배울 필요는 없다고 생각한다.\n2. 문서를 살펴본 결과 v5의 '획기적인 변화'는 대부분 기존의 불필요하거나 문제를 일으키기 쉬운 기능을 제거하는 것이었다. 따라서 만약 v4를 학습해야 한다면 기존의 v5 공부 내용에서 덧붙이는 방법으로 가능할 것이다.\n3. 현재 피움 서비스에서는 복잡한 서버와의 데이터 통신은 없을 것 같다. 따라서 alpha 단계이기 때문에 무언가가 안 되는 상황은 없을 것으로 예상된다.\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론  http://pium.life 로 접속하면 피움의 홈페이지가 나오기까지의 과정을 기록으로 남겨보려고 한다. 본 과정은 서버 구축을 처음 하면서 과정을 이해하기위해 작성된 글입니다. 빠르게 따라할 수 있는 설정 기반 글을 원한다면 마지막 목차인 정리를 확인해보시길 바랍니다. 본 과정에서는 소량의 …","fields":{"slug":"/pium-deploy-step/"},"frontmatter":{"date":"July 18, 2023","title":"피움의 배포과정","tags":["배포","인프라"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n \n\n## 서론\n\n![](.index_images/3162b98e.png)\n\nhttp://pium.life 로 접속하면 피움의 홈페이지가 나오기까지의 과정을 기록으로 남겨보려고 한다.\n\n> 본 과정은 서버 구축을 처음 하면서 과정을 이해하기위해 작성된 글입니다.\n> \n> 빠르게 따라할 수 있는 설정 기반 글을 원한다면 마지막 목차인 정리를 확인해보시길 바랍니다.\n\n> 본 과정에서는 소량의 과금(?)이 존재합니다.\n> (도메인 구입비용 : 약 4000원)\n\n## 도메인 정하기\n\n![](.index_images/8e211be2.png)\n\n> 도메인을 구입하기 위해 도메인 호스팅 사이트 [가비아](https://www.gabia.com/)를 이용했다.\n\n`pium.com` `pium.co.kr` `pium.net` 등등.\n서비스의 이름으로 자주 사용하는 도메인을 생성하고싶었으나 이미 존재하거나 가격이 너무 비싸다는 문제가 있었다.\n\n적당한 가격에 서비스의 이름을 가진 도메인인 `pium.life`를 선택했다. (1년 4000원)\n\n## 도메인 설정하기\n\n> 가비아 서비스를 기준으로 과정이 진행됩니다.\n\n`3.123.123.123`과 같은 인스턴스 public ip 주소를 `pium.life`로 별칭을 지정해주자.\n\n![](.index_images/ce5d100b.png)\n\n우측 상단의 `My 가비아` 탭으로 들어간다.\n\n![](.index_images/48a0d2d6.png)\n\n우측의 `DNS 관리툴` 탭으로 들어간다.\n\n![](.index_images/dbaa42b0.png)\n\n도메인의 `설정` 탭으로 들어간다.\n\n![](.index_images/0b4d4be0.png)\n\nDNS 관리 탭의 `설정` 혹은 `레코드 수정`에 들어간다.\n\n![](.index_images/d830ba27.png)\n\n위와 같이 레코드를 추가하고 저장한다.\n\n> CNAME 설정은 향후 서브도메인 설정을 위한 설정입니다.\n\n이제 http://pium.life 로 접속하면 서버로 바로 접속할 수 있게된다. (SSH 접속이 아닌 HTTP 접속을 의미한다.)\n\n## 환경 살펴보기\n\n피움은 하나의 EC2에 프론트엔드, 백엔드의 배포를 모두 진행하려고한다.\n\n이를 위해서라면 서로 다른 포트에 서비스를 띄워둔 뒤 각 요청별로 포트를 분산시킬 필요가 있다.\n\n![](.index_images/ee357fa5.png)\n\n때문에 위와 같은 흐름으로 요청이 진행되는 것이 최종 목표다!\n\n### EC2 보안규칙\n\n우아한테크코스 과정에서 제공되는 EC2의 보안규칙은 다음과 같다.\n\n- 사내에서만 22번 포트 접속 가능\n- 80, 443은 열려있음\n- 이외의 포트는 열려있지 않음.\n\n우리가 흔히 개발을 하면서 사용하는 3000번 포트 혹은 8080번 포트를 통해 해당 서버에 접근하지 못한다는 이야기다.\n\n즉, 다시말해 pium.life:8080과 같은 접근이 안된다는 이야기다.\n\n인바운드 규칙에서 8080 혹은 3000번 포트를 열어버리는 방법도 있겠지만 보안 규칙에 손대지 않으면서 해결할 수 있는 방법을 찾아보자.\n\n## nginx로 포트포워딩하기\n\nlinux에 존재하는 `iptables` 명령어를 통해 서버 자체적으로 포트포워딩 환경을 구성할 수도 있지만 현재 피움은 하나의 서버에서 두개의 어플리케이션을 배포하고 있고, 도메인도 하나다.\n\n따라서 후에 설명할 서브도메인을 적용하기 위해서라도 nginx를 이용하기로 한다.\n\n### nginx 설치하기\n\n```shell\nsudo apt install nginx\n```\n\n```shell\nsystemctl status nginx\n```\n\n![](.index_images/28e0139a.png)\n\n이제 주소창에 인스턴스 ip 를 치고 접속하면 다음과 같이 nginx default 페이지를 볼 수 있다. \n\n![](.index_images/9345764b.png)\n\n### 80 포트 접속을 8080 포트로\n\n> 해당 과정은 8080포트에 애플리케이션이 구동중인 상태에서 진행됩니다. \n> 다음 명령어로 8080포트에서 애플리케이션이 구동중인지 확인한다.\n>\n> ```shell\n> lsof -i tcp:8080\n> ```\n> ![](.index_images/1573063d.png)\n\n이제 nginx 설정파일을 작성하여 80포트로 오는 요청을 8080포트로 전환시켜본다.\n\n```shell\ncd /etc/nginx/sites-available/\n\nsudo vi default\n```\n\n> 주석처리 되어있는 부분은 제외하고 코드로 표현합니다.\n\n```shell\n# 변경 전 파일내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n}\n```\n\n처음 파일을 열어보면 위와같이 설정되어있을 것이다.\n\n최대한 간략하게 포트를 변경한다는 목적만 달성해보기 위해 다음 내용들을 수정해보자.\n\n- `root /var/www/html;` 주석처리\n- `index index.html index.htm index.nginx-debian.html;` 주석처리\n- `location / { ... }` 내부 내용에 다음 내용 추가\n  - `proxy_set_header Host $host:$server_port;`\n  - `proxy_set_header X-Real-IP $remote_addr;`\n  - `proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;`\n  - `proxy_pass http://127.0.0.1:8080;`\n\n```shell\n# 변경된 파일내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        # root /var/www/html;\n\n        # index index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                proxy_set_header Host $host:$server_port;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_pass http://127.0.0.1:8080;\n        }\n}\n```\n\n위와같이 설정파일을 수정했다면 다음 명령어를 수행해 nginx를 다시 시작한다.\n\n```shell\nsudo systemctl restart nginx\n```\n\n![](.index_images/232c6d5d.png)\n\n스프링부트 애플리케이션을 실행중이였다면 위와같은 Whitelabel Error Page를 확인할 수 있을것이다.\n\n## 서브도메인 구성하기\n\n이제 80포트에서 8080포트로 요청을 변환하는 과정은 성공했다.\n\n하나의 서버에 프론트엔드, 백엔드 코드가 같이 띄워져야하므로 서브도메인을 통해 페이지와 API를 구분해줘야한다.\n\n### index 파일 지정하기 - default 파일 수정\n\n프론트에서 빌드 산출물로 나온 index.html을 기본 페이지로 참고하도록 설정하자.\n\n```shell\ncd /etc/nginx/sites-available/\n\nsudo vi default\n```\n\n```shell\n# 변경 전 설정내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        # root /var/www/html;\n\n        # index index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                proxy_set_header Host $host:$server_port;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_pass http://127.0.0.1:8080;\n        }\n}\n```\n\n기존에 80포트를 8080포트로 포워딩시켜주는 설정을 바꿔준다.\n\n```shell\n# 변경 후 설정내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html;\n\n        server_name pium.life;\n\n        location / {\n        }\n}\n```\n\n이제 pium.life로 접속하면 `/var/www/html` 경로에 있는 `index.html`을 참조하도록 설정되었다.\n\n위 설정에 따르면 `index.html` 및 `bundle.js`, `assets` 등의 프론트 리소스들은 서버의 `/var/www/html`를 기준으로 위치시키면 된다.\n\n> 빌드 산출물 및 CD와 관련된 내용은 본 게시글에서 다루지 않습니다.\n\n### API 서버 지정하기 - api_config 파일 만들기\n\nhttp://api.pium.life 로 API를 호출하도록 구성해보자.\n\nnginx는 기본적으로 `/etc/nginx/sites-enabled` 경로에 있는 파일들을 참고하여 설정파일에 적용한다.\n\n> `/etc/nginx/nginx.conf` 파일의 60번째 줄 참고\n> \n> ![](.index_images/f4ceb33e.png)\n\n다음 과정을 통해 서브도메인 설정을 해보자.\n\n```shell\ncd /etc/nginx/sites-available\n\nsudo vi api_config\n```\n\n```shell\n# api_config 파일 내용\nserver {\n        listen 80;\n        listen [::]:80;\n\n        server_name api.pium.life;\n        \n        location / {\n                proxy_set_header Host $host:$server_port;\n                proxy_set_header X-Real-IP $remote_addr;\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n                proxy_pass http://127.0.0.1:8080;\n        }\n}\n```\n\n`server_name` 옵션을 통해 api.pium.life 라는 도메인으로 접근하는 요청을 확인하고, 맞다면 8080 포트로 변경한다.\n\n위와 같이 파일을 작성했으면 `sites-enabled` 경로에 soft link(바로가기)를 생성한다.\n\n```shell\nln -s /etc/nginx/sites-available/api_config /etc/nginx/sites-enabled/api_config\n```\n\n이제 nginx를 재시작하여 설정을 적용해보자.\n\n> 💡 Tip : `nginx -t` 명령어를 통해 현재 설정파일이 정상적으로 작성되었는지 확인할 수있다.\n> \n> ![](.index_images/10d7627b.png)\n\n![](.index_images/04601361.png)\n\nhttp://api.pium.life 로 접속하면 API로 연결되는것을 확인할 수 있다.\n\n## 정리\n\n- 8080 포트에 API 애플리케이션을 실행한다.\n- 도메인 구입 및 A 레코드, CNAME을 설정한다.\n- nginx를 설치한다.\n- 프론트 빌드 산출물을 `/var/www/html` 경로에 준비한다. (index.html, bundle.js, assets 등)\n- 서브도메인 및 포트포워딩 설정한다.\n\n### nginx 설정 명령어 따라가기\n\n```shell\n# nginx 설치\nsudo apt install nginx\n\n# default 파일 수정\nsudo vi /etc/nginx/sites-available/default\n```\n\n```shell\n# /etc/nginx/sites-available/default 파일 내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html;\n\n        server_name pium.life;\n\n        location / {\n        }\n}\n```\n\n```shell\n# api_config 파일 작성\nsudo vi /etc/nginx/sites-available/api_config\n```\n\n```shell\n# api_config 파일 내용\nserver {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html;\n\n        server_name pium.life;\n\n        location / {\n        }\n}\n```\n\n```shell\n# nginx 재시작 및 설정적용\nsudo systemctl restart nginx\n```\n\n## Reference\n\n- https://jminie.tistory.com/110\n- https://customer.gabia.com/manual/domain/287/1201"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 피움팀에서 Jenkins를 도입하고 적용하는 과정을 거쳤다. 정확한 동작과정을 몰랐기 때문에 헤매는 상황이 빈번하게 있었다.\n이에 Jenkins를 설치하고 환경설정을 빌드서버 환경을 구축한 일련의 과정을 기록해두려고한다. 작업 환경 : Ubuntu 22.04.2 LTS 공식문서 살펴보기 젠킨스 공…","fields":{"slug":"/jenkins-setting/"},"frontmatter":{"date":"July 16, 2023","title":"젠킨스 설치하기","tags":["CI/CD","젠킨스","설정"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n \n\n## 서론\n\n피움팀에서 Jenkins를 도입하고 적용하는 과정을 거쳤다.\n\n정확한 동작과정을 몰랐기 때문에 헤매는 상황이 빈번하게 있었다.\n이에 Jenkins를 설치하고 환경설정을 빌드서버 환경을 구축한 일련의 과정을 기록해두려고한다.\n\n> 작업 환경 : Ubuntu 22.04.2 LTS\n\n## 공식문서 살펴보기\n\n[젠킨스 공식문서](https://www.jenkins.io/doc/book/installing/linux/)를 확인하면 설치 방법이 친절하게 나와있다.\n\n아무것도 없는 EC2 환경에서 Jenkins를 설치하고 구동하는 과정을 따라가보자.\n\n## (Optional) Swap 메모리 설정하기\n\n프리티어환경에서 Jenkins를 사용하면 자칫 메모리가 부족한 상황이 발생할 수도 있다.\n2GB의 메모리는 젠킨스와 빌드를 모두 견디기 어려울 수도 있다.\n\n다음 명령어를 통해 서버에 Swap 메모리를 할당하자.\n\n```shell\nsudo fallocate -l 2G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\nfree -h # 확인명령어\n```\n\n> [Swap 메모리 할당하기](https://velog.io/@junho5336/Swap-%EB%A9%94%EB%AA%A8%EB%A6%AC-%ED%95%A0%EB%8B%B9%ED%95%98%EA%B8%B0)를 참고\n\n## java 설치하기\n\n> 설치 방법은 [공식문서](https://www.jenkins.io/doc/book/installing/linux/)를 확인하면서 진행하고있습니다.\n\n현재 사용할 jenkins의 버전은 java 17을 사용한다.\njava 17버전을 설치하자\n\n```shell\nsudo apt update\nsudo apt install openjdk-17-jre\njava -version\n```\n\n![](.index_images/93d56503.png)\n\n## jenkins 설치하기\n\n다음 명령어를 수행하여 jenkins를 설치한다.\n\n```shell\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\\n  /usr/share/keyrings/jenkins-keyring.asc > /dev/null\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list > /dev/null\nsudo apt-get update\nsudo apt-get install jenkins\n\nsudo systemctl status jenkins # 실행상태 확인\n```\n\n![](.index_images/99b45540.png)\n\n\n## jenkins 접속하기 & 초기설정\n\n> jenkins는 기본적으로 8080포트를 사용한다.\n> jenkins의 포트를 변경하는 방법에 대해서는 본 글에서 다루지는 않는다.\n\n`{인스턴스-public-ip}:8080` 으로 접속하면 다음과 같은 화면을 볼 수 있다.\n\n![](.index_images/4492665d.png)\n\n다시 EC2 화면으로 넘어와서 다음 명령어를 수행하여 `initialAdminPassword`를 알 수 있다.\n\n```shell\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n```\n\n![](.index_images/f55b94c9.png)\n\n확인한 암호를 화면에 입력한 뒤 Continue 버튼을 눌러 계속 진행한다.\n\n![](.index_images/28fea7e9.png)\n\n초기 플러그인 세팅 화면이 나오는데 `Install suggested plugins` 를 눌러 초기 플러그인 설정을 진행한다. \n\n![](.index_images/c78a9517.png)\n\n![](.index_images/82bae04f.png)\n\n초기 Admin 유저를 생성할 수 있다.\n\n![](.index_images/a8f14e43.png)\n\njenkins의 기본 URL을 설정할 수 있다. 기본값을 권장한다. \n\n![](.index_images/84efb2c8.png)\n\n여기까지 했으면 초기 설정은 완료했다고 볼 수 있다.\n\n만약 위 과정에서 admin 유저를 생성하지 않았다면 `admin` 이라는 기본 유저로 로그인 할 수 있다고 설명하고 있다.\n비밀번호는 `initialAdminPassword` 과정에서 입력했던 값으로 설정되어있다.\n\n![](.index_images/60ba97ad.png)\n\n이렇게 Jenkins를 설치할 수 있다.\n\n## Reference\n\n- https://www.jenkins.io/doc/book/installing/linux/\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '그레이'가 작성했습니다. 서론 JPA를 이용해 엔티티를 설계하다가  어노테이션과 @Column의 의 차이점이 궁금해져 알아보았습니다. nullable = false ?? JPA는 엔티티를 매핑하고 설계하면 자동으로 DDL을 생성해 주는 기능을 제공합니다. \n또한 엔티티의 각 컬럼들에 대해 NOT NULL, UNIQUE 등과 …","fields":{"slug":"/jpa-notnull-nullable/"},"frontmatter":{"date":"July 16, 2023","title":"@NotNull과 nullable = false는 어떤 차이가 있을까?","tags":["JPA","Validation","nullable"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[그레이](https://github.com/Kim0914)'가 작성했습니다.\n\n\n## 서론\n\nJPA를 이용해 엔티티를 설계하다가 `@NotNull` 어노테이션과 @Column의 `nullable=false`의 차이점이 궁금해져 알아보았습니다.\n\n\n## nullable = false ??\n\nJPA는 엔티티를 매핑하고 설계하면 자동으로 DDL을 생성해 주는 기능을 제공합니다. \n또한 엔티티의 각 컬럼들에 대해 NOT NULL, UNIQUE 등과 같은 제약 조건이 포함되는 경우에도 자동으로 DDL을 생성해 줍니다.\n\n\n\n엔티티를 설계할 때 일반적으로 @Column 어노테이션을 활용합니다. \n@Column 어노테이션에서 해당 컬럼의 NOT NULL 제약을 설정할 수 있는데 `@Column(name = \"email\", nullable = false)`와 같이 nullable=false 설정을 걸어주면, 해당 제약 조건을 명시할 수 있습니다. \n따로 명시하지 않으면 nullable의 기본 값은 true입니다.\n\n\n\n간단한 User 엔티티를 생성해 보겠습니다.\n\n```java\n@Entity\n@Table(name = \"user\")\npublic class User extends AuditingEntity {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    @Column(name = \"id\", updatable = false)\n    private Long id;\n\n    @Column(name = \"user_id\", length = 20)\n    private String userId;\n\n    @Column(name = \"password\", length = 20)\n    private String password;\n\n    @Column(name = \"name\", length = 20)\n    private String name;\n\n    @Column(name = \"email\", nullable = false, length = 50)\n    private String email;\n}\n```\n\nUser 테이블의 email 필드는 반드시 값이 존재해야 합니다. \n즉, **NOT NULL 제약 조건이 필요**합니다. \n여기서 JPA가 자동으로 생성하는 DDL을 확인해 보겠습니다.\n\n![](./.index_images/img.png)\n\n`email varchar(50) not null` 과 같이 not null 제약 조건이 자동으로 명시됨을 알 수 있습니다.\n\n\n\n하지만 DDL은 애플리케이션 내부가 아닌 데이터베이스 컬럼의 속성에 대한 조건이므로, 스프링(자바) 애플리케이션 내 User 엔티티의 email 필드는 null이 허용됩니다.\n\n\n\n테스트 코드로 확인해 보겠습니다.\n\n```java\n@TestConstructor(autowireMode = TestConstructor.AutowireMode.ALL)\n@DataJpaTest\nclass UserRepositoryTest {\n\n    private final UserRepository userRepository;\n\n    public UserRepositoryTest(UserRepository userRepository) {\n        this.userRepository = userRepository;\n    }\n\n    @Test\n    void save() {\n        User user = new User(\"gray1234\", \"password\", \"gray\", null);\n\n        User savedUser = userRepository.save(user);\n\n        assertThat(savedUser).isEqualTo(user);\n    }\n}\n```\n\n![](./.index_images/img2.png)\n\n테스트가 실패함을 확인할 수 있습니다.\n\n\n\n하지만 여기서 주의 깊게 살펴볼 점이 있습니다. 예외 메시지를 따라 올라가 보면 insert 쿼리가 한 번 발생한 것을 알 수 있습니다. 또한 발생한 예외 타입이 JdbcSQL.... Exception입니다.\n\n\n\n즉 애플리케이션 내부가 아닌, 데이터베이스 내에서 예외가 발생했다는 것입니다.\n\n![](./.index_images/img3.png)\n\n애플리케이션 내에서는 email 컬럼의 null을 체크하지 못하고, 데이터베이스에 쿼리가 실행될 때 예외가 발생한다는 것입니다. 또한 insert 쿼리가 데이베이스에서 실행되므로 Auto Increment인 PK가 증가합니다.\n\n![](./.index_images/img4.png)\n\n첫 번째 유저를 저장할 때는 email 값을 null로 할당한 후 try - catch를 이용해 예외를 삼키고, 두 번째 유저는 정상적으로 저장한 후 PK를 출력해 봤습니다. 예상한 것처럼, 두 번째 유저의 PK는 2 임을 확인할 수 있습니다.\n\n\n\n**그렇다면, 애플리케이션 내 엔티티에서는 null을 어떻게 막을 수 있을까요?**\n\n\n## @NotNull과 함께라면?\n\n`@NotNull`은 java validation에 속하며 `@Valid`를 이용한 요청을 처리하는 단계에서 예외를 잡을 수도 있고, bean validation을 통해 예외를 검증할 수도 있습니다.\n\n\n\n우리가 DTO와 같이 클래스의 필드를 검증하는 방법과 동일하게 엔티티의 필드도 검증할 수 있습니다. 당연히 같은 자바 객체이고 도메인이기 때문에 동일하게 적용할 수 있습니다.\n\n\n\n@Valid 어노테이션도 없이 어떻게?라고 생각할 수 있는데요, Hibernate는 엔티티에 적용된 Bean Validation 어노테이션 역시 DDL로 변환합니다. \n@NotNull은 nullable=false와 마찬가지로 DDL 상에서 해당 컬럼이 NOT NULL 제약조건을 가짐을 명시합니다.\n\n\n\n스프링 공식문서인 밸덩에서도 이를 명확히 언급하고 있습니다. [밸덩 바로가기](https://www.baeldung.com/hibernate-notnull-vs-nullable)\n\n\n\n이전 User 테이블에 nullable=false를 제거한 후 @NotNull 어노테이션만 붙여준 후 생성되는 DDL을 살펴보겠습니다.\n\n```java\n@Entity\n@Table(name = \"user\")\npublic class User extends AuditingEntity {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    @Column(name = \"id\", updatable = false)\n    private Long id;\n\n    @Column(name = \"user_id\", length = 20)\n    private String userId;\n\n    @Column(name = \"password\", length = 20)\n    private String password;\n\n    @Column(name = \"name\", length = 20)\n    private String name;\n\n    @NotNull\n    @Column(name = \"email\", length = 50)\n    private String email;\n}\n```\n\n![](./.index_images/img5.png)\n\n동일하게 **email varchar(50) not null** 과 같이 not null 제약 조건이 자동으로 명시됨을 알 수 있습니다.\n\nHibernate에 의해 DDL이 자동으로 생성되는 기능을 끄고 싶다면 아래와 같이 설정하면 됩니다.\n\n```properties\nspring.jpa.properties.hibernate.validator.apply_to_ddl=false\n```\n\n그렇다면 nullable = false와 어떤 차이가 있을까요?\n\n차이점을 알아보기 위해 이전에 User의 email 필드에 null을 세팅한 후 저장하는 테스트 코드를 실행해보겠습니다.\n\n![](./.index_images/img6.png)\n\n다음과 같이 `javax.validation.ConstraintViolationException` 예외가 발생한다는 것을 알 수 있습니다. 또한 nullable=false 방식 때 발생하던 **insert 쿼리가 발생하지 않는다**는 점도 함께 볼 수 있습니다.\n\n\n\n`javax.validation.ConstraintViolationException` 예외가 발생했다는 것은 스프링(자바) 애플리케이션 내에서 예외를 잡았다는 것입니다. 그러므로 데이터베이스에 쿼리가 실행될 일이 없습니다.\n\n\n\n좀 더 정확하게 말하면 객체가 생성되는 시점이 아닌 **엔티티가 영속화되는 시점에 예외가 발생**합니다.\n\n\n\n확실하게 알아보기 위해 동일한 User1, User2 저장 테스트를 실행해 보면 2번째로 저장한 유저의 PK가 1 임을 알 수 있습니다.\n\n![](./.index_images/img7.png)\n\n또한 @NotNull을 어노테이션을 사용하게 되면 예외 메시지를 직접 작성할 수 있는 장점도 존재합니다.\n\n---\n\n여담으로, @NotNull 없이 nullable=false 만 설정한 경우에도 NULL 검증을 진행하고 싶다면 아래와 같이 설정하면 됩니다.\n```properties\nspring.jpa.properties.hibernate.check_nullability=true\n```\n![](./.index_images/img8.png)\n\n\b스프링(자바) 애플리케이션 단에서 예외가 잡히는 것을 볼 수 있습니다. 하지만 예외 타입이 hibernate.PropertyValueException으로 다릅니다.\n\n\n\n굳이 추천하는 방법은 아닌 것 같습니다.\n\n\n\n또한 `@NotBlank`나 `@NotEmpty`는 자동으로 DDL을 생성해주지 않습니다.\n\n그러므로 @NotBlank나 @NotEmpty를 사용하는 경우에는 `@Column(nullable=false)`를 반드시 추가해야 DDL에 NOT NULL 제약조건이 반영됩니다.\n\n---\n\n## 정리\n**@Column(nullable = false)**\nDDL을 자동으로 생성하지만, 애플리케이션의 영속성 컨텍스트에서 엔티티 필드에 null이 들어가는 것을 막지 못한다.\n\n또한 데이터베이스에 쿼리가 실행된 후 예외가 발생한다.\n\n\n\n**@NotNull**\nDDL을 자동으로 생성하고 애플리케이션의 영속성 컨텍스트에서 엔티티 필드에 null이 들어가는 것을 막는다.\n\n잘못된 제약 조건인 경우 데이터베이스에 불필요한 쿼리를 날리지 않는다.\n\n추가적으로 Bean Validation 방식은 예외 메세지를 직접 작성할 수 있다.\n\n또한 @Size와 같이 다른 validation을 통해 엔티티 필드 값을 검증할 수 있다.\n\n\n단점으로는 코드가 지저분해지고, JPA를 이용한 엔티티화 시킨 것이 아닌 객체라면 @NotNull 어노테이션을 해석하지 못한다.\n\n\n하이버네이트에서는 nullable=false보다 @NotNull 사용을 조금 더 권장하고 있다.\n\n\n## 레퍼런스\nhttps://www.baeldung.com/hibernate-notnull-vs-nullable\n\nhttps://kafcamus.tistory.com/15\n\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 피움팀의 기술블로그를 만들기까지의 험난한 과정을 정리해보려고 한다. 플랫폼 선정 팀 블로그를 작성하기 위해 다양한 플랫폼들을 고려해봤다. 티스토리 티스토리는 팀 블로그 기능을 통해 다양한 작성자 및 관리자를 지정할 수 있다. 하지만 UI를 직접 수정할 수 없을 뿐더러 제공되어있는 스킨들이 대부분 …","fields":{"slug":"/blog-starter/"},"frontmatter":{"date":"July 09, 2023","title":"피움 블로그 생성과정","tags":["블로그","세팅"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n \n\n## 서론\n\n피움팀의 기술블로그를 만들기까지의 험난한 과정을 정리해보려고 한다.\n\n## 플랫폼 선정\n\n팀 블로그를 작성하기 위해 다양한 플랫폼들을 고려해봤다.\n\n### 티스토리\n\n티스토리는 팀 블로그 기능을 통해 다양한 작성자 및 관리자를 지정할 수 있다.\n\n하지만 UI를 직접 수정할 수 없을 뿐더러 제공되어있는 스킨들이 대부분 마음에 들지 않았다.\n\nUI 선정과정에 너무많은 힘을 들일 것 같아서 티스토리는 사용하지 않았다.\n\n### Notion\n\n현재 팀 문서 정리를 Notion으로 관리하고있다.\n\n자유롭게 템플릿을 관리할 수 있어서 가장 매력적으로 다가온 툴이였지만 SEO를 적용하지 못하고, 도메인을 별도로 지정해야한다는 단점이 있어 선택하지 않았다.\n\n> 중간에 vercel에서 제공하는 notion 배포기능을 사용하는 방식이 있었지만 Organizaiton으로 레포지토리를 관리하고 있는 현재 상황에서 해당 서비스를 이용하려면 유료였기 때문에 해당 부분도 사용하지 못했다.\n[Notion-backed Next.js Blog](https://vercel.com/templates/next.js/notion-blog)\n\n\n### Velog\n\n팀 계정을 생성해서 Velog에 글을 게시하는 방법을 생각해봤다.\n\n팀 계정을 생성한다는 부분에서 팀 프로젝트가 계속 진행되면서 계정을 관리하는 리소스가 추가되는것이 우려되었기 때문에 선택하지 않았다.\n\n\n### GitBook\n\nGitBook을 이용해서 팀 블로그를 운영할 수도 있겠지만 UI 구성적인 측면에서 기술블로그와는 거리가 멀었기 때문에 선택하지 않았다.\n\n\n### GitHub Pages\n\n정적인 페이지를 무료로 호스팅할 수 있고, 마음만 먹으면 자유롭게 커스터마이징 할 수 있는 환경이라는 부분이 매력있게 다가와 GitHub Pages를 선택했다.\n\n## GitHub Pages로 팀 블로그 만들기\n\n이제 본격적으로 GitHub Pages를 이용하여 팀 블로그를 만드는 과정을 차근차근 따라가보자.\n\ngatsby를 이용하여 블로그 배포를 진행했다. 사용한 템플릿은 gatsby-starter-hoodie이며 블로그에 친절하게 사용법이 게시되어있다.\n\n[🐙 gatsby-starter-hoodie](https://github.com/devHudi/gatsby-starter-hoodie)\n[🚀 gatsby-starter-hoodie 사용 방법 보러가기](https://hoodie.gatsbyjs.io/about-hoodie-kr/)\n\n> 필자는 gatsby에 대해 아무것도 모르는 상태로 해당 작업을 진행했습니다. 😵‍💫\n최대한 알기 쉽게 정리하려고 노력했습니다만... 잘못된 부분이 있다면 댓글로 남겨주시면 감사하겠습니다.\n\n### gatsby란?\n\n우선 사용하고자하는 gatsby가 무엇인지부터 알고 진행해보자.\n\n[위키백과](https://en.wikipedia.org/wiki/Gatsby_(JavaScript_framework))에 따르면\n\n**개츠비는 React와 GraphQL을 사용하여 Node.js 위에 구축된 오픈소스 정적 사이트 생성기입니다.**\n\n라고 설명되어있다.\n\n`Node.js 위에 구축된` 해당 문구로부터 로컬 환경에 Node.js가 필요하다는 것을 알 수 있다.\n\n### node 설치하기\n\n```shell\nbrew install node\n```\n\n![](.index_images/dd266184.png)\n\n`node -v` 명령어를 통해 잘 설치되었는지 확인해본다.\n\n### npx 설치하기\n\n리액트를 공부하자는 글이 아니기 때문에 간단히만 알고 넘어가자\n\nnpx는 npm 사용시에 발생하는 발생할 수 있는 여러 문제점을 해결하기 위해 설계되었다고한다.\n진행하는 과정에서 npx를 사용할 것이기 때문에 npx도 설치하고 넘어가자.\n\n```shell\nnpm install npx -g\n```\n\n![](.index_images/ff1c3736.png)\n\n`npx -v` 명령어를 통해 잘 설치되었는지 확인해본다.\n\n### gatsby-cli 설치하기\n\ngatsby 명령어를 사용하기 위해 gatsby-cli를 설치한다.\n\n```shell\nnpm install -g gatsby-cli\n```\n\n![](.index_images/6494ac93.png)\n\n`gatsby -v` 명령어를 통해 잘 설치되었는지 확인해본다.\n\n### Gatsby 사이트 생성\n\n> 여기서부터는 [Hudi의 Gatsby 환경 구성하기](https://hoodie.gatsbyjs.io/quick-start-kr/)를 따라가는 내용입니다.\n\n> 환경은 Pium의 프로젝트 환경에 맞게 진행됩니다.\n\ngatsby를 이용하여 새로운 사이트를 생성한다.\n이때 `gatsby-starter-hoodie`를 참고하여 구성한다.\n\n```node\nnpx gatsby new pium-official.github.io https://github.com/devHudi/gatsby-starter-hoodie\n```\n\n### 프로젝트 시작해보기\n\n``` shell\ncd pium-official.github.io\n\nnpm run start\n```\n\n![](.index_images/19f5a8e2.png)\n\n`localhost:8000`으로 프로젝트가 열리는 모습을 볼 수 있다.\n\n![](.index_images/7810b698.png)\n\n### 블로그 커스텀하기\n\n해당 부분은 원작자의 [🚀 2.빠르게 시작하기](https://hoodie.gatsbyjs.io/quick-start-kr/)를 참고하여 진행하면 된다.\n\n### GitHub Pages로 배포하기\n\n이제 배포할 페이지를 다 만들었다!\nGitHub Pages를 이용해 배포를 진행해보자.\n\n우선 GitHub Repository를 생성한다.\n\n> pium-official Organization에서 `pium-official.github.io` 레포지토리를 생성했다.\n\n![](.index_images/b084df05.png)\n\n```shell\ngit remote add origin https://github.com/pium-official/pium-official.github.io\n```\n\n```node\nnpm run deploy-gh\n```\n\n위 명령어를 수행하면 팀 레포지토리에 gh-pages 브랜치가 생성되고 build된 결과물이 올라온다. 해당 파일을 기준으로 GitHub Pages에서 배포를 진행한다.\n\n![](.index_images/3db58ec3.png)\n\n> GitHub Pages에서 배포 기준으로 잡는 브랜치를 확인 & 변경하고 싶다면 Settings - Pages 에서 다음 항목을 확인하면 된다.\n> ![](.index_images/b9a5c99c.png)\n\n![](.index_images/1877666d.png)\n\nGitHub Actions 탭에서 `pages build and deployment` 작업이 완료된것을 확인하고 https://pium-official.github.io/ 로 접속해보면 페이지가 성공적으로 배포된것을 확인할 수 있다!!\n\n> https:://{팀 레포명}.github.io\n\n![](.index_images/f35d015e.png)\n\n### 글 작성하기\n\n해당 템플릿에 글을 작성하는 방식은 원작자의 [🤔 3. 작성 가이드](https://hoodie.gatsbyjs.io/writing-guide-kr/)를 참고하면 된다.\n\n### GitHub Actions workflow\n\n현재까지 진행한 작업으로는 다음과 같은 흐름으로 배포를 진행할 수 있다.\n\n1. 로컬에서 글을 작성한다.\n2. 글을 작성한 뒤 `npm run deploy-gh` 명령어를 수행해서 배포를 진행한다.\n\n위 방식은 혼자서 작업할 때는 문제가 없으나 여러명이 함께 글을 작성하기에는 많이 번거롭다.\n\n우리는 다음과 같은 방식으로 배포를 하는 방향을 기대하고 있다.\n\n1. 로컬에서 글을 작성한다.\n2. 글을 작성한 뒤 main 브랜치로 push한다.\n3. main 브랜치에 작업내용이 push 되었을 때 빌드 및 배포가 자동적으로 이뤄진다.\n\n위에서 3번 작업내용을 수행하기 위해 GitHub Actions의 workflow 기능을 이용할 수 있다.\n\n### workflow 작성하기\n\n`gatsby-starter-hoodie`에서 기본적으로 제공하고 있는 workflow는 다음과 같이 구성되어있다.\n\n```yml\nname: CI\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 20.3.1\n\n      - name: Install node packages\n        run: yarn\n        \n      - name: Check lint\n        run: yarn check:lint\n        \n      - name: Check prettier\n        run: yarn check:prettier\n      \n      - name: Build\n        run: yarn build\n```\n\n> 자세한 구문에 대한 설명 및 내용은 [공식문서](https://docs.github.com/ko/actions/using-workflows/workflow-syntax-for-github-actions)를 참고해보면 좋다.\n\n기존에 존재하는 ci.yml 파일은 삭제하고 아래와 같은 deploy.yml을 작성해보자.\n\n> workflow에 대한 yml파일의 경로는 반드시 `.github/workflows` 폴더 내부에 존재해야한다.\n> ![](.index_images/d9fc2f50.png)\n \n\n```yml\nname: Deploy\n\non: # 어떤 작업이 수행될 때 deploy.yml 작업이 수행된다. (트리거)\n  push: # push 작업이 수행될 때\n    branches: # 특정 브랜치를 대상으로\n      - main\n\npermissions: # github action이 수행되는 환경에서 특정 권한을 준다\n  contents: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 20.3.1\n\n      - name: Install node packages\n        run: yarn\n\n      - name: Build\n        run: yarn build\n\n      - name: Deploy 🚀\n        uses: JamesIves/github-pages-deploy-action@v4\n        with:\n          folder: public\n```\n\n> `JamesIves/github-pages-deploy-action@v4` 작업에서 수행하는 내용 중 쓰기권한을 요구하는 작업이 있기 때문에 permissions 설정을 해야한다.\n[관련 이슈 - Failed with exit code 128, Permission Denied](https://github.com/JamesIves/github-pages-deploy-action/issues/1110)\n\n위 deploy.yml 파일을 작성했다면 프로젝트 내부에서 변경사항에 대한 commit을 수행하고 git push를 해본다.\n\n``` shell\ngit branch -m main\n\ngit push origin main\n```\n\n![](.index_images/0491f96f.png)\n\n위 내용을 대략적으로 설명해보자면 다음과 같다.\n\nmain 브랜치에 대한 코드를 build해보고 성공한다면 결과물을 gh-pages 브랜치로 배포한다.\n\n이제 글을 작성하고 main 브랜치로 push하기만 하면 배포까지 자동으로 수행된다.\n로컬에서 `npm run deploy-gh`는 더이상 사용할 필요가 없다.\n\n## 결론\n\n블로그를 생성하고 배포하고 자동화하는 과정까지 차근차근 진행해봤다.\n\n여러버 삽질을 거친 끝에 완성했기 때문에 더 애정이 많이 가는 것 같다. 👍\n\n## Reference\n\n- https://hoodie.gatsbyjs.io/\n- https://docs.github.com/ko/actions/using-workflows/workflow-syntax-for-github-actions\n- https://studium-anywhere.tistory.com/21\n"},{"excerpt":"이 글은 우테코 피움팀 크루 '주노'가 작성했습니다. 서론 기획부터 개발까지 하나의 프로그램이 생성되는 모든 프로세스를 경험할 수 있는 시간을 가졌다. 피움팀에서 어떤 방식을 통해 아이디어와 기능을 도출해나갔는지 기록해두고자 한다. 스프린트란? 스프린트는 팀이 일정량의 작업을 완료하는 시간이 정해진 짧은 기간을 의미한다. 여기서는 기획부터 데모수준의 프로…","fields":{"slug":"/sprint-idea/"},"frontmatter":{"date":"July 08, 2023","title":"구글 스프린트 기반 아이디어 도출","tags":["아이디어","기획"]},"rawMarkdownBody":"\n> 이 글은 우테코 피움팀 크루 '[주노](https://github.com/Choi-JJunho)'가 작성했습니다.\n\n## 서론\n\n기획부터 개발까지 하나의 프로그램이 생성되는 모든 프로세스를 경험할 수 있는 시간을 가졌다.\n\n피움팀에서 어떤 방식을 통해 아이디어와 기능을 도출해나갔는지 기록해두고자 한다.\n\n## 스프린트란?\n\n스프린트는 팀이 일정량의 작업을 완료하는 시간이 정해진 짧은 기간을 의미한다.\n\n여기서는 기획부터 데모수준의 프로그램이 나오는 단계까지를 하나의 스프린트라고 생각해볼 수 있겠다.\n\n피움 팀은 구글 스프린트의 방법론을 차용해서 기능을 도출했다.\n\n### 구글 스프린트\n\n![](.index_images/googlesprint.png)\n\n5일간 결과물을 도출하는 방법론으로 다음과 같은 17단계로 이어진다.\n\n1. 현재 상황에 대한 구성원들의 생각과 문제 상황 공유\n2. 스프린트를 통해 해결하고자 하는 목표 설정\n3. 스프린트 질문 도출\n4. 이해관계자의 구매 여정 Map 작성\n5. 전문가 조언\n6. HMW(어떻게 하면 ~할 수 있을까) 작성 및 선정\n7. 스프린트 기간에 주력하고자 하는 1순위 타깃과 질문 선정\n8. 번갯불 대화 및 솔루션 스케치\n9. 고객 선정을 위한 설문지 작성\n10. 프로토타입 제작을 위한 최종 솔루션 선정\n11. 프로토타입 제작을 위한 스토리보드 만들기\n12. 프로토타입 제작\n13. 프로토타입 완성 및 시연\n14. 고객 인터뷰용 질문 만들기 (인터뷰 담당자)\n15. 타깃 고객 인터뷰 및 학습\n16. 스프린트 이후의 개발 플랜 점검 및 향후 일정 논의\n17. 성찰\n\n### Figma - Figjam\n\n오프라인 환경에서 종이, 캔버스, 포스트잇을 이용해서 각자의 의견을 종합해볼 수도 있지만 시간, 공간적 제약이 있을 때는 Figma를 사용해볼 수도 있다.\n\n디자인 툴인 Figma에는 Figjam이라는 기능을 제공한다.\n\n![](.index_images/figjam.png)\n\nFigjam에서는 포스트잇, 연필그리기, 스티커붙이기, 타이머 등의 기능들을 제공한다.\n\n> 개인적으로 타이머 기능을 적극 활용하는 것이 좋다고 생각했다.\n위 [구글 스프린트](###구글-스프린트)의 `8. 번갯불 대화 및 솔루션 스케치` 항목에서 추구하는 목적이 무엇인지 생각해보면 정해진 시간내로 서로의 의견을 제시하여 회의가 늘어지지 않게 하기 위함임을 알 수 있다.\n정해야할 내용이 많은 만큼 시간이 늘어지는 것만큼은 가장 경계해야한다고 생각한다.\n\n## 서비스 주제\n\n![](.index_images/b95b5dfe.png)\n\n`식물을 잘 키울 수 있도록 도와주는 서비스`라는 주제로 팀이 이뤄졌다.\n\n위 포스터를 확인했을 때 큰 주제로 추상적인 목표들이 즐비해있음을 확인할 수 있다.\n주어진 시간이 짧기 때문에 핵심기능을 추려야할 필요가 있다.\n\n> 💡 모든 과정을 진행할 때 이야기를 듣는 사람들은 형광펜 혹은 스티커 기능을 이용해 중요하다고 생각하는 부분에 색칠을 하며 듣는다. 형광펜과 스티커를 이용한 호응을 이용해 공통 관심사에 집중할 수 있으며 집중력이 분산되는 것도 예방할 수 있다.\n\n> ⏰ 모든 과정에는 제한시간이 존재한다.\n제한시간을 두고 회의가 루즈해지는 것을 막아야만한다!!\n\n### 서비스의 목적과 가치\n\n![](.index_images/959281a8.png)\n\n서비스의 목적과 가치에 대해 구상한다.\n\n이 단계에서는 `서비스가 어떤 기능을 제공함으로서 어떤 문제를 해결한다`를 이야기하며 각자가 추상적인 목표를 어떤 방향으로 구체화하고 있는지 공유한다.\n\n> ⏰ 제한시간 : 3분\n\n### 서비스의 대상\n\n![](.index_images/db53c739.png)\n\n어떤 사용자가 이 서비스를 사용할 지 떠올리고 각자의 생각을 작성한다.\n\n이후 각자가 자신의 생각을 이야기하는 시간을 가진다.\n\n> ⏰ 제한시간 : 3분\n\n\n### 어떻게 하면 ~문제를 해결할 수 있을까?\n\n![](.index_images/1c560fdf.png)\n\n어떻게 하면 ~ 문제를 해결할 수 있을까? 라는 질문들을 작성하고 이야기한다.\n\n이 과정을 통해 해결해야할 문제들을 인지할 수 있다.\n\n이 때 기술적인 질문이 나오지 않도록 경계해야한다.\n해당 과정을 수행하는 이유는 서비스의 목적을 구체화하기 위함이다.\n\n- 어떻게 하면 제 시간에 식물에 물을 줄 수 있을까? (O)\n- 어떻게 하면 무중단 배포를 할 수 있을까? (X)\n\n> ⏰ 제한시간 : 5 ~ 8분\n\n### 워드 클라우드\n\n![](.index_images/07da0c75.png)\n\n위 단계에서 나온 키워드들을 기준으로 단어들을 나열한다.\n중요하게 생각되는 단어들은 크고 굵게 표기한다.\n\n이 과정을 통해 키워드를 간략하게 추릴 수 있다.\n\n> ⏰ 제한시간 : X\n\n### (선택) 모바일 vs 데스크탑\n\n![](.index_images/8f5a28d2.png)\n\n생각하는 서비스가 어떤 환경에서 운영될지 생각해본다.\n\n이 과정을 통해 서비스가 어떻게 그려질지 대략적으로 생각해볼 수 있다.\n\n> ⏰ 제한시간 : 2분\n\n### 장치/요소/기능 브레인스토밍\n\n![](.index_images/2d693ea9.png)\n\n앞서 설정한 문제, 키워드 등을 참고하여 개발하고자 하는 서비스에 있을 것 같은 기능들을 브레인 스토밍 식으로 작성한다.\n\n이 과정을 통해 서비스에 존재할 수 있는 기능들을 파악할 수 있다.\n\n> ⏰ 제한시간 : 5 ~ 8분\n\n### 지도 만들기\n\n![](.index_images/2bb017bc.png)\n\n앞서 정의한 서비스 대상을 참고하여 구체적인 페르소나를 지정한다.\n페르소나를 참고하여 어떤 페이지들이 존재할지 생각해본다.\n\n위(`장치/요소/기능 브레인스토밍`)에서 도출된 기능들을 기반으로 각 페이지에 기능들을 작성한다.\n\n이 과정을 통해 대략적인 서비스 흐름과 구성을 생각해 볼 수 있다.\n\n> ⏰ 제한시간 : X\n\n### 페이지 그려보기\n\n![](.index_images/efe27f26.png)\n\n위 `지도 만들기` 활동에서 도출된 페이지들을 직접 그려보는 시간을 가진다.\n팀원들 각자가 페이지가 어떻게 구성될지 대략적으로 그려본다.\n\n잘 그릴 필요는 없다. 연필로 어떤 버튼이 어디에 위치할지, 어떤 기능이 존재하는지 정도만 알아볼 수 있을정도로 가볍게 스케치해도 좋다.\n\n이 과정을 통해 각자가 생각하는 View를 공유하면서 서비스의 구현 방향성에 대한 싱크를 맞춰갈 수 있다.\n\n> ⏰ 제한시간 : 각 페이지별 5분\n\n### 결정권자 정하기 & 페이지 정하기\n\n![](.index_images/9f853c4e.png)\n\n위 과정에서 작성한 페이지 중 하나의 페이지를 결정하기 위해 결정권자를 정한다.\n\n결정권자는 팀원들의 투표로 정하고 페이지를 결정할 때 만큼은 결정권자의 권한이 가장 높은 수직적인 구조로 진행된다.\n즉, 다시말해 페이지는 결정권자의 선택으로 정해진다.\n\n결정권자를 뽑았다면 각 페이지를 정하는 시간을 가진다.\n\n이 때 결정권자가 아닌 다른 팀원들은 자신의 페이지 혹은 자신이 마음에 드는 페이지에 대해 결정권자를 설득한다.\n\n페이지를 결정 할 때 여러 페이지의 기능을 합칠 수도 있다.\n\n> ⏰ 제한시간 : 각 페이지별 5분 \n> 결정된 페이지들은 명예의전당👑 에 복사하여 올려둔다.\n> ![](.index_images/e4e8a0fb.png)\n\n\n### BDD\n\nBDD(Behavior Driven Development, 행위 주도 개발) 방법론을 차용한 방식으로 given - when - then 절로 해당 페이지에서 일어날 수 있는 모든 이벤트에 대해 정의한다.\n\n이 과정을 통해 구체적인 기능 명세를 할 수 있다.\n\n![](.index_images/faa7372e.png)\n\n해당 방식은 UI 기반의 기능 명세에 치중할 수 있기 때문에 API 설계가 요구된다면 API 명세를 하는것을 추천한다.\n\n> ⏰ 제한시간 : X\n\n## 결론\n\n구글 스프린트를 기반으로 피움 팀이 아이디어를 구체화하는 과정을 정리해봤다.\n\n아이디어 도출 과정에 정답은 없지만 스프린트를 처음 해본 입장에서는 참고하면 좋은 방법이라고 생각되어 기록으로 남겨봤다.\n\n## Reference\n\n- [구글스프린트](https://brunch.co.kr/@brunchjwshim/90)\n- [테오의 스프린트](https://velog.io/@teo/google-sprint-14)\n- [Atlassian Sprint](https://www.atlassian.com/ko/agile/scrum/sprints)\n- https://www.thesprintbook.com/the-design-sprint"}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}